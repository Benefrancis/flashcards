| id | afirma√ß√£o                                                                                                                                                                                                                                                                    | resposta | explica√ß√£o                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|:---|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1  | Em MLOps, o foco principal reside na automa√ß√£o do treinamento de modelos, sendo o versionamento de dados um aspecto secund√°rio e opcional, j√° que os modelos s√£o primariamente definidos pelo c√≥digo.                                                                        |    F     | ‚ùå **Incorreto!** MLOps trata **dados, c√≥digo e modelos** como artefatos de primeira classe. O versionamento de dados √© crucial para reprodutibilidade e rastreabilidade, t√£o importante quanto o versionamento de c√≥digo. **Dica de Concurseiro:** CEBRASPE adora restringir o escopo ("foco principal", "secund√°rio"). MLOps √© hol√≠stico! Pense no trip√©: C√≥digo, Dados, Modelo. Todos s√£o VIPs. (Ref: "Hidden Technical Debt in Machine Learning Systems", Sculley et al.)                                                                                                                           |
| 2  | A Integra√ß√£o Cont√≠nua (CI) em MLOps se restringe √† verifica√ß√£o e teste do c√≥digo-fonte dos scripts de treinamento, n√£o abrangendo a valida√ß√£o de dados ou testes de qualidade espec√≠ficos do modelo de ML.                                                                   |    F     | ‚ùå **Errado!** CI em MLOps √© mais abrangente que no DevOps tradicional. Inclui n√£o apenas testes de c√≥digo, mas tamb√©m **valida√ß√£o de dados**, **testes de qualidade do modelo** (ex: acur√°cia, precis√£o), e at√© **testes de equidade (fairness)**. **Dica de Concurseiro:** "Restringe-se a" √© uma pegadinha comum. CI para ML expande o conceito de CI. (Ref: Google Cloud, "MLOps: Continuous delivery and automation pipelines in machine learning").                                                                                                                                               |
| 3  | O monitoramento de *model drift* e *concept drift* √© essencial em MLOps para garantir que os modelos de Machine Learning mantenham seu desempenho e relev√¢ncia ao longo do tempo em ambientes de produ√ß√£o.                                                                   |    V     | ‚úÖ **Correto!** A realidade muda, e com ela os dados e as rela√ß√µes que os modelos aprendem. O monitoramento de *data drift* (mudan√ßa na distribui√ß√£o dos dados de entrada) e *concept drift* (mudan√ßa na rela√ß√£o entre entrada e sa√≠da) √© um pilar do MLOps para disparar re-treinamentos e manter a acur√°cia. **Dica de Concurseiro:** "Drift" √© palavra-chave para monitoramento em MLOps. Se o modelo "desafina" com o tempo, MLOps tem o "afinador" (monitoramento + CT).                                                                                                                           |
| 4  | MLOps pode ser considerado id√™ntico ao DevOps, aplicando-se as mesmas ferramentas e processos sem adapta√ß√µes significativas, dado que ambos visam a automa√ß√£o e a entrega cont√≠nua.                                                                                          |    F     | ‚ùå **Falso!** MLOps √© *inspirado* no DevOps, mas lida com complexidades adicionais: o ciclo de vida experimental do ML, a gest√£o de dados como artefato central, o versionamento de modelos e a necessidade de monitoramento espec√≠fico de modelos. **Dica de Concurseiro:** Palavras como "id√™ntico" ou "sem adapta√ß√µes" s√£o suspeitas. MLOps √© DevOps ++ para o mundo de ML. (Ref: Martin Fowler, "Continuous Delivery for Machine Learning (CD4ML)").                                                                                                                                                |
| 5  | A governan√ßa de modelos em MLOps se concentra exclusivamente na efici√™ncia da implanta√ß√£o, n√£o se preocupando com aspectos como auditabilidade, rastreabilidade ou equidade (fairness) dos modelos.                                                                          |    F     | ‚ùå **Absolutamente falso!** Governan√ßa em MLOps √© abrangente. Inclui auditabilidade (quem fez o qu√™, quando), rastreabilidade (de onde veio esta predi√ß√£o?), e cada vez mais, a incorpora√ß√£o de testes e monitoramento de equidade para mitigar vieses. **Dica de Concurseiro:** "Exclusivamente" √© um forte indicador de erro em quest√µes sobre conceitos amplos como governan√ßa. Governan√ßa √© sobre controle e responsabilidade, n√£o s√≥ velocidade. (Ref: Treveil et al., "Introducing MLOps").                                                                                                       |
| 6  | A implementa√ß√£o de um pipeline de Treinamento Cont√≠nuo (CT) √© uma pr√°tica recomendada em MLOps, permitindo que os modelos sejam automaticamente re-treinados quando novos dados relevantes est√£o dispon√≠veis ou quando o desempenho do modelo em produ√ß√£o degrada.           |    V     | ‚úÖ **Verdadeiro!** O Treinamento Cont√≠nuo (CT) √© um componente chave da automa√ß√£o em MLOps. Ele automatiza o processo de re-treinamento, garantindo que os modelos permane√ßam atualizados e perform√°ticos. **Dica de Concurseiro:** Pense no ciclo: CI (integra) -> CD (entrega/implanta) -> CT (treina continuamente) -> Monitora -> Repete. CT fecha o loop de aprendizado.                                                                                                                                                                                                                           |
| 7  | Um *Feature Store* √© um componente de MLOps que serve principalmente para armazenar os modelos treinados antes de sua implanta√ß√£o, funcionando como um reposit√≥rio de modelos.                                                                                               |    F     | ‚ùå **Incorreto.** Um *Feature Store* √© um sistema para gerenciar, versionar e servir *features* (caracter√≠sticas) de dados usadas tanto para treinamento quanto para infer√™ncia. O reposit√≥rio de modelos √© geralmente chamado de *Model Registry*. **Dica de Concurseiro:** N√£o confunda "Feature Store" com "Model Registry". "Feature" remete a dados/caracter√≠sticas; "Model" ao artefato treinado.                                                                                                                                                                                                 |
| 8  | A reprodutibilidade em MLOps, alcan√ßada atrav√©s do versionamento de c√≥digo, dados e par√¢metros de treinamento, √© crucial para depura√ß√£o, auditoria e colabora√ß√£o em projetos de Machine Learning.                                                                            |    V     | ‚úÖ **Correto!** A capacidade de reproduzir um treinamento ou uma predi√ß√£o √© fundamental. Sem isso, depurar erros, auditar decis√µes ou colaborar em equipe se torna um pesadelo. MLOps enfatiza o versionamento de todos os artefatos para garantir essa reprodutibilidade. **Dica de Concurseiro:** Reprodutibilidade √© um dos "superpoderes" que MLOps confere. Se n√£o pode reproduzir, n√£o pode confiar totalmente.                                                                                                                                                                                   |
| 9  | Em um cen√°rio de MLOps maduro (N√≠vel 2, conforme classifica√ß√£o do Google), a implanta√ß√£o de modelos em produ√ß√£o ainda √© um processo predominantemente manual, focado na revis√£o humana de cada etapa.                                                                        |    F     | ‚ùå **Falso.** No N√≠vel 2 de maturidade MLOps (segundo o Google), espera-se um sistema de CI/CD *automatizado*, onde o pipeline de ML √© totalmente automatizado, desde o treinamento at√© a implanta√ß√£o. A interven√ß√£o manual √© minimizada para aprova√ß√µes estrat√©gicas, n√£o para cada etapa operacional. **Dica de Concurseiro:** N√≠veis de maturidade progridem em automa√ß√£o. N√≠vel mais alto = mais automa√ß√£o, menos interven√ß√£o manual rotineira. (Ref: Google Cloud MLOps Maturity Levels).                                                                                                          |
| 10 | A utiliza√ß√£o de cont√™ineres, como Docker, √© uma pr√°tica comum em MLOps para encapsular o ambiente de treinamento e infer√™ncia, garantindo consist√™ncia e portabilidade dos modelos de ML.                                                                                    |    V     | ‚úÖ **Verdadeiro!** Cont√™ineres s√£o essenciais para MLOps. Eles empacotam o modelo, suas depend√™ncias e configura√ß√µes, assegurando que ele funcione da mesma forma em diferentes ambientes (desenvolvimento, teste, produ√ß√£o) e facilitando a escalabilidade. **Dica de Concurseiro:** Docker (ou similar) √© o "tupperware" do MLOps: mant√©m tudo junto, fresco e pronto para ir para qualquer lugar! üì¶                                                                                                                                                                                                 |
| 11 | A orquestra√ß√£o de pipelines em MLOps, utilizando ferramentas como Apache Airflow ou Kubeflow Pipelines, √© fundamental apenas para projetos de grande escala, sendo um exagero para o gerenciamento de um √∫nico modelo de ML.                                                 |    F     | ‚ùå **Incorreto!** Embora a complexidade da orquestra√ß√£o aumente com a escala, os benef√≠cios de automatizar e gerenciar o fluxo de trabalho (pipeline) s√£o v√°lidos mesmo para um √∫nico modelo cr√≠tico. Garante reprodutibilidade, facilita o re-treinamento e o monitoramento. **Dica de Concurseiro:** Cuidado com "apenas para grande escala". Princ√≠pios de MLOps s√£o escal√°veis. A *ferramenta* pode ser mais simples para projetos menores, mas a *necessidade* de orquestrar o ciclo de vida existe.                                                                                               |
| 12 | O versionamento de modelos em um *Model Registry* permite n√£o apenas armazenar diferentes vers√µes de um modelo, mas tamb√©m gerenciar seu ciclo de vida, como promover um modelo de "Staging" para "Production" ap√≥s valida√ß√£o.                                               |    V     | ‚úÖ **Correto!** Um *Model Registry* (como o do MLflow) vai al√©m do simples armazenamento. Ele √© central para a governan√ßa de modelos, permitindo versionamento, anota√ß√£o com metadados (m√©tricas, par√¢metros) e o gerenciamento de est√°gios (ex: desenvolvimento, staging, produ√ß√£o, arquivado), facilitando implanta√ß√µes controladas. **Dica de Concurseiro:** Pense no Model Registry como a "biblioteca central de modelos aprovados", com controle de qualidade e status.                                                                                                                           |
| 13 | Em MLOps, a fase de experimenta√ß√£o, onde cientistas de dados testam diferentes algoritmos e hiperpar√¢metros, est√° completamente dissociada dos pipelines de produ√ß√£o e n√£o se beneficia de ferramentas de versionamento ou rastreamento.                                     |    F     | ‚ùå **Falso!** Uma boa pr√°tica de MLOps √© integrar a fase de experimenta√ß√£o. Ferramentas como MLflow Tracking permitem registrar cada experimento, seus par√¢metros, m√©tricas e artefatos (incluindo modelos), facilitando a compara√ß√£o, reprodutibilidade e a transi√ß√£o de um experimento bem-sucedido para um pipeline de produ√ß√£o. **Dica de Concurseiro:** MLOps visa quebrar silos. A experimenta√ß√£o √© o *in√≠cio* do ciclo de vida gerenciado, n√£o uma ilha isolada.                                                                                                                                 |
| 14 | A explicabilidade de modelos (XAI) no contexto de MLOps √© uma preocupa√ß√£o puramente acad√™mica, sem aplica√ß√µes pr√°ticas relevantes para a opera√ß√£o e governan√ßa de modelos no setor p√∫blico.                                                                                  |    F     | ‚ùå **Muito Falso!** XAI √© crucial no setor p√∫blico! Entender *por que* um modelo toma certas decis√µes (ex: negar um benef√≠cio, sinalizar uma transa√ß√£o como fraude) √© vital para transpar√™ncia, accountability, detec√ß√£o de vieses e para ganhar a confian√ßa dos cidad√£os e auditores. MLOps pode integrar ferramentas de XAI nos pipelines. **Dica de Concurseiro:** No setor p√∫blico, "caixas-pretas" s√£o problem√°ticas. XAI √© a "lanterna" para iluminar o interior.                                                                                                                                 |
| 15 | Uma das principais motiva√ß√µes para a ado√ß√£o de MLOps √© mitigar o risco de "d√≠vida t√©cnica" espec√≠fica de sistemas de Machine Learning, como o emaranhamento de c√≥digo e dados ou a depend√™ncia de features inst√°veis.                                                        |    V     | ‚úÖ **Verdadeiro!** Sistemas de ML podem acumular formas √∫nicas de d√≠vida t√©cnica (conforme destacado no paper do Google "Hidden Technical Debt in Machine Learning Systems"). MLOps, com seu foco em modularidade, versionamento, testes rigorosos e monitoramento, ajuda a prevenir e gerenciar essa d√≠vida. **Dica de Concurseiro:** D√≠vida t√©cnica em ML √© um monstro sorrateiro. MLOps s√£o as boas pr√°ticas de "higiene" para mant√™-lo sob controle.                                                                                                                                                |
| 16 | MLOps se aplica exclusivamente a modelos de Deep Learning, dada a sua complexidade, n√£o sendo relevante para modelos estat√≠sticos mais simples como regress√£o log√≠stica ou √°rvores de decis√£o.                                                                               |    F     | ‚ùå **Incorreto.** Os princ√≠pios de MLOps s√£o aplic√°veis a **qualquer** tipo de modelo de Machine Learning que precise ser operacionalizado de forma confi√°vel e escal√°vel, independentemente de sua complexidade algor√≠tmica. A necessidade de versionamento, automa√ß√£o e monitoramento existe para todos. **Dica de Concurseiro:** Evite generaliza√ß√µes com "exclusivamente". Se um modelo vai para produ√ß√£o e tem impacto, MLOps √© relevante.                                                                                                                                                         |
| 17 | A estrat√©gia de implanta√ß√£o "Canary Release" (Implanta√ß√£o Can√°rio) em MLOps envolve liberar uma nova vers√£o do modelo para um pequeno subconjunto de usu√°rios ou tr√°fego antes de uma libera√ß√£o completa, permitindo monitorar seu desempenho e mitigar riscos.              |    V     | ‚úÖ **Correto!** "Canary releases" s√£o uma t√©cnica de implanta√ß√£o gradual. No MLOps, isso permite testar um novo modelo em um ambiente de produ√ß√£o real com impacto limitado. Se o "can√°rio" (o novo modelo) se sair bem, a implanta√ß√£o √© expandida. Se n√£o, pode ser revertida rapidamente. **Dica de Concurseiro:** Pense no can√°rio na mina de carv√£o. Se ele "morrer" (performar mal), voc√™ sabe que h√° um problema antes de afetar todos.                                                                                                                                                           |
| 18 | A responsabilidade pela qualidade dos dados de entrada para treinamento e infer√™ncia de modelos de ML recai unicamente sobre as equipes de DataOps, n√£o sendo uma preocupa√ß√£o direta das pr√°ticas de MLOps.                                                                  |    F     | ‚ùå **Falso.** Embora DataOps seja crucial para fornecer dados de qualidade, MLOps tamb√©m se preocupa com os dados. Pipelines de MLOps incluem etapas de **valida√ß√£o de dados** antes do treinamento e **monitoramento de data drift** em produ√ß√£o. H√° uma forte intersec√ß√£o e colabora√ß√£o necess√°ria entre DataOps e MLOps. **Dica de Concurseiro:** "Unicamente" √© outra palavra perigosa. MLOps e DataOps s√£o parceiros na jornada do dado ao modelo.                                                                                                                                                 |
| 19 | O conceito de "Infraestrutura como C√≥digo" (IaC), embora comum em DevOps, n√£o possui aplica√ß√£o direta em MLOps, pois a infraestrutura de ML √© geralmente gerenciada manualmente por especialistas.                                                                           |    F     | ‚ùå **Errado!** IaC (ex: usando Terraform, CloudFormation) √© altamente relevante em MLOps. Permite definir e gerenciar a infraestrutura necess√°ria para treinamento, implanta√ß√£o e monitoramento de modelos de forma program√°tica, garantindo consist√™ncia, reprodutibilidade e escalabilidade dos ambientes. **Dica de Concurseiro:** Se algo pode ser automatizado e versionado, MLOps provavelmente o abra√ßa. Infraestrutura √© um desses "algos".                                                                                                                                                     |
| 20 | No contexto de concursos p√∫blicos, compreender MLOps √© relevante apenas para cargos de alta gest√£o ou especialistas em IA, n√£o tendo impacto para analistas de controle ou auditores governamentais.                                                                         |    F     | ‚ùå **Incorreto!** Com a crescente ado√ß√£o de IA no governo, analistas de controle e auditores precisam entender como esses sistemas s√£o desenvolvidos, implantados e gerenciados para avaliar sua conformidade, riscos, efici√™ncia e equidade. MLOps fornece o framework para essa compreens√£o e para a realiza√ß√£o de auditorias eficazes em sistemas de ML. **Dica de Concurseiro:** A IA est√° se tornando transversal. Auditores do futuro (e do presente!) precisam entender os "bastidores" dos sistemas que auditam, e MLOps √© parte fundamental disso.                                             |
| 21 | A principal diferen√ßa entre MLOps e DataOps reside no fato de que MLOps foca na operacionaliza√ß√£o do modelo de ML, enquanto DataOps se concentra exclusivamente na engenharia de features para esses modelos.                                                                |    F     | ‚ùå **Incorreto.** Embora a engenharia de features seja uma parte importante do ciclo de vida dos dados, DataOps tem um escopo mais amplo, abrangendo todo o pipeline de dados: ingest√£o, armazenamento, transforma√ß√£o, qualidade e entrega de dados para diversos consumidores, n√£o apenas modelos de ML. MLOps usa os dados preparados pelo DataOps. **Dica de Concurseiro:** "Exclusivamente" √© uma bandeira vermelha. DataOps √© sobre o fluxo de dados; MLOps √© sobre o fluxo de modelos. Eles s√£o complementares.                                                                                   |
| 22 | O monitoramento em MLOps deve se ater apenas a m√©tricas t√©cnicas do modelo, como acur√°cia e lat√™ncia, sendo desnecess√°rio o acompanhamento de m√©tricas de neg√≥cio ou impacto social da aplica√ß√£o de ML.                                                                      |    F     | ‚ùå **Falso.** Um sistema MLOps maduro busca conectar o desempenho do modelo com os objetivos de neg√≥cio ou impacto p√∫blico. Por exemplo, um modelo de detec√ß√£o de fraude n√£o √© √∫til apenas por ser acurado, mas por quanto `R$ X` ele economiza ou qual a redu√ß√£o na taxa de fraudes. M√©tricas de impacto s√£o cruciais para justificar e guiar o desenvolvimento de ML. **Dica de Concurseiro:** ML n√£o existe no v√°cuo. MLOps visa entregar valor, e isso precisa ser medido em termos relevantes para a organiza√ß√£o/sociedade.                                                                        |
| 23 | A ado√ß√£o de pr√°ticas de MLOps, como a automa√ß√£o de pipelines de CI/CD, invariavelmente aumenta os custos iniciais de um projeto de ML, mas tende a reduzir os custos de manuten√ß√£o e retrabalho a longo prazo, resultando em um ROI positivo.                                |    V     | ‚úÖ **Correto!** Implementar MLOps requer um investimento inicial em ferramentas, configura√ß√£o de pipelines e treinamento. No entanto, essa automa√ß√£o e rigor reduzem erros manuais, agilizam re-treinamentos, facilitam a detec√ß√£o de problemas e diminuem o custo de manter modelos em produ√ß√£o ao longo do tempo. **Dica de Concurseiro:** Pense em MLOps como um investimento. Custa um pouco mais no come√ßo, mas paga dividendos em efici√™ncia e confiabilidade depois. "Invariavelmente" pode ser forte, mas a "tend√™ncia" e o "ROI positivo a longo prazo" s√£o o cerne da justificativa de MLOps. |
| 24 | O conceito de "Shadow IT" em MLOps refere-se ao desenvolvimento e implanta√ß√£o de modelos de ML por equipes de cientistas de dados sem o conhecimento ou supervis√£o da equipe de TI/opera√ß√µes, o que pode gerar riscos de seguran√ßa e conformidade.                           |    V     | ‚úÖ **Verdadeiro.** "Shadow IT" √© um risco quando modelos s√£o desenvolvidos e colocados em uso sem seguir os processos de governan√ßa e infraestrutura estabelecidos. MLOps busca trazer esses desenvolvimentos "sombra" para dentro de um framework gerenciado, colaborativo e seguro. **Dica de Concurseiro:** Shadow IT √© o "Velho Oeste" do ML. MLOps √© o "xerife" que traz ordem e seguran√ßa. ü§†‚û°Ô∏èüëÆ‚Äç‚ôÇÔ∏è                                                                                                                                                                                              |
| 25 | Em um pipeline de MLOps, o versionamento de c√≥digo com Git √© suficiente para garantir a reprodutibilidade completa de um modelo, n√£o sendo necess√°rio versionar os dados de treinamento ou os hiperpar√¢metros utilizados.                                                    |    F     | ‚ùå **Falso.** Para reprodutibilidade completa, √© essencial versionar **todos** os componentes que influenciam o modelo: o c√≥digo, os dados de treinamento (com ferramentas como DVC), os hiperpar√¢metros (geralmente com ferramentas de experiment tracking como MLflow) e o ambiente de execu√ß√£o. **Dica de Concurseiro:** Reprodutibilidade em ML √© um quebra-cabe√ßa de v√°rias pe√ßas. Git √© apenas uma delas. üß©                                                                                                                                                                                      |
| 26 | A pr√°tica de "teste A/B" para modelos de ML em produ√ß√£o, onde diferentes vers√µes do modelo s√£o expostas a diferentes segmentos de usu√°rios para comparar seu desempenho, √© uma t√©cnica de implanta√ß√£o e valida√ß√£o facilitada por MLOps.                                      |    V     | ‚úÖ **Correto!** Testes A/B (ou testes multivariados) s√£o uma forma robusta de comparar o desempenho de modelos em um ambiente real. MLOps, com seus pipelines de CD e capacidades de monitoramento, facilita a configura√ß√£o e a an√°lise desses experimentos, permitindo decis√µes baseadas em dados sobre qual modelo promover. **Dica de Concurseiro:** Teste A/B √© como uma competi√ß√£o justa entre modelos. MLOps organiza a corrida e apura o vencedor. üèÅ                                                                                                                                            |
| 27 | O √∫nico objetivo do monitoramento em MLOps √© detectar a degrada√ß√£o do desempenho preditivo do modelo (ex: queda na acur√°cia). Outros aspectos, como o uso de recursos computacionais ou a lat√™ncia das predi√ß√µes, s√£o irrelevantes.                                          |    F     | ‚ùå **Incorreto.** O monitoramento em MLOps √© multifacetado. Inclui: 1) **Performance do modelo** (acur√°cia, precis√£o, recall, F1), 2) **Drift de dados e conceito**, 3) **M√©tricas operacionais** (lat√™ncia, throughput, taxa de erro do servidor, uso de recursos), e 4) **M√©tricas de neg√≥cio/impacto**. Todos s√£o importantes para a sa√∫de do sistema. **Dica de Concurseiro:** Um modelo pode ser preciso, mas lento demais ou custoso para rodar. MLOps olha o todo.                                                                                                                               |
| 28 | O uso de "notebooks" (como Jupyter Notebooks) para experimenta√ß√£o inicial em projetos de ML √© intrinsecamente incompat√≠vel com as pr√°ticas de MLOps, devendo ser completamente abandonados em favor de scripts Python.                                                       |    F     | ‚ùå **Falso.** Notebooks s√£o excelentes para explora√ß√£o e prototipagem. O desafio do MLOps √© como integrar o trabalho feito em notebooks no ciclo de vida de produ√ß√£o. Pr√°ticas como refatorar c√≥digo de notebooks em scripts modulares, versionar notebooks e usar ferramentas que permitem executar notebooks como parte de um pipeline (ex: Papermill) ajudam a compatibilizar. **Dica de Concurseiro:** N√£o √© "notebooks OU MLOps", mas "notebooks E MLOps". A quest√£o √© como fazer a transi√ß√£o da explora√ß√£o para a produ√ß√£o de forma robusta. üßë‚Äçüî¨‚û°Ô∏èüè≠                                            |
| 29 | A rastreabilidade de ponta a ponta (end-to-end lineage) em MLOps, que permite conectar uma predi√ß√£o espec√≠fica de volta aos dados, c√≥digo e vers√£o do modelo que a geraram, √© um requisito fundamental para a conformidade regulat√≥ria em setores como o financeiro e sa√∫de. |    V     | ‚úÖ **Verdadeiro!** Em setores regulados, ser capaz de explicar e auditar como uma decis√£o algor√≠tmica foi tomada √© crucial. A linhagem de dados e modelos, um pilar do MLOps, fornece essa rastreabilidade, permitindo reconstruir o contexto de qualquer predi√ß√£o. **Dica de Concurseiro:** Se o regulador perguntar "como esse modelo decidiu isso?", MLOps ajuda a ter a resposta na ponta da l√≠ngua (ou do log!). üìúüîç                                                                                                                                                                              |
| 30 | MLOps √© um padr√£o r√≠gido e inflex√≠vel, com um conjunto √∫nico de ferramentas e arquiteturas que devem ser adotadas por todas as organiza√ß√µes da mesma forma para garantir o sucesso.                                                                                          |    F     | ‚ùå **Falso.** MLOps √© um conjunto de **princ√≠pios e pr√°ticas** que podem ser implementados de diversas maneiras, com diferentes ferramentas e n√≠veis de maturidade, dependendo das necessidades, escala e recursos da organiza√ß√£o. Flexibilidade e adapta√ß√£o s√£o chave. **Dica de Concurseiro:** MLOps √© mais uma filosofia e um guia do que uma receita de bolo. üç∞ Cada organiza√ß√£o assa o seu, seguindo os princ√≠pios.                                                                                                                                                                               |


