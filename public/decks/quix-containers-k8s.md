| id  | afirma√ß√£o                                                                                                                                                                                                                                                                                                                                                                                                                | resposta | explica√ß√£o                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-----|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1   | Cont√™ineres, como os gerenciados pelo Docker, virtualizam o hardware subjacente, permitindo que cada cont√™iner execute um sistema operacional completamente isolado.                                                                                                                                                                                                                                                     | F        | ‚ùå Errado! Cont√™ineres virtualizam o *sistema operacional*, compartilhando o kernel do SO hospedeiro. S√£o as M√°quinas Virtuais (VMs) que virtualizam o hardware. üí° Pegadinha cl√°ssica: confundir virtualiza√ß√£o de SO (cont√™iner) com virtualiza√ß√£o de hardware (VM). Pense: Cont√™iner = Leveza e Compartilhamento de Kernel.                                                                                                                                                                                                           |
| 2   | Uma imagem de cont√™iner √© uma inst√¢ncia em execu√ß√£o de um cont√™iner.                                                                                                                                                                                                                                                                                                                                                     | F        | ‚ùå Falso! O oposto √© verdadeiro. Uma imagem √© um *template* ou *blueprint* (est√°tico, imut√°vel). Um cont√™iner √© uma inst√¢ncia em execu√ß√£o de uma imagem. üñºÔ∏è Imagem = Receita do bolo, Cont√™iner = Bolo pronto.                                                                                                                                                                                                                                                                                                                         |
| 3   | Docker √© uma plataforma de orquestra√ß√£o de cont√™ineres, sendo o principal concorrente do Kubernetes.                                                                                                                                                                                                                                                                                                                     | F        | ‚ùå Parcialmente falso e confuso! Docker √© uma plataforma para *criar e rodar* cont√™ineres. Kubernetes √© um *orquestrador*. Docker Swarm (parte do Docker) √© um orquestrador, mas o Kubernetes √© muito mais robusto e popular. üê≥ Docker = Ferramenta para cont√™ineres; K8s = Gerente dos cont√™ineres.                                                                                                                                                                                                                                   |
| 4   | Kubernetes, frequentemente abreviado como K8s, √© uma plataforma open-source originalmente desenvolvida pelo Google para automatizar a implanta√ß√£o, o escalonamento e a opera√ß√£o de aplica√ß√µes em cont√™ineres.                                                                                                                                                                                                            | V        | ‚úÖ Verdadeiro! Defini√ß√£o precisa do Kubernetes. O "8" em K8s representa as oito letras entre "K" e "s" em "Kubernetes". üè¢ K8s = O "maestro" da orquestra de cont√™ineres.                                                                                                                                                                                                                                                                                                                                                               |
| 5   | O `etcd` √© um componente do Worker Node no Kubernetes, respons√°vel por executar os cont√™ineres definidos nos Pods.                                                                                                                                                                                                                                                                                                       | F        | ‚ùå Errado! O `etcd` √© o banco de dados chave-valor distribu√≠do que armazena toda a configura√ß√£o e o estado do cluster Kubernetes, e ele reside no *Control Plane* (Master Node). Quem executa cont√™ineres no Worker Node √© o `kubelet` em conjunto com o container runtime. üíæ `etcd` = O c√©rebro/mem√≥ria do K8s.                                                                                                                                                                                                                       |
| 6   | Um Pod no Kubernetes √© a menor unidade de implanta√ß√£o e pode conter um ou mais cont√™ineres que compartilham recursos de rede e armazenamento.                                                                                                                                                                                                                                                                            | V        | ‚úÖ Correto! Esta √© a defini√ß√£o fundamental de um Pod. Os cont√™ineres dentro de um mesmo Pod rodam no mesmo "localhost" e podem se comunicar via `localhost`. üì¶ Pod = Uma "ervilha" na vagem, contendo um ou mais "gr√£os" (cont√™ineres).                                                                                                                                                                                                                                                                                                |
| 7   | O `kube-scheduler` √© respons√°vel por monitorar a sa√∫de dos n√≥s e dos Pods, reiniciando cont√™ineres que falham.                                                                                                                                                                                                                                                                                                           | F        | ‚ùå Falso! O `kube-scheduler` decide em qual *N√≥* um novo Pod deve ser executado. Quem monitora e reinicia cont√™ineres/Pods √© o `kubelet` (no n√≠vel do n√≥) e os *Controllers* (como o Deployment Controller, no n√≠vel do cluster). üóìÔ∏è `kube-scheduler` = O "planejador de assentos" dos Pods.                                                                                                                                                                                                                                           |
| 8   | Um `Service` do tipo `ClusterIP` no Kubernetes exp√µe a aplica√ß√£o para acesso externo ao cluster, geralmente atrav√©s de um endere√ßo IP p√∫blico.                                                                                                                                                                                                                                                                           | F        | ‚ùå Errado! `ClusterIP` exp√µe o servi√ßo *apenas internamente* no cluster, no IP virtual do cluster. Para acesso externo, usa-se `NodePort` ou `LoadBalancer`. üè† `ClusterIP` = Interfone do pr√©dio (s√≥ para dentro); `LoadBalancer` = Campainha da rua (acesso externo).                                                                                                                                                                                                                                                                 |
| 9   | `Deployments` no Kubernetes gerenciam `ReplicaSets` e permitem realizar rollouts e rollbacks de aplica√ß√µes de forma declarativa.                                                                                                                                                                                                                                                                                         | V        | ‚úÖ Verdadeiro! `Deployments` s√£o uma abstra√ß√£o de n√≠vel superior que controlam `ReplicaSets` para manter o estado desejado das aplica√ß√µes, incluindo atualiza√ß√µes e revers√µes. üöÄ `Deployment` = O "comandante da miss√£o" para suas aplica√ß√µes.                                                                                                                                                                                                                                                                                         |
| 10  | Rancher √© uma plataforma que substitui o Kubernetes, oferecendo uma orquestra√ß√£o de cont√™ineres mais simplificada.                                                                                                                                                                                                                                                                                                       | F        | ‚ùå Falso! Rancher √© uma plataforma de *gerenciamento* para Kubernetes (e outros). Ele n√£o substitui o Kubernetes, mas o *complementa*, simplificando sua opera√ß√£o. üë®‚Äçüåæ Rancher = O "gerente da fazenda" que ajuda a cuidar das "planta√ß√µes" Kubernetes.                                                                                                                                                                                                                                                                               |
| 11  | O `kube-apiserver` √© o front-end do Control Plane do Kubernetes, validando e processando requisi√ß√µes REST para modificar o estado dos objetos do cluster.                                                                                                                                                                                                                                                                | V        | ‚úÖ Correto! Toda comunica√ß√£o com o cluster Kubernetes, seja do `kubectl`, da UI ou de outros componentes, passa pelo `kube-apiserver`. üó£Ô∏è `kube-apiserver` = O "recepcionista oficial" do Kubernetes.                                                                                                                                                                                                                                                                                                                                  |
| 12  | `Namespaces` no Kubernetes s√£o utilizados para dividir os recursos de um cluster f√≠sico em m√∫ltiplos clusters virtuais isolados, cada um com seu pr√≥prio Control Plane.                                                                                                                                                                                                                                                  | F        | ‚ùå Errado! `Namespaces` fornecem um escopo para nomes e uma forma de dividir recursos *dentro de um √∫nico cluster*, mas n√£o criam clusters virtuais com Control Planes separados. Pense neles como "pastas" para organizar recursos. üìÇ `Namespace` = Divis√≥rias em um escrit√≥rio, n√£o pr√©dios diferentes.                                                                                                                                                                                                                              |
| 13  | `ConfigMaps` s√£o utilizados para armazenar dados n√£o sens√≠veis de configura√ß√£o em pares chave-valor, enquanto `Secrets` s√£o destinados a dados sens√≠veis como senhas e tokens.                                                                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro! Essa √© a distin√ß√£o fundamental entre `ConfigMaps` e `Secrets`. Ambos desacoplam a configura√ß√£o da imagem do cont√™iner. ‚öôÔ∏è `ConfigMap` = Post-it com lembretes; ü§´ `Secret` = Cofre com a senha.                                                                                                                                                                                                                                                                                                                          |
| 14  | O comando `kubectl apply -f manifest.yaml` √© usado para criar ou atualizar recursos no Kubernetes de forma imperativa.                                                                                                                                                                                                                                                                                                   | F        | ‚ùå Falso! `kubectl apply -f` √© uma abordagem *declarativa*. Voc√™ declara o estado desejado no arquivo YAML, e o Kubernetes faz o necess√°rio para alcan√ß√°-lo. Comandos como `kubectl run` ou `kubectl create` (sem `apply`) podem ser mais imperativos. üìú `apply -f` = "Aqui est√° o plano, execute-o!"                                                                                                                                                                                                                                  |
| 15  | Rancher permite gerenciar m√∫ltiplos clusters Kubernetes, independentemente de onde eles estejam rodando (on-premises, nuvens p√∫blicas, etc.), a partir de uma √∫nica interface.                                                                                                                                                                                                                                           | V        | ‚úÖ Correto! Esta √© uma das principais propostas de valor do Rancher: gerenciamento centralizado de clusters heterog√™neos. üåê Rancher = O "painel de controle universal" para seus clusters K8s.                                                                                                                                                                                                                                                                                                                                         |
| 16  | Um `PersistentVolume` (PV) √© uma solicita√ß√£o de armazenamento feita por um usu√°rio ou Pod, enquanto um `PersistentVolumeClaim` (PVC) √© uma pe√ßa de armazenamento no cluster.                                                                                                                                                                                                                                             | F        | ‚ùå Errado! √â o contr√°rio. PV (`PersistentVolume`) √© a *pe√ßa* de armazenamento (o "disco"). PVC (`PersistentVolumeClaim`) √© a *solicita√ß√£o* (o "pedido de disco"). üíæ PV = O HD na prateleira; PVC = O bilhete "Preciso de um HD".                                                                                                                                                                                                                                                                                                       |
| 17  | O `kube-proxy` √© um componente que roda em cada n√≥ e √© respons√°vel por manter as regras de rede que permitem a comunica√ß√£o com os Pods atrav√©s dos `Services`.                                                                                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro! O `kube-proxy` implementa parte do conceito de `Service`, gerenciando o encaminhamento de tr√°fego para os Pods corretos. üö¶ `kube-proxy` = O "guarda de tr√¢nsito" da rede em cada n√≥.                                                                                                                                                                                                                                                                                                                                    |
| 18  | Ao utilizar um `Service` do tipo `LoadBalancer` em um provedor de nuvem, o Kubernetes automaticamente provisiona um balanceador de carga externo espec√≠fico da nuvem.                                                                                                                                                                                                                                                    | V        | ‚úÖ Correto! Esta √© a m√°gica do tipo `LoadBalancer`. Ele integra-se com o provedor de nuvem (AWS, Azure, GCP) para criar um balanceador de carga que exp√µe o servi√ßo externamente. ‚òÅÔ∏è `LoadBalancer` = Seu "assistente pessoal" na nuvem para tr√°fego externo.                                                                                                                                                                                                                                                                           |
| 19  | O Rancher Kubernetes Engine (RKE) √© uma distribui√ß√£o Kubernetes que pode ser usada pelo Rancher para provisionar clusters, mas o Rancher tamb√©m pode importar e gerenciar clusters criados com outras distribui√ß√µes como EKS ou GKE.                                                                                                                                                                                     | V        | ‚úÖ Verdadeiro! RKE √© uma das op√ß√µes de provisionamento do Rancher, mas a for√ßa do Rancher est√° em sua capacidade de gerenciar uma variedade de clusters Kubernetes certificados pela CNCF. üõ†Ô∏è RKE = Uma das "ferramentas" do Rancher para construir K8s.                                                                                                                                                                                                                                                                               |
| 20  | `StatefulSets` s√£o ideais para aplica√ß√µes stateless que podem ser facilmente replicadas sem se preocupar com identidade de rede ou armazenamento persistente individual.                                                                                                                                                                                                                                                 | F        | ‚ùå Falso! `StatefulSets` s√£o projetados para aplica√ß√µes *stateful* (com estado), como bancos de dados, que requerem identidades de rede est√°veis e armazenamento persistente √∫nico por r√©plica. Para stateless, `Deployments` s√£o mais adequados. üêò `StatefulSet` = Para aplica√ß√µes que "lembram" das coisas.                                                                                                                                                                                                                          |
| 21  | As `Liveness Probes` no Kubernetes s√£o usadas para determinar quando um cont√™iner est√° pronto para come√ßar a aceitar tr√°fego.                                                                                                                                                                                                                                                                                            | F        | ‚ùå Errado! `Liveness Probes` verificam se um cont√™iner est√° *vivo/saud√°vel*. Se falhar, o `kubelet` reinicia o cont√™iner. S√£o as `Readiness Probes` que indicam se o cont√™iner est√° *pronto* para receber tr√°fego. ‚ù§Ô∏è `Liveness` = "Est√° respirando?"; ‚úÖ `Readiness` = "Pronto para trabalhar?".                                                                                                                                                                                                                                        |
| 22  | O `Horizontal Pod Autoscaler` (HPA) no Kubernetes pode escalar o n√∫mero de Pods em um `Deployment` ou `StatefulSet` com base no uso de CPU, mem√≥ria ou m√©tricas customizadas.                                                                                                                                                                                                                                            | V        | ‚úÖ Correto! O HPA √© a ferramenta chave para escalabilidade autom√°tica baseada em demanda no Kubernetes. üìà HPA = O "termostato inteligente" para o n√∫mero de Pods.                                                                                                                                                                                                                                                                                                                                                                      |
| 23  | Rancher obriga o uso de sua pr√≥pria distribui√ß√£o Kubernetes (RKE ou K3s) para todos os clusters gerenciados por ele.                                                                                                                                                                                                                                                                                                     | F        | ‚ùå Falso! Embora Rancher ofere√ßa RKE e K3s (uma distro leve), ele pode importar e gerenciar qualquer cluster Kubernetes certificado pela CNCF, como GKE, EKS, AKS, ou at√© mesmo clusters "vanilla" customizados. ü§ù Rancher = "Poliglota" em termos de distribui√ß√µes K8s.                                                                                                                                                                                                                                                               |
| 24  | `DaemonSets` garantem que uma c√≥pia de um Pod espec√≠fico seja executada em todos (ou em um subconjunto de) os n√≥s do cluster.                                                                                                                                                                                                                                                                                            | V        | ‚úÖ Verdadeiro! `DaemonSets` s√£o √∫teis para agentes de monitoramento, coletores de logs ou qualquer pod que precise rodar em cada n√≥. üëª `DaemonSet` = Um "agente residente" em cada n√≥.                                                                                                                                                                                                                                                                                                                                                 |
| 25  | `Secrets` no Kubernetes s√£o armazenados no `etcd` de forma criptografada por padr√£o em todas as instala√ß√µes, sem necessidade de configura√ß√£o adicional.                                                                                                                                                                                                                                                                  | F        | ‚ùå Cuidado! Por padr√£o, `Secrets` s√£o armazenados no `etcd` como base64-encoded, que *n√£o √© criptografia*. A criptografia em repouso (encryption at rest) para `Secrets` precisa ser configurada explicitamente. üîë Base64 ‚â† Criptografia. √â apenas um disfarce! Exija criptografia de verdade para segredos.                                                                                                                                                                                                                           |
| 26  | Helm √© considerado o "gerenciador de pacotes para Kubernetes", permitindo definir, instalar e atualizar aplica√ß√µes Kubernetes complexas atrav√©s de "charts".                                                                                                                                                                                                                                                             | V        | ‚úÖ Correto! Helm simplifica o gerenciamento de aplica√ß√µes no Kubernetes, empacotando todos os manifestos YAML necess√°rios e suas configura√ß√µes em um formato reutiliz√°vel (charts). üéÅ Helm = O "apt-get" ou "yum" do mundo Kubernetes.                                                                                                                                                                                                                                                                                                 |
| 27  | Os "Projetos" no Rancher s√£o uma abstra√ß√£o que agrupa m√∫ltiplos `Namespaces` do Kubernetes, facilitando a gest√£o de recursos e permiss√µes para equipes em um mesmo cluster.                                                                                                                                                                                                                                              | V        | ‚úÖ Verdadeiro! Projetos s√£o um conceito do Rancher que ajuda na organiza√ß√£o e no multitenancy dentro de um cluster Kubernetes gerenciado. üë• Projetos no Rancher = "Departamentos" dentro da "empresa" (cluster).                                                                                                                                                                                                                                                                                                                       |
| 28  | O comando `kubectl get pods --all-namespaces` lista todos os Pods em todos os `Namespaces` do cluster.                                                                                                                                                                                                                                                                                                                   | V        | ‚úÖ Correto! A flag `--all-namespaces` (ou sua forma curta `-A`) √© usada para visualizar recursos em todos os namespaces. üîç Dica de concurseiro: `-A` √© mais r√°pido de digitar que `--all-namespaces`!                                                                                                                                                                                                                                                                                                                                  |
| 29  | `NetworkPolicies` no Kubernetes s√£o aplicadas por padr√£o para bloquear todo o tr√°fego entre Pods, exigindo configura√ß√£o expl√≠cita para permitir a comunica√ß√£o.                                                                                                                                                                                                                                                           | F        | ‚ùå Falso! Por padr√£o, no Kubernetes (sem `NetworkPolicies` aplicadas), todo o tr√°fego entre Pods dentro do cluster √© permitido. `NetworkPolicies` s√£o opt-in; voc√™ as cria para *restringir* o tr√°fego. üö™ Por padr√£o, as portas est√£o abertas; `NetworkPolicy` = O seguran√ßa que controla o acesso.                                                                                                                                                                                                                                    |
| 30  | K3s √© uma distribui√ß√£o Kubernetes leve, totalmente compat√≠vel e certificada pela CNCF, otimizada para cen√°rios de borda, IoT e recursos limitados.                                                                                                                                                                                                                                                                       | V        | ‚úÖ Verdadeiro! K3s, desenvolvido pela Rancher Labs (agora SUSE), √© projetado para ser pequeno e f√°cil de operar, mantendo a conformidade com o Kubernetes. üéà K3s = O Kubernetes "compacto" para lugares apertados.                                                                                                                                                                                                                                                                                                                     |
| 31  | A arquitetura do Kubernetes √© composta por um √∫nico Master Node e m√∫ltiplos Worker Nodes, onde o Master Node n√£o pode ser configurado para alta disponibilidade.                                                                                                                                                                                                                                                         | F        | ‚ùå Errado! Embora possa funcionar com um Master, para produ√ß√£o e alta disponibilidade (HA), o Control Plane (Master) do Kubernetes √© tipicamente configurado com m√∫ltiplos Master Nodes. üè∞ Um castelo forte (produ√ß√£o) tem v√°rias torres de comando (Masters HA).                                                                                                                                                                                                                                                                      |
| 32  | `Taints` e `Tolerations` no Kubernetes trabalham juntos para garantir que Pods n√£o sejam agendados em n√≥s inadequados; `Taints` s√£o aplicados aos n√≥s e `Tolerations` aos Pods.                                                                                                                                                                                                                                          | V        | ‚úÖ Correto! N√≥s com `Taints` repelem Pods que n√£o t√™m `Tolerations` correspondentes. √â um mecanismo para restringir o agendamento de Pods em certos n√≥s. üö´ `Taint` no n√≥ = "N√£o entre a menos que..."; `Toleration` no Pod = "...voc√™ tenha esta chave!".                                                                                                                                                                                                                                                                              |
| 33  | O Rancher Server, componente central da plataforma Rancher, deve obrigatoriamente ser instalado em um cluster Kubernetes dedicado e n√£o pode rodar em um cluster j√° existente gerenciado pelo pr√≥prio Rancher.                                                                                                                                                                                                           | F        | ‚ùå Falso! O Rancher Server pode ser instalado em um cluster Kubernetes dedicado (recomendado para produ√ß√£o) ou at√© mesmo em um cluster K3s single-node para testes. Ele pode, inclusive, gerenciar o cluster onde ele mesmo est√° rodando (embora com cuidados). üè† O "gerente" pode morar em uma das "fazendas".                                                                                                                                                                                                                        |
| 34  | `ResourceQuotas` no Kubernetes permitem que administradores definam limites para o consumo total de recursos (CPU, mem√≥ria, storage) por `Namespace`.                                                                                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro! `ResourceQuotas` s√£o essenciais para o multitenancy e para evitar que um `Namespace` consuma todos os recursos do cluster. üí∞ `ResourceQuota` = O "or√ßamento" de recursos para cada time/projeto.                                                                                                                                                                                                                                                                                                                        |
| 35  | `kubectl exec -it my-pod -- /bin/sh` permite obter um shell interativo dentro de um cont√™iner espec√≠fico em um Pod chamado `my-pod`.                                                                                                                                                                                                                                                                                     | V        | ‚úÖ Correto! Este comando √© fundamental para depura√ß√£o e inspe√ß√£o de cont√™ineres em execu√ß√£o. O `-it` garante interatividade e aloca√ß√£o de TTY. üíª `kubectl exec` = A "chave mestra" para entrar nos seus cont√™ineres.                                                                                                                                                                                                                                                                                                                   |
| 36  | A descoberta de servi√ßo no Kubernetes √© primariamente realizada atrav√©s de vari√°veis de ambiente injetadas nos Pods, sendo este o m√©todo mais flex√≠vel e recomendado.                                                                                                                                                                                                                                                    | F        | ‚ùå Parcialmente falso! Vari√°veis de ambiente s√£o um m√©todo, mas a descoberta via DNS (para `Services`) √© geralmente mais flex√≠vel e recomendada, pois atualiza dinamicamente se os IPs dos Pods mudarem. üó∫Ô∏è DNS = O "mapa" atualizado dos servi√ßos; Vari√°veis de ambiente = Um "endere√ßo anotado" que pode desatualizar.                                                                                                                                                                                                               |
| 37  | Um `Job` no Kubernetes cria um ou mais Pods e garante que um n√∫mero especificado deles termine com sucesso, sendo ideal para tarefas em lote (batch).                                                                                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro! `Jobs` s√£o para cargas de trabalho que rodam at√© a conclus√£o, diferentemente de `Deployments` que mant√™m Pods rodando indefinidamente. üèÅ `Job` = Uma "tarefa com come√ßo, meio e fim".                                                                                                                                                                                                                                                                                                                                   |
| 38  | O Cat√°logo de Aplica√ß√µes do Rancher utiliza exclusivamente formatos propriet√°rios para empacotar aplica√ß√µes, n√£o sendo compat√≠vel com Helm charts.                                                                                                                                                                                                                                                                       | F        | ‚ùå Errado! O Cat√°logo do Rancher √© amplamente baseado em Helm charts, que √© o padr√£o da comunidade Kubernetes para empacotamento de aplica√ß√µes. üìö Cat√°logo Rancher = Uma "biblioteca" cheia de "livros" (Helm charts).                                                                                                                                                                                                                                                                                                                 |
| 39  | Um `CronJob` no Kubernetes cria `Jobs` em uma programa√ß√£o recorrente, similar ao `cron` em sistemas Linux.                                                                                                                                                                                                                                                                                                               | V        | ‚úÖ Correto! `CronJobs` s√£o perfeitos para tarefas agendadas, como backups, relat√≥rios ou outras manuten√ß√µes peri√≥dicas. ‚è∞ `CronJob` = O "despertador" do Kubernetes para seus `Jobs`.                                                                                                                                                                                                                                                                                                                                                   |
| 40  | A remo√ß√£o de um `Deployment` no Kubernetes automaticamente remove os `PersistentVolumeClaims` (PVCs) que foram criados e utilizados pelos Pods daquele `Deployment`.                                                                                                                                                                                                                                                     | F        | ‚ùå Falso! Por padr√£o, PVCs n√£o s√£o removidos quando o `Deployment` (ou `StatefulSet`) que os utilizava √© deletado. Isso √© uma medida de seguran√ßa para evitar perda de dados. A remo√ß√£o do PVC deve ser expl√≠cita. üíæ Dados persistentes s√£o "teimosos" e n√£o somem f√°cil!                                                                                                                                                                                                                                                              |
| 41  | O RBAC (Role-Based Access Control) no Kubernetes permite definir permiss√µes granulares para usu√°rios e contas de servi√ßo interagirem com recursos da API do K8s.                                                                                                                                                                                                                                                         | V        | ‚úÖ Verdadeiro! RBAC √© o mecanismo padr√£o para autoriza√ß√£o no Kubernetes, usando `Roles`, `ClusterRoles`, `RoleBindings` e `ClusterRoleBindings`. üîë RBAC = O "porteiro" que decide quem pode fazer o qu√™.                                                                                                                                                                                                                                                                                                                               |
| 42  | Ao escalar um `Deployment` para zero r√©plicas (`kubectl scale deployment my-app --replicas=0`), todos os `Services` associados a ele tamb√©m s√£o automaticamente deletados.                                                                                                                                                                                                                                               | F        | ‚ùå Falso! Escalar um `Deployment` para zero r√©plicas remove os Pods, mas o `Service` continua existindo (embora n√£o tenha endpoints para rotear tr√°fego). O `Service` precisa ser deletado separadamente. üëª `Service` sem Pods = Um "endere√ßo fantasma".                                                                                                                                                                                                                                                                               |
| 43  | O comando `kubectl logs my-pod -c my-container --previous` exibe os logs da inst√¢ncia anterior de um cont√™iner reiniciado dentro de um Pod.                                                                                                                                                                                                                                                                              | V        | ‚úÖ Correto! A flag `--previous` (ou `-p`) √© muito √∫til para depurar por que um cont√™iner falhou e foi reiniciado. üìú `--previous` = "Me mostre o di√°rio da vida passada do cont√™iner".                                                                                                                                                                                                                                                                                                                                                  |
| 44  | Rancher e Kubernetes s√£o tecnologias mutuamente exclusivas; uma organiza√ß√£o deve escolher usar uma ou outra, mas n√£o ambas em conjunto.                                                                                                                                                                                                                                                                                  | F        | ‚ùå Totalmente Falso! Rancher √© uma plataforma de gerenciamento *para* Kubernetes. Eles s√£o projetados para trabalhar juntos, com Rancher simplificando a opera√ß√£o de clusters Kubernetes. ü§ù Eles s√£o parceiros, n√£o rivais!                                                                                                                                                                                                                                                                                                            |
| 45  | Em um cluster Kubernetes com v√°rios n√≥s, se um n√≥ falhar, o Kubernetes automaticamente tentar√° reagendar os Pods que estavam naquele n√≥ em outros n√≥s saud√°veis, desde que os Pods sejam gerenciados por um controller como um `Deployment`.                                                                                                                                                                             | V        | ‚úÖ Verdadeiro! Esta √© uma das principais caracter√≠sticas de autocorre√ß√£o (self-healing) do Kubernetes, garantindo alta disponibilidade. üöë K8s = O "param√©dico" dos seus Pods.                                                                                                                                                                                                                                                                                                                                                          |
| 46  | O uso de `latest` como tag para imagens de cont√™iner em `Deployments` de produ√ß√£o √© uma pr√°tica recomendada, pois garante que a aplica√ß√£o esteja sempre atualizada.                                                                                                                                                                                                                                                      | F        | ‚ùå Errado! Usar a tag `latest` em produ√ß√£o √© arriscado, pois pode levar a implanta√ß√µes inesperadas de novas vers√µes n√£o testadas, quebrando o princ√≠pio da imutabilidade e dificultando rollbacks precisos. Sempre use tags espec√≠ficas (ex: `meu-app:v1.2.3`). üè∑Ô∏è `latest` = Roleta russa para produ√ß√£o.                                                                                                                                                                                                                              |
| 47  | `Operators` no Kubernetes s√£o uma forma de estender a API do K8s para criar, configurar e gerenciar inst√¢ncias de aplica√ß√µes complexas e stateful de forma automatizada.                                                                                                                                                                                                                                                 | V        | ‚úÖ Verdadeiro! `Operators` encapsulam o conhecimento operacional de uma aplica√ß√£o, automatizando tarefas que normalmente exigiriam um humano. ü§ñ `Operator` = Um "administrador de sistemas em software" para sua aplica√ß√£o.                                                                                                                                                                                                                                                                                                            |
| 48  | O comando `kubectl describe pod my-pod` mostra informa√ß√µes detalhadas sobre um Pod, incluindo seus cont√™ineres, volumes, eventos recentes e condi√ß√µes, sendo √∫til para troubleshooting.                                                                                                                                                                                                                                  | V        | ‚úÖ Correto! `describe` √© um dos comandos mais importantes para entender o estado e os problemas de um recurso Kubernetes. üïµÔ∏è `kubectl describe` = A "lupa de detetive" para inspecionar seus recursos.                                                                                                                                                                                                                                                                                                                                 |
| 49  | Rancher Multi-Cluster App Management permite implantar e gerenciar aplica√ß√µes Helm em m√∫ltiplos clusters Kubernetes de forma centralizada.                                                                                                                                                                                                                                                                               | V        | ‚úÖ Verdadeiro! Esta funcionalidade do Rancher √© poderosa para garantir consist√™ncia e simplificar a gest√£o de aplica√ß√µes em ambientes distribu√≠dos. üöÄ Rancher Multi-Cluster Apps = "Lan√ßamento coordenado" de apps em toda a sua frota de clusters.                                                                                                                                                                                                                                                                                    |
| 50  | Para garantir a seguran√ßa em um cluster Kubernetes, basta configurar `NetworkPolicies` robustas, n√£o sendo necess√°rio se preocupar com o RBAC ou a seguran√ßa das imagens de cont√™iner.                                                                                                                                                                                                                                   | F        | ‚ùå Falso! Seguran√ßa em Kubernetes √© uma abordagem em camadas (defense in depth). `NetworkPolicies` s√£o uma parte, mas RBAC, seguran√ßa de imagens (vulnerabilidades, provenance), Pod Security Admission/Policies, segredos, e seguran√ßa do n√≥ host s√£o igualmente cruciais. üõ°Ô∏è Seguran√ßa K8s = Cebola (muitas camadas!).                                                                                                                                                                                                               |
| 51  | Um `Ingress` no Kubernetes √© um objeto que gerencia o acesso externo aos servi√ßos em um cluster, tipicamente HTTP e HTTPS, podendo fornecer balanceamento de carga, termina√ß√£o SSL e roteamento baseado em nome/host.                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro! `Ingress` √© como o porteiro inteligente da sua aplica√ß√£o web no Kubernetes. Ele recebe o tr√°fego de fora e o direciona para o servi√ßo certo, baseado em regras como o endere√ßo que voc√™ digitou no navegador (ex: `meusite.com/app1`). üö¶ `Ingress` = O "controlador de tr√°fego avan√ßado" para web.                                                                                                                                                                                                                      |
| 52  | O `kubelet`, rodando em cada n√≥ master, √© o componente prim√°rio que interage com o `etcd` para persistir o estado desejado dos recursos do cluster.                                                                                                                                                                                                                                                                      | F        | ‚ùå Falso! O `kubelet` roda nos *Worker Nodes* e garante que os cont√™ineres descritos nos Pods estejam rodando e saud√°veis *naquele n√≥*. Quem interage primariamente com o `etcd` (via `kube-apiserver`) para ler/escrever o estado √© o `kube-apiserver` e os controllers no *Control Plane*. üë∑ `kubelet` = O "capataz" em cada n√≥ trabalhador.                                                                                                                                                                                         |
| 53  | `Labels` s√£o pares chave/valor anexados a objetos Kubernetes, como Pods e Services, usados para organizar e selecionar subconjuntos de objetos.                                                                                                                                                                                                                                                                          | V        | ‚úÖ Correto! `Labels` s√£o como etiquetas que voc√™ coloca nos seus objetos K8s. Eles n√£o t√™m significado direto para o sistema, mas s√£o cruciais para que `Selectors` (usados por Services, Deployments, etc.) encontrem e agrupem os objetos certos. üè∑Ô∏è `Labels` = As "etiquetas organizadoras" do seu invent√°rio K8s.                                                                                                                                                                                                                  |
| 54  | `Annotations` no Kubernetes s√£o funcionalmente id√™nticas aos `Labels`, sendo usadas primariamente para selecionar objetos para opera√ß√µes.                                                                                                                                                                                                                                                                                | F        | ‚ùå Falso! Enquanto `Labels` s√£o para identificar e selecionar objetos, `Annotations` s√£o para metadados *n√£o identificadores* que podem ser usados por ferramentas ou bibliotecas externas (ex: descri√ß√£o, contato do dono, informa√ß√µes para um dashboard). üìù `Annotations` = "Notas de rodap√©" ou "informa√ß√µes extras" nos objetos.                                                                                                                                                                                                   |
| 55  | Um `Service` do tipo `NodePort` exp√µe o servi√ßo em uma porta est√°tica em cada IP de N√≥ do cluster. Isso permite acessar o servi√ßo externamente usando `<NodeIP>:<NodePort>`.                                                                                                                                                                                                                                             | V        | ‚úÖ Verdadeiro! `NodePort` abre uma porta espec√≠fica em *todos* os seus n√≥s. √ötil para expor servi√ßos quando um `LoadBalancer` n√£o est√° dispon√≠vel ou n√£o √© desejado. üö™ `NodePort` = Uma "porta dos fundos" em cada casa (n√≥) do seu condom√≠nio (cluster).                                                                                                                                                                                                                                                                              |
| 56  | O Rancher, ao gerenciar um cluster Kubernetes importado (como um EKS da AWS), assume controle total sobre o Control Plane do cluster EKS, substituindo os componentes da AWS.                                                                                                                                                                                                                                            | F        | ‚ùå Errado! Ao importar um cluster gerenciado como EKS, o Rancher atua como uma camada de gerenciamento *sobre* ele. O Control Plane do EKS continua sendo gerenciado pela AWS. O Rancher interage com a API do EKS para facilitar opera√ß√µes. ü§ù Rancher e EKS = Colabora√ß√£o, n√£o substitui√ß√£o do controle da AWS.                                                                                                                                                                                                                       |
| 57  | O comando `kubectl taint nodes nome-do-no chave=valor:NoSchedule` impede que novos Pods sejam agendados no `nome-do-no`, a menos que tenham uma `Toleration` correspondente.                                                                                                                                                                                                                                             | V        | ‚úÖ Correto! `Taints` "mancham" um n√≥, e `Tolerations` permitem que Pods "tolerem" essas manchas. `NoSchedule` significa que nenhum novo Pod ser√° agendado l√°, mas os existentes continuam rodando. üõë `Taint` com `NoSchedule` = "Vaga reservada, n√£o estacione aqui sem permiss√£o (Toleration)".                                                                                                                                                                                                                                       |
| 58  | Um `StorageClass` no Kubernetes permite o provisionamento din√¢mico de `PersistentVolumes` (PVs), abstraindo os detalhes do sistema de armazenamento subjacente.                                                                                                                                                                                                                                                          | V        | ‚úÖ Verdadeiro! Com `StorageClasses`, voc√™ define "tipos" de armazenamento (ex: "r√°pido-ssd", "backup-lento"), e quando um `PersistentVolumeClaim` (PVC) solicita um tipo, o K8s (ou um plugin de storage) pode criar um PV automaticamente. üì¶ `StorageClass` = O "cat√°logo de op√ß√µes de disco" para provisionamento sob demanda.                                                                                                                                                                                                       |
| 59  | Todos os cont√™ineres dentro de um mesmo Pod no Kubernetes compartilham o mesmo endere√ßo IP e namespace de rede, podendo se comunicar via `localhost`.                                                                                                                                                                                                                                                                    | V        | ‚úÖ Correto! Esta √© uma caracter√≠stica fundamental dos Pods. M√∫ltiplos cont√™ineres em um Pod s√£o como processos rodando na mesma "m√°quina" (o Pod), facilitando a comunica√ß√£o e o compartilhamento de volumes. üîó Cont√™ineres no Pod = Colegas de quarto compartilhando o mesmo endere√ßo e rede Wi-Fi.                                                                                                                                                                                                                                   |
| 60  | O `kube-controller-manager` executa um √∫nico processo de controlador que gerencia todos os aspectos do cluster, como n√≥s, r√©plicas e endpoints.                                                                                                                                                                                                                                                                          | F        | ‚ùå Falso! O `kube-controller-manager` executa *m√∫ltiplos* processos de controlador (Node Controller, Replication Controller/ReplicaSet, Endpoints Controller, Service Account & Token Controllers, etc.). Cada um √© um loop de reconcilia√ß√£o para um recurso espec√≠fico. üõ†Ô∏è `kube-controller-manager` = Uma "caixa de ferramentas" com v√°rios controladores especializados.                                                                                                                                                            |
| 61  | A autentica√ß√£o de usu√°rios no Rancher pode ser integrada com provedores de identidade externos como Active Directory, LDAP ou GitHub.                                                                                                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro! Esta √© uma funcionalidade chave do Rancher para ambientes corporativos, permitindo usar sistemas de autentica√ß√£o j√° existentes. üîë Rancher Auth = "Portaria central" que pode se conectar com seu sistema de crach√°s (AD, LDAP).                                                                                                                                                                                                                                                                                         |
| 62  | `Init Containers` em um Pod s√£o cont√™ineres que devem ser executados e conclu√≠dos com sucesso, na ordem especificada, antes que os cont√™ineres principais da aplica√ß√£o iniciem.                                                                                                                                                                                                                                          | V        | ‚úÖ Correto! `Init Containers` s√£o perfeitos para tarefas de setup, como esperar por um banco de dados, baixar configura√ß√µes, ou preparar o ambiente. üö¶ `Init Containers` = A "equipe de prepara√ß√£o" que arruma o palco antes do show principal (cont√™ineres da app).                                                                                                                                                                                                                                                                   |
| 63  | Se um `Liveness Probe` de um cont√™iner falhar repetidamente, o `kubelet` ir√° marcar o Pod como "N√£o Pronto" (NotReady) e o `Service` parar√° de enviar tr√°fego para ele.                                                                                                                                                                                                                                                  | F        | ‚ùå Errado! Se um `Liveness Probe` falha, o `kubelet` *reinicia o cont√™iner*. √â o `Readiness Probe` que, ao falhar, faz o Pod ser marcado como "N√£o Pronto" e removido dos endpoints do `Service`. üîÑ Falha no `Liveness` = Reiniciar; üö´ Falha no `Readiness` = Parar tr√°fego.                                                                                                                                                                                                                                                          |
| 64  | Um `ReplicaSet` no Kubernetes tem como objetivo principal garantir que um n√∫mero especificado de r√©plicas de Pods esteja sempre em execu√ß√£o.                                                                                                                                                                                                                                                                             | V        | ‚úÖ Verdadeiro! Essa √© a fun√ß√£o central do `ReplicaSet`. Se um Pod gerenciado por ele falhar ou for deletado, o `ReplicaSet` criar√° um novo para manter a contagem desejada. üëØ `ReplicaSet` = O "xerife" que garante o n√∫mero certo de "deputados" (Pods).                                                                                                                                                                                                                                                                              |
| 65  | O Rancher s√≥ pode ser instalado em clusters Kubernetes provisionados pelo pr√≥prio Rancher (RKE ou K3s).                                                                                                                                                                                                                                                                                                                  | F        | ‚ùå Falso! O Rancher Server (a aplica√ß√£o de gerenciamento) pode ser instalado em qualquer cluster Kubernetes certificado pela CNCF, incluindo GKE, EKS, AKS, ou at√© mesmo um `kind` ou `minikube` para testes. üè° O Rancher pode ser "hospedado" em v√°rios tipos de "terrenos" K8s.                                                                                                                                                                                                                                                      |
| 66  | `EndpointSlices` s√£o uma otimiza√ß√£o no Kubernetes para `Endpoints`, permitindo melhor escalabilidade e desempenho para `Services` que apontam para um grande n√∫mero de Pods.                                                                                                                                                                                                                                             | V        | ‚úÖ Correto! `Endpoints` tradicionais podiam ficar muito grandes. `EndpointSlices` dividem essa informa√ß√£o em peda√ßos menores, melhorando a performance da atualiza√ß√£o e propaga√ß√£o desses dados no cluster. üç∞ `EndpointSlices` = Fatias do "bolo" de Endpoints para facilitar a digest√£o pelo sistema.                                                                                                                                                                                                                                 |
| 67  | O comando `kubectl port-forward deployment/meu-deploy 8080:80` permite acessar a porta 80 do primeiro Pod do `meu-deploy` atrav√©s da `localhost:8080` na sua m√°quina local.                                                                                                                                                                                                                                              | V        | ‚úÖ Verdadeiro! `port-forward` √© uma ferramenta muito √∫til para acessar e depurar aplica√ß√µes rodando no cluster diretamente da sua m√°quina, sem precisar expor um `Service` externamente. üöá `port-forward` = Um "t√∫nel direto" para dentro do seu Pod.                                                                                                                                                                                                                                                                                  |
| 68  | `PodDisruptionBudgets` (PDBs) s√£o usados para especificar o n√∫mero m√°ximo de Pods de uma aplica√ß√£o que podem ficar indispon√≠veis voluntariamente ao mesmo tempo devido a opera√ß√µes como upgrades de n√≥s.                                                                                                                                                                                                                 | V        | ‚úÖ Verdadeiro! PDBs s√£o cruciais para manter a alta disponibilidade durante manuten√ß√µes planejadas. Voc√™ diz, por exemplo, "minha aplica√ß√£o precisa de pelo menos 2 r√©plicas rodando". üõ°Ô∏è PDB = O "escudo de disponibilidade" da sua aplica√ß√£o durante manuten√ß√µes.                                                                                                                                                                                                                                                                    |
| 69  | O `cloud-controller-manager` √© um componente opcional do Kubernetes que roda no Control Plane e cont√©m l√≥gicas de controle espec√≠ficas de um provedor de nuvem.                                                                                                                                                                                                                                                          | V        | ‚úÖ Correto! Ele permite que o Kubernetes interaja com a API do provedor de nuvem para recursos como `LoadBalancers`, `Nodes` (informa√ß√µes da VM) e `Storage`. ‚òÅÔ∏è `cloud-controller-manager` = O "embaixador" do Kubernetes no provedor de nuvem.                                                                                                                                                                                                                                                                                        |
| 70  | Alterar a imagem de um cont√™iner em um manifesto de `Deployment` e aplicar com `kubectl apply -f` resulta em uma atualiza√ß√£o do tipo "recreate", onde todos os Pods antigos s√£o terminados antes dos novos serem criados.                                                                                                                                                                                                | F        | ‚ùå Falso! Por padr√£o, `Deployments` usam a estrat√©gia `RollingUpdate`. Pods antigos s√£o substitu√≠dos gradualmente pelos novos, garantindo que a aplica√ß√£o permane√ßa dispon√≠vel durante a atualiza√ß√£o. `Recreate` √© uma estrat√©gia alternativa que pode ser configurada. üåä `RollingUpdate` = Trocar os pneus com o carro andando (com cuidado!).                                                                                                                                                                                        |
| 71  | O `Vertical Pod Autoscaler` (VPA) ajusta automaticamente os `requests` de CPU e mem√≥ria dos cont√™ineres em um Pod, ajudando a otimizar o uso de recursos.                                                                                                                                                                                                                                                                | V        | ‚úÖ Verdadeiro! Enquanto o HPA (Horizontal Pod Autoscaler) muda o *n√∫mero* de Pods, o VPA muda o *tamanho* (recursos) dos Pods. Eles podem, em alguns casos, ser usados juntos. üìè VPA = O "alfaiate" que ajusta o "tamanho da roupa" (recursos) dos seus Pods.                                                                                                                                                                                                                                                                          |
| 72  | K3s, sendo uma distribui√ß√£o leve de Kubernetes, remove componentes essenciais do Control Plane como o `etcd` para reduzir seu footprint.                                                                                                                                                                                                                                                                                 | F        | ‚ùå Falso! K3s substitui o `etcd` por `SQLite` (por padr√£o, mas pode usar `etcd` ou outros) para simplificar e reduzir o footprint, mas ainda precisa de um backend de armazenamento para o estado do cluster. Ele tamb√©m condensa outros bin√°rios, mas mant√©m a funcionalidade essencial do K8s. üí° K3s: Leve, mas funcional!                                                                                                                                                                                                           |
| 73  | Um `ServiceAccount` no Kubernetes fornece uma identidade para processos que rodam dentro de Pods, permitindo que eles interajam autenticadamente com o `kube-apiserver`.                                                                                                                                                                                                                                                 | V        | ‚úÖ Correto! Quando seus Pods precisam acessar a API do K8s (ex: para listar outros Pods), eles usam um `ServiceAccount` e seu token associado. ü§ñ `ServiceAccount` = O "crach√° de identifica√ß√£o" para seus Pods se comunicarem com o chefe (`kube-apiserver`).                                                                                                                                                                                                                                                                          |
| 74  | O `Scheduler` do Kubernetes apenas considera os `requests` de recursos (CPU/mem√≥ria) ao decidir onde agendar um Pod, ignorando `taints`, `tolerations` ou `affinity rules`.                                                                                                                                                                                                                                              | F        | ‚ùå Errado! O `Scheduler` considera m√∫ltiplos fatores: `requests` de recursos, `taints/tolerations`, `node selectors`, `affinity/anti-affinity rules`, e outras pol√≠ticas para tomar a melhor decis√£o de agendamento. üß† `Scheduler` = Um "estrategista" complexo, n√£o um simples alocador de recursos.                                                                                                                                                                                                                                  |
| 75  | O Rancher CLI permite gerenciar clusters e recursos do Rancher via linha de comando, oferecendo uma alternativa √† UI web.                                                                                                                                                                                                                                                                                                | V        | ‚úÖ Verdadeiro! Para quem prefere terminais ou precisa automatizar tarefas, o Rancher CLI √© uma ferramenta poderosa. üñ•Ô∏è Rancher CLI = "Controle remoto via texto" para seu Rancher.                                                                                                                                                                                                                                                                                                                                                     |
| 76  | `Startup Probes` s√£o usadas para verificar se uma aplica√ß√£o dentro de um cont√™iner iniciou corretamente, especialmente para aplica√ß√µes legadas ou que demoram muito para iniciar, antes que `Liveness Probes` comecem a atuar.                                                                                                                                                                                           | V        | ‚úÖ Correto! Se uma app demora muito para iniciar, `Liveness Probes` poderiam mat√°-la prematuramente. `Startup Probes` d√£o um tempo extra para a inicializa√ß√£o antes de passar o bast√£o para as `Liveness Probes`. üèÅ `Startup Probe` = O "juiz de largada" que espera a app ficar realmente pronta.                                                                                                                                                                                                                                     |
| 77  | Ao importar um cluster para o Rancher, todos os workloads existentes no cluster s√£o automaticamente migrados para usar o cat√°logo de aplica√ß√µes do Rancher.                                                                                                                                                                                                                                                              | F        | ‚ùå Falso! Importar um cluster para o Rancher permite *gerenci√°-lo* atrav√©s do Rancher. Workloads existentes continuam rodando como est√£o. Voc√™ *pode* ent√£o usar o cat√°logo para novas implanta√ß√µes ou para gerenciar/atualizar apps existentes se elas forem compat√≠veis com Helm. üö¢ Importar ‚â† Transformar tudo.                                                                                                                                                                                                                     |
| 78  | `CustomResourceDefinitions` (CRDs) permitem estender a API do Kubernetes com seus pr√≥prios tipos de objetos, que podem ser gerenciados com `kubectl` como os recursos nativos.                                                                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro! CRDs s√£o a base do `Operator Pattern`. Se voc√™ tem um tipo de aplica√ß√£o com l√≥gica de gerenciamento pr√≥pria, pode criar um CRD (ex: `Kind: MeuBancoDeDados`) e um `Operator` para gerenci√°-lo. üß© CRD = "Pe√ßas de Lego customizadas" para construir sua pr√≥pria API K8s.                                                                                                                                                                                                                                                 |
| 79  | O `kube-apiserver` √© o √∫nico componente do Kubernetes que se comunica diretamente com o `etcd`. Todos os outros componentes acessam o `etcd` atrav√©s do `kube-apiserver`.                                                                                                                                                                                                                                                | V        | ‚úÖ Correto! Esta arquitetura centraliza o acesso e a valida√ß√£o dos dados no `etcd`, tornando o `kube-apiserver` o gateway para o estado do cluster. üõ°Ô∏è `kube-apiserver` = O "guardi√£o do cofre" (`etcd`).                                                                                                                                                                                                                                                                                                                              |
| 80  | Rancher utiliza Prometheus e Grafana por padr√£o para fornecer monitoramento e dashboards para os clusters Kubernetes que gerencia.                                                                                                                                                                                                                                                                                       | V        | ‚úÖ Verdadeiro! Rancher facilita a implanta√ß√£o e integra√ß√£o dessas ferramentas populares de monitoramento, oferecendo visibilidade sobre a sa√∫de e performance dos clusters e aplica√ß√µes. üìä Rancher + Prometheus/Grafana = "Painel de controle com todos os indicadores vitais" do seu cluster.                                                                                                                                                                                                                                         |
| 81  | `Affinity` e `Anti-Affinity` no Kubernetes permitem influenciar as decis√µes de agendamento de Pods, especificando prefer√™ncias ou requisitos para que Pods sejam (ou n√£o) colocados juntos no mesmo n√≥, zona ou regi√£o.                                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro! `Node Affinity` atrai Pods para n√≥s com certos `labels`. `Pod Affinity` atrai Pods para n√≥s onde outros Pods (com certos `labels`) j√° est√£o rodando. `Anti-Affinity` faz o oposto. üß≤ `Affinity` = "√çm√£s" para juntar ou separar seus Pods nos n√≥s.                                                                                                                                                                                                                                                                      |
| 82  | O comando `kubectl rollout undo deployment/meu-app` reverte o `Deployment` `meu-app` para a pen√∫ltima revis√£o bem-sucedida.                                                                                                                                                                                                                                                                                              | V        | ‚úÖ Correto! `rollout undo` √© o comando para reverter uma implanta√ß√£o. Por padr√£o, volta para a vers√£o anterior. Voc√™ pode especificar `--to-revision=<numero>` para ir para uma revis√£o espec√≠fica. ‚è™ `rollout undo` = O "bot√£o de desfazer" para seus `Deployments`.                                                                                                                                                                                                                                                                   |
| 83  | Apenas Pods no mesmo `Namespace` podem se comunicar entre si; a comunica√ß√£o entre `Namespaces` √© bloqueada por padr√£o pelo Kubernetes.                                                                                                                                                                                                                                                                                   | F        | ‚ùå Falso! Por padr√£o, todos os Pods em um cluster Kubernetes podem se comunicar entre si, independentemente do `Namespace`, desde que tenham os endere√ßos IP uns dos outros (geralmente via `Services`). `NetworkPolicies` s√£o usadas para *restringir* essa comunica√ß√£o. üåê Por padr√£o, rede aberta no cluster.                                                                                                                                                                                                                        |
| 84  | O Rancher pode gerenciar clusters Kubernetes que utilizam diferentes Container Network Interfaces (CNIs), como Calico, Flannel ou Canal.                                                                                                                                                                                                                                                                                 | V        | ‚úÖ Verdadeiro! O Rancher √© agn√≥stico em rela√ß√£o ao CNI usado pelo cluster Kubernetes, desde que o cluster seja funcional e certificado pela CNCF. O CNI √© uma responsabilidade do pr√≥prio K8s. üîå Rancher = Compat√≠vel com "diferentes provedores de rede" (CNIs) dos seus clusters.                                                                                                                                                                                                                                                    |
| 85  | `Secrets` no Kubernetes, quando montados como volumes em Pods, s√£o atualizados automaticamente nos cont√™ineres se o `Secret` original for alterado no `etcd`.                                                                                                                                                                                                                                                            | V        | ‚úÖ Correto! O `kubelet` monitora os `Secrets` montados e atualiza os arquivos no Pod quando o `Secret` √© modificado. No entanto, a aplica√ß√£o dentro do cont√™iner precisa ser capaz de recarregar essa configura√ß√£o. üîÑ `Secrets` montados = "Arquivos vivos" que refletem mudan√ßas.                                                                                                                                                                                                                                                     |
| 86  | `RKE` (Rancher Kubernetes Engine) √© uma ferramenta CLI que o Rancher usa para provisionar e gerenciar o ciclo de vida de clusters Kubernetes em infraestrutura pr√≥pria (bare-metal ou VMs).                                                                                                                                                                                                                              | V        | ‚úÖ Verdadeiro! `RKE` √© a distribui√ß√£o e ferramenta do Rancher para instalar e manter clusters K8s em seus pr√≥prios servidores, cuidando da instala√ß√£o do Docker, componentes K8s, upgrades, etc. üèóÔ∏è `RKE` = O "construtor de clusters K8s" do Rancher para seu data center.                                                                                                                                                                                                                                                            |
| 87  | Um `HorizontalPodAutoscaler` (HPA) s√≥ pode escalar um `Deployment` com base na utiliza√ß√£o m√©dia de CPU de seus Pods.                                                                                                                                                                                                                                                                                                     | F        | ‚ùå Falso! O HPA pode escalar com base na CPU, mem√≥ria e tamb√©m em m√©tricas customizadas (ex: itens em uma fila, requisi√ß√µes por segundo) expostas via `custom.metrics.k8s.io` ou `external.metrics.k8s.io` API. üéØ HPA = Pode mirar em v√°rios "alvos" (m√©tricas) para escalar.                                                                                                                                                                                                                                                          |
| 88  | A interface de usu√°rio (UI) do Rancher permite executar um shell `kubectl` diretamente no navegador para interagir com qualquer cluster gerenciado.                                                                                                                                                                                                                                                                      | V        | ‚úÖ Verdadeiro! Esta √© uma funcionalidade muito conveniente do Rancher, permitindo acesso `kubectl` sem precisar configurar o `kubeconfig` localmente para cada cluster. üêö UI Rancher com `kubectl` = "Terminal K8s na nuvem (do Rancher)".                                                                                                                                                                                                                                                                                             |
| 89  | Um `Pod` √© removido de um `Service` (deixa de receber tr√°fego) apenas quando o `Pod` √© explicitamente deletado.                                                                                                                                                                                                                                                                                                          | F        | ‚ùå Errado! Um `Pod` tamb√©m √© removido dos endpoints de um `Service` se sua `Readiness Probe` falhar ou se ele estiver no estado `Terminating` (desligando). O `Service` s√≥ envia tr√°fego para Pods saud√°veis e prontos. üö¶ `Service` = S√≥ envia tr√°fego para quem est√° "no sinal verde".                                                                                                                                                                                                                                                |
| 90  | O cat√°logo de aplica√ß√µes globais do Rancher √© fixo e n√£o pode ser estendido com reposit√≥rios Helm customizados.                                                                                                                                                                                                                                                                                                          | F        | ‚ùå Falso! O Rancher permite adicionar seus pr√≥prios reposit√≥rios Helm (ou de terceiros) ao cat√°logo, tornando suas aplica√ß√µes customizadas ou ferramentas espec√≠ficas facilmente dispon√≠veis para implanta√ß√£o via UI. ‚ûï Cat√°logo Rancher = Expans√≠vel com suas "pr√≥prias prateleiras de apps".                                                                                                                                                                                                                                          |
| 91  | `Labels` e `Selectors` s√£o o mecanismo prim√°rio pelo qual um `Service` no Kubernetes identifica quais Pods devem receber tr√°fego.                                                                                                                                                                                                                                                                                        | V        | ‚úÖ Correto! O `selector` no `Service` define quais `labels` os Pods devem ter para serem inclu√≠dos como backends (endpoints) daquele `Service`. √â o "elo" fundamental. üéØ `Service Selector` + `Pod Label` = Combina√ß√£o perfeita!                                                                                                                                                                                                                                                                                                       |
| 92  | `PersistentVolumeClaims` (PVCs) s√£o namespaced, o que significa que um PVC em um `Namespace` "dev" n√£o pode acessar um `PersistentVolume` (PV) que foi vinculado a um PVC no `Namespace` "prod".                                                                                                                                                                                                                         | V        | ‚úÖ Verdadeiro! PVCs s√£o recursos namespaced, e um PV s√≥ pode ser vinculado a um PVC por vez. Isso ajuda no isolamento e gerenciamento de storage por `Namespace`. üìÅ PVCs vivem "dentro" de `Namespaces`.                                                                                                                                                                                                                                                                                                                               |
| 93  | O Rancher depende exclusivamente do `etcd` como backend de armazenamento para sua pr√≥pria configura√ß√£o e estado.                                                                                                                                                                                                                                                                                                         | F        | ‚ùå Falso! O servidor Rancher em si roda *dentro* de um cluster Kubernetes (que pode ser RKE, K3s, EKS, etc.). A configura√ß√£o do *Rancher Server* √© armazenada no `etcd` (ou backend alternativo como SQLite no caso do K3s) *desse cluster Kubernetes onde o Rancher est√° instalado*. üß† Rancher usa o "c√©rebro" do K8s onde ele mora.                                                                                                                                                                                                  |
| 94  | `Taints` e `Tolerations` s√£o usados para controle de acesso e autentica√ß√£o no Kubernetes, similar ao RBAC.                                                                                                                                                                                                                                                                                                               | F        | ‚ùå Errado! `Taints` e `Tolerations` s√£o para influenciar o *agendamento* de Pods em n√≥s. RBAC (Roles, RoleBindings, etc.) √© para controle de acesso e autoriza√ß√£o (o que um usu√°rio/SA pode *fazer* com os recursos da API). üóìÔ∏è Taints/Tolerations = Onde o Pod pode ir; üîë RBAC = O que o Pod/usu√°rio pode fazer.                                                                                                                                                                                                                     |
| 95  | O `API Server` do Kubernetes √© stateless; todo o estado do cluster √© mantido exclusivamente no `etcd`.                                                                                                                                                                                                                                                                                                                   | V        | ‚úÖ Verdadeiro! O `API Server` processa requisi√ß√µes, valida-as, e persiste/recupera dados do `etcd`. Ele pode ser escalado horizontalmente porque n√£o mant√©m estado pr√≥prio. ‚òÅÔ∏è `API Server` = Um "atendente eficiente" que consulta o "grande livro de registros" (`etcd`).                                                                                                                                                                                                                                                             |
| 96  | O comando `kubectl diff -f meu-manifesto.yaml` mostra as diferen√ßas entre o estado definido no arquivo `meu-manifesto.yaml` e o estado atual dos recursos no cluster, sem aplicar as mudan√ßas.                                                                                                                                                                                                                           | V        | ‚úÖ Correto! `kubectl diff` √© uma ferramenta excelente para "prever" o que `kubectl apply` faria, permitindo revisar as mudan√ßas antes de efetiv√°-las. üëÄ `kubectl diff` = "Espelho, espelho meu, o que vai mudar se eu aplicar isso a√≠?".                                                                                                                                                                                                                                                                                               |
| 97  | K3s inclui um `Service Load Balancer` embutido chamado Klipper (baseado no `kube-vip` ou `servicelb`) que permite criar `Services` do tipo `LoadBalancer` mesmo em ambientes bare-metal sem um balanceador de carga externo.                                                                                                                                                                                             | V        | ‚úÖ Verdadeiro! Esta √© uma das funcionalidades que tornam o K3s muito pr√°tico para desenvolvimento e ambientes on-premises, simulando a funcionalidade de `LoadBalancer` de nuvem.  –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫ (balansirovshchik - balanceador em russo, para variar o emoji) Klipper = "Seu pr√≥prio balanceador de carga" no K3s.                                                                                                                                                                                                                    |
| 98  | Rancher n√£o oferece suporte para gerenciamento de clusters Kubernetes rodando em arquiteturas ARM (como Raspberry Pi).                                                                                                                                                                                                                                                                                                   | F        | ‚ùå Falso! Tanto o K3s (distribui√ß√£o do Rancher) quanto o pr√≥prio Rancher suportam arquiteturas ARM64. K3s √©, inclusive, muito popular para clusters em Raspberry Pi. üçì Rancher & K3s = Amigos do ARM e do Raspberry Pi.                                                                                                                                                                                                                                                                                                                |
| 99  | Se um `Deployment` est√° configurado com `spec.strategy.type: Recreate`, durante uma atualiza√ß√£o, todos os Pods antigos s√£o terminados antes que qualquer novo Pod seja criado, o que pode causar um breve tempo de indisponibilidade.                                                                                                                                                                                    | V        | ‚úÖ Correto! A estrat√©gia `Recreate` √© mais simples mas causa downtime. `RollingUpdate` (o padr√£o) √© projetada para evitar downtime. üí• `Recreate` = "Desligar tudo, ligar o novo"; üåä `RollingUpdate` = "Troca suave".                                                                                                                                                                                                                                                                                                                  |
| 100 | Apenas `Services` do tipo `LoadBalancer` podem receber tr√°fego de fora do cluster Kubernetes. `NodePort` e `Ingress` s√£o apenas para comunica√ß√£o interna.                                                                                                                                                                                                                                                                | F        | ‚ùå Falso! `NodePort` exp√µe o servi√ßo em uma porta em cada n√≥, acess√≠vel externamente. `Ingress` gerencia acesso externo HTTP/S para `Services`. `LoadBalancer` √© uma forma, mas n√£o a √∫nica, de expor servi√ßos externamente. üö™ M√∫ltiplas "portas de entrada" para seu cluster K8s!                                                                                                                                                                                                                                                     |
| 101 | `PriorityClass` √© um objeto Kubernetes que permite definir a prioridade de Pods. Pods com prioridade mais alta podem preemptar (desalocar) Pods de prioridade mais baixa se n√£o houver recursos suficientes.                                                                                                                                                                                                             | V        | ‚úÖ Verdadeiro! `PriorityClass` √© crucial para garantir que workloads cr√≠ticos tenham prefer√™ncia por recursos, especialmente em clusters com alta utiliza√ß√£o. ü•á `PriorityClass` = O "selo VIP" para seus Pods mais importantes.                                                                                                                                                                                                                                                                                                        |
| 102 | `RBAC` (Role-Based Access Control) no Kubernetes s√≥ se aplica a usu√°rios humanos e n√£o a `ServiceAccounts` usados por Pods.                                                                                                                                                                                                                                                                                              | F        | ‚ùå Errado! `RBAC` se aplica tanto a usu√°rios humanos quanto a `ServiceAccounts`. `Roles` e `ClusterRoles` podem ser vinculados a `ServiceAccounts` atrav√©s de `RoleBindings` e `ClusterRoleBindings` para conceder permiss√µes a processos rodando em Pods. ü§ñ `ServiceAccounts` tamb√©m precisam de "crach√°s de permiss√£o" (RBAC).                                                                                                                                                                                                       |
| 103 | O Rancher utiliza o conceito de "Projetos" para agrupar m√∫ltiplos `Namespaces` e aplicar pol√≠ticas de RBAC e `ResourceQuotas` a eles de forma unificada dentro de um cluster.                                                                                                                                                                                                                                            | V        | ‚úÖ Verdadeiro! Projetos no Rancher s√£o uma camada de abstra√ß√£o sobre `Namespaces` que facilita a organiza√ß√£o multi-tenant e a aplica√ß√£o de pol√≠ticas de governan√ßa para equipes ou aplica√ß√µes distintas dentro de um mesmo cluster. üèõÔ∏è Projetos no Rancher = "Departamentos" com suas pr√≥prias regras e or√ßamentos.                                                                                                                                                                                                                    |
| 104 | O comando `kubectl api-resources` lista todos os tipos de recursos dispon√≠veis na API do cluster Kubernetes que voc√™ est√° conectado, incluindo CRDs.                                                                                                                                                                                                                                                                     | V        | ‚úÖ Correto! √â um comando √∫til para descobrir quais `kinds` de objetos voc√™ pode criar e gerenciar, especialmente em clusters com muitas extens√µes (CRDs). üìú `api-resources` = O "√≠ndice completo" da API do seu K8s.                                                                                                                                                                                                                                                                                                                   |
| 105 | `VolumeSnapshots` no Kubernetes permitem criar c√≥pias point-in-time de `PersistentVolumes`, mas este recurso depende de um provisionador CSI (Container Storage Interface) que suporte snapshots.                                                                                                                                                                                                                        | V        | ‚úÖ Verdadeiro! `VolumeSnapshots` s√£o para backup e restaura√ß√£o de dados persistentes. A funcionalidade real de tirar o snapshot √© delegada ao driver de armazenamento via CSI. üì∏ `VolumeSnapshot` = Uma "fotografia" do seu disco, mas precisa de uma "c√¢mera" (driver CSI) compat√≠vel.                                                                                                                                                                                                                                                |
| 106 | `ConfigMaps` e `Secrets` podem ser montados como arquivos em cont√™ineres ou expostos como vari√°veis de ambiente.                                                                                                                                                                                                                                                                                                         | V        | ‚úÖ Correto! Estas s√£o as duas formas principais de disponibilizar dados de configura√ß√£o e segredos para as aplica√ß√µes dentro dos Pods. üìÅ Montar como arquivo ou üí® injetar como vari√°vel de ambiente.                                                                                                                                                                                                                                                                                                                                  |
| 107 | O Rancher Agent, instalado em clusters gerenciados, comunica-se diretamente com o `etcd` do cluster gerenciado para aplicar configura√ß√µes.                                                                                                                                                                                                                                                                               | F        | ‚ùå Falso! O Rancher Agent (`cluster-agent`) comunica-se com o `kube-apiserver` do cluster gerenciado. √â o `kube-apiserver` quem interage com o `etcd`. O Rancher segue o fluxo padr√£o de comunica√ß√£o do K8s. üó£Ô∏è Agente Rancher fala com o "recepcionista" (`kube-apiserver`), n√£o direto com o "cofre" (`etcd`).                                                                                                                                                                                                                       |
| 108 | A ferramenta `Helm` usa um arquivo chamado `Chart.yaml` para metadados do chart (nome, vers√£o, descri√ß√£o) e um diret√≥rio `templates/` para os manifestos Kubernetes parametrizados.                                                                                                                                                                                                                                      | V        | ‚úÖ Verdadeiro! Esta √© a estrutura b√°sica de um Helm chart. O diret√≥rio `templates/` cont√©m os arquivos YAML que ser√£o renderizados com os valores de `values.yaml` ou fornecidos na linha de comando. üì¶ Helm Chart = Uma "caixa de montagem" com manual (`Chart.yaml`) e pe√ßas (`templates/`).                                                                                                                                                                                                                                         |
| 109 | `NetworkPolicy` no Kubernetes, por padr√£o, nega todo o tr√°fego de entrada (ingress) e sa√≠da (egress) para os Pods selecionados, a menos que regras expl√≠citas permitam.                                                                                                                                                                                                                                                  | V        | ‚úÖ Correto! Quando voc√™ aplica uma `NetworkPolicy` a um Pod, se ela n√£o tiver regras de `ingress` ou `egress`, ela bloqueia todo o tr√°fego correspondente. Voc√™ precisa definir explicitamente o que √© permitido. üß± `NetworkPolicy` = Come√ßa com um "muro" e voc√™ abre "port√µes" (regras).                                                                                                                                                                                                                                             |
| 110 | O comando `kubectl top podNOME_DO_POD -n NOME_DO_NAMESPACE` mostra o consumo atual de CPU e mem√≥ria do Pod especificado.                                                                                                                                                                                                                                                                                                 | V        | ‚úÖ Verdadeiro! Para que este comando funcione, o `metrics-server` precisa estar instalado e funcionando no cluster. √â essencial para monitoramento r√°pido de recursos. üå°Ô∏è `kubectl top pod` = "Medir a febre" (uso de recursos) do seu Pod.                                                                                                                                                                                                                                                                                            |
| 111 | K3s, por ser leve, n√£o suporta funcionalidades avan√ßadas do Kubernetes como `CustomResourceDefinitions` (CRDs) ou `Admission Controllers`.                                                                                                                                                                                                                                                                               | F        | ‚ùå Falso! K3s √© uma distribui√ß√£o Kubernetes totalmente compat√≠vel e certificada pela CNCF. Ele suporta CRDs, `Admission Controllers` e todas as APIs padr√£o do Kubernetes. Sua leveza vem da otimiza√ß√£o de empacotamento e substitui√ß√£o de componentes como `etcd` por padr√£o. üí™ K3s: Pequeno, mas poderoso e completo!                                                                                                                                                                                                                |
| 112 | Ao usar Rancher para provisionar um cluster em um provedor de nuvem como AWS, o Rancher cria e gerencia diretamente as VMs (EC2 instances) e outros recursos de infraestrutura da nuvem.                                                                                                                                                                                                                                 | V        | ‚úÖ Verdadeiro! Quando o Rancher provisiona um cluster usando um "driver de n√≥" (node driver) para uma nuvem (ex: AWS EC2), ele interage com a API da nuvem para criar as VMs, configurar redes, security groups, etc., que formar√£o o cluster K8s. ‚òÅÔ∏è Rancher como construtor de nuvem = "Pedreiro digital" na AWS/Azure/GCP.                                                                                                                                                                                                           |
| 113 | `Finalizers` s√£o chaves especiais que os controladores Kubernetes adicionam a objetos para garantir que certas a√ß√µes de limpeza (pre-delete hooks) sejam executadas antes que o objeto seja completamente removido do `etcd`.                                                                                                                                                                                            | V        | ‚úÖ Correto! Se um objeto tem um `finalizer`, uma requisi√ß√£o `DELETE` apenas define o `metadata.deletionTimestamp`. O objeto s√≥ √© realmente deletado depois que o controlador respons√°vel remove o `finalizer` (ap√≥s a limpeza). üßπ `Finalizer` = "N√£o delete ainda, tenho que limpar a casa primeiro!"                                                                                                                                                                                                                                  |
| 114 | A atualiza√ß√£o de um `DaemonSet` segue a mesma estrat√©gia `RollingUpdate` padr√£o de um `Deployment`, substituindo os Pods nos n√≥s um a um.                                                                                                                                                                                                                                                                                | V        | ‚úÖ Verdadeiro! `DaemonSets` tamb√©m suportam `RollingUpdate` (e `OnDelete`) para atualizar os Pods em cada n√≥ de forma controlada, minimizando o impacto. üîÑ `DaemonSet` Update = "Atualiza√ß√£o organizada" em todos os n√≥s.                                                                                                                                                                                                                                                                                                              |
| 115 | Rancher √© capaz de gerenciar apenas clusters Kubernetes que foram provisionados usando o pr√≥prio Rancher (RKE, K3s ou via drivers de nuvem).                                                                                                                                                                                                                                                                             | F        | ‚ùå Falso! Uma das grandes vantagens do Rancher √© sua capacidade de *importar* e gerenciar clusters Kubernetes j√° existentes, criados por qualquer meio (ex: `kubeadm`, GKE, EKS, AKS), desde que sejam conformes com a CNCF. üîó Rancher = "Gerente vers√°til" que adota clusters de v√°rias origens.                                                                                                                                                                                                                                      |
| 116 | `Ephemeral Containers` s√£o um tipo especial de cont√™iner que pode ser adicionado a um Pod j√° em execu√ß√£o para fins de troubleshooting, sem necessidade de reiniciar o Pod.                                                                                                                                                                                                                                               | V        | ‚úÖ Correto! `Ephemeral Containers` (uma feature mais recente) s√£o como "sondas de diagn√≥stico" que voc√™ pode injetar em um Pod problem√°tico para investigar, usando ferramentas que n√£o estavam na imagem original. üî¨ `Ephemeral Container` = "Kit de ferramentas de emerg√™ncia" para Pods.                                                                                                                                                                                                                                            |
| 117 | Um `Service` do tipo `ExternalName` no Kubernetes cria um balanceador de carga no provedor de nuvem para um servi√ßo externo.                                                                                                                                                                                                                                                                                             | F        | ‚ùå Falso! `ExternalName` simplesmente cria um alias DNS *interno* ao cluster para um nome DNS externo. Ele n√£o provisiona nenhum recurso de balanceamento ou proxy; apenas retorna um registro CNAME. ‚û°Ô∏è `ExternalName` = "Apelido DNS" para um servi√ßo fora do cluster.                                                                                                                                                                                                                                                                |
| 118 | O comando `kubectl apply -f meu-diretorio/ [pipe] kubectl delete -f -` primeiro aplicaria todos os manifestos no `meu-diretorio/` e depois deletaria os mesmos recursos.                                                                                                                                                                                                                                                 | F        | ‚ùå Cuidado com a pegadinha do pipe! Este comando tentaria passar a *sa√≠da* do `kubectl apply` (que √© uma descri√ß√£o dos recursos aplicados/criados) como *entrada* para o `kubectl delete -f -`. O `-f -` espera manifestos YAML do stdin. Isso provavelmente n√£o faria o que se espera ou daria erro. ‚ö° O pipe envia o *output* de um para o *input* do outro.                                                                                                                                                                          |
| 119 | A ferramenta `kubeadm` √© usada pelo Rancher como o principal mecanismo para provisionar clusters RKE e K3s.                                                                                                                                                                                                                                                                                                              | F        | ‚ùå Falso! `kubeadm` √© uma ferramenta da comunidade Kubernetes para bootstrapping de clusters "vanilla". RKE e K3s t√™m seus pr√≥prios mecanismos de provisionamento otimizados, embora possam compartilhar alguns conceitos ou componentes subjacentes com o ecossistema K8s. üîß RKE/K3s usam seus pr√≥prios "construtores".                                                                                                                                                                                                               |
| 120 | `Pod Security Admission` (PSA) √© o substituto mais recente para `PodSecurityPolicy` (PSP) no Kubernetes, fornecendo uma forma mais simples e integrada de aplicar padr√µes de seguran√ßa de Pods em n√≠vel de `Namespace`.                                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro! PSA simplifica a aplica√ß√£o de pol√≠ticas de seguran√ßa (Privileged, Baseline, Restricted) com base em labels de `Namespace`, sendo mais f√°cil de usar que as complexas PSPs. ‚ú® PSA = "Seguran√ßa de Pods simplificada e moderna".                                                                                                                                                                                                                                                                                           |
| 121 | O `etcd` armazena apenas a configura√ß√£o desejada dos objetos Kubernetes, enquanto o estado atual √© mantido na mem√≥ria dos `kubelets`.                                                                                                                                                                                                                                                                                    | F        | ‚ùå Falso! O `etcd` armazena *tanto* o estado desejado (specs) *quanto* o estado observado/atual (status) dos objetos Kubernetes. Os `kubelets` reportam o estado atual dos Pods que gerenciam, e essa informa√ß√£o √© persistida no `etcd` via `API Server`. üíæ `etcd` = O "livro raz√£o" completo do cluster.                                                                                                                                                                                                                              |
| 122 | O Rancher pode realizar backups e restaura√ß√µes de clusters `etcd` para clusters RKE provisionados por ele.                                                                                                                                                                                                                                                                                                               | V        | ‚úÖ Verdadeiro! Para clusters RKE, o Rancher oferece funcionalidades integradas para agendar backups do `etcd` (que √© o "cora√ß√£o" do cluster K8s) e restaur√°-los em caso de desastre. üõ°Ô∏è Backup `etcd` via Rancher = "Seguro de vida" para seus clusters RKE.                                                                                                                                                                                                                                                                           |
| 123 | Um `Service` sem `selector` (headless service com `clusterIP: None`) √© usado principalmente para se integrar com sistemas de descoberta de servi√ßo externos ou para fornecer registros DNS diretamente para os IPs dos Pods de um `StatefulSet`.                                                                                                                                                                         | V        | ‚úÖ Correto! Headless services n√£o fazem proxy/load balancing. Em vez disso, o DNS do K8s retorna os IPs de todos os Pods que corresponderiam ao selector (se houvesse) ou, para `StatefulSets`, os IPs dos Pods individuais. üó∫Ô∏è Headless Service = "Mapa direto" para os Pods, sem intermedi√°rios.                                                                                                                                                                                                                                     |
| 124 | O uso de `hostPath` volumes em Pods √© uma pr√°tica de seguran√ßa recomendada, pois permite que os Pods acessem diretamente o filesystem do n√≥ para maior performance.                                                                                                                                                                                                                                                      | F        | ‚ùå Perigoso! `hostPath` volumes podem ser um grande risco de seguran√ßa, pois d√£o ao Pod acesso ao filesystem do n√≥, podendo permitir escalada de privil√©gios ou acesso a dados sens√≠veis. Deve ser usado com extrema cautela e restrito por pol√≠ticas. üö® `hostPath` = "Chave do filesystem do n√≥", use com responsabilidade!                                                                                                                                                                                                           |
| 125 | O `Controller Manager` no Kubernetes inclui um `Garbage Collector` que limpa objetos dependentes quando um objeto propriet√°rio √© deletado (ex: deletar os `ReplicaSets` e Pods quando um `Deployment` √© deletado).                                                                                                                                                                                                       | V        | ‚úÖ Verdadeiro! O `Garbage Collector` √© crucial para manter o cluster limpo, removendo recursos √≥rf√£os ou dependentes quando seus "donos" s√£o exclu√≠dos. üóëÔ∏è `Garbage Collector` K8s = O "faxineiro autom√°tico" do cluster.                                                                                                                                                                                                                                                                                                              |
| 126 | O Rancher s√≥ permite o uso de sua pr√≥pria interface de usu√°rio (UI) para interagir com os clusters gerenciados, bloqueando o acesso via `kubectl` diretamente aos API Servers dos clusters.                                                                                                                                                                                                                              | F        | ‚ùå Falso! O Rancher facilita o acesso via `kubectl` fornecendo arquivos `kubeconfig` para cada cluster gerenciado. A UI √© uma conveni√™ncia, n√£o uma restri√ß√£o. Voc√™ pode usar a UI, o Rancher CLI, ou `kubectl` diretamente. üö™ Rancher oferece "m√∫ltiplas portas" de acesso aos seus clusters.                                                                                                                                                                                                                                         |
| 127 | `TokenReview` e `SubjectAccessReview` s√£o APIs do Kubernetes usadas para, respectivamente, validar um token de autentica√ß√£o e verificar se um usu√°rio/grupo/SA tem permiss√£o para realizar uma a√ß√£o em um recurso.                                                                                                                                                                                                       | V        | ‚úÖ Correto! Estas s√£o APIs internas importantes para os mecanismos de autentica√ß√£o e autoriza√ß√£o do Kubernetes. Elas permitem que componentes (como o `API Server` ou webhooks) verifiquem credenciais e permiss√µes. üëÆ `TokenReview` / `SubjectAccessReview` = "Verifica√ß√£o de crach√° e permiss√µes" na API.                                                                                                                                                                                                                            |
| 128 | O `metrics-server` √© um componente opcional, mas essencial para funcionalidades como `kubectl top` e o `HorizontalPodAutoscaler` (HPA) quando baseado em m√©tricas de recursos (CPU/mem√≥ria).                                                                                                                                                                                                                             | V        | ‚úÖ Verdadeiro! Sem o `metrics-server` coletando e expondo as m√©tricas b√°sicas de recursos dos Pods e N√≥s, o HPA n√£o pode tomar decis√µes de escalonamento baseadas em CPU/mem√≥ria, e `kubectl top` n√£o funciona. üìä `metrics-server` = O "coletor de estat√≠sticas vitais" para HPA e `kubectl top`.                                                                                                                                                                                                                                      |
| 129 | K3s utiliza `containerd` como o container runtime padr√£o, mas n√£o permite a utiliza√ß√£o do Docker engine como runtime.                                                                                                                                                                                                                                                                                                    | F        | ‚ùå Falso! K3s usa `containerd` por padr√£o, que √© mais leve. No entanto, o K3s *pode* ser configurado para usar o Docker engine como runtime se ele j√° estiver instalado no sistema, embora `containerd` seja o preferido para leveza. üîÑ K3s √© flex√≠vel com runtimes, mas prefere `containerd`.                                                                                                                                                                                                                                         |
| 130 | Um `PersistentVolume` (PV) pode ter diferentes `accessModes` como `ReadWriteOnce` (RWO), `ReadOnlyMany` (ROX), e `ReadWriteMany` (RWX), que definem como o volume pode ser montado por m√∫ltiplos n√≥s/Pods.                                                                                                                                                                                                               | V        | ‚úÖ Verdadeiro! `AccessModes` s√£o cruciais. RWO: montado como leitura-escrita por um √∫nico n√≥. ROX: leitura por muitos n√≥s. RWX: leitura-escrita por muitos n√≥s (depende do tipo de storage, ex: NFS). üö¶ `AccessModes` = "Regras de tr√¢nsito" para acesso ao seu disco.                                                                                                                                                                                                                                                                 |
| 131 | A ferramenta Fleet, integrada ao Rancher, √© usada para gerenciamento de configura√ß√£o e implanta√ß√£o cont√≠nua de aplica√ß√µes em escala (GitOps) em m√∫ltiplos clusters Kubernetes.                                                                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro! Fleet √© a solu√ß√£o GitOps do Rancher. Voc√™ define o estado desejado dos seus clusters e aplica√ß√µes em um reposit√≥rio Git, e o Fleet garante que os clusters reflitam esse estado. ‚õµ Fleet = "Navegando" seus clusters e apps com GitOps.                                                                                                                                                                                                                                                                                  |
| 132 | `kubectl exec POD_NAME -c CONTAINER_NAME -- date` executa o comando `date` dentro do `CONTAINER_NAME` do `POD_NAME` e exibe a sa√≠da no seu terminal local.                                                                                                                                                                                                                                                               | V        | ‚úÖ Correto! `kubectl exec` √© a forma padr√£o de executar comandos ad-hoc dentro de cont√™ineres para diagn√≥stico ou tarefas r√°pidas. üó£Ô∏è `kubectl exec` = "Telefone" direto para executar comandos no seu cont√™iner.                                                                                                                                                                                                                                                                                                                      |
| 133 | Rancher 2.x foca exclusivamente no gerenciamento de clusters Kubernetes, tendo descontinuado o suporte a outros orquestradores como Docker Swarm ou Mesos que existiam em vers√µes anteriores (Rancher 1.x).                                                                                                                                                                                                              | V        | ‚úÖ Verdadeiro! Rancher 1.x era multi-orquestrador. A partir do Rancher 2.x, o foco total passou a ser o Kubernetes, refletindo a domin√¢ncia do K8s no mercado. ‚ú® Rancher 2.x = "All-in" no Kubernetes.                                                                                                                                                                                                                                                                                                                                  |
| 134 | `Readiness Probes` devem ser configuradas para verificar depend√™ncias externas da aplica√ß√£o (como um banco de dados), pois se o banco estiver fora, a aplica√ß√£o n√£o est√° pronta.                                                                                                                                                                                                                                         | F        | ‚ùå Cuidado! `Readiness Probes` devem verificar a *prontid√£o da pr√≥pria inst√¢ncia da aplica√ß√£o* para servir tr√°fego. Se todas as inst√¢ncias dependem de um DB externo e ele cai, todas ficar√£o "NotReady", causando um outage completo. Depend√™ncias externas devem ser tratadas com circuit breakers, retries, ou monitoramento separado. üí° Foco da `ReadinessProbe`: a app *neste Pod* est√° pronta?                                                                                                                                   |
| 135 | O comando `kubectl get events --sort-by=.metadata.creationTimestamp` exibe os eventos do cluster ordenados do mais recente para o mais antigo.                                                                                                                                                                                                                                                                           | F        | ‚ùå Quase! `.metadata.creationTimestamp` ordena do mais *antigo* para o mais *recente*. Para ver os mais recentes primeiro, voc√™ geralmente faria `kubectl get events --sort-by=.lastTimestamp` ou usaria ferramentas que formatam melhor, como `kubectl get events -w` (watch) para ver em tempo real. üìÖ Aten√ß√£o na ordena√ß√£o!                                                                                                                                                                                                         |
| 136 | O `kube-scheduler` no Kubernetes pode ser estendido com m√∫ltiplas pol√≠ticas de agendamento e perfis para customizar como os Pods s√£o distribu√≠dos pelos n√≥s do cluster.                                                                                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro! O scheduler √© altamente configur√°vel e extens√≠vel, permitindo definir prioridades, preemp√ß√£o, e at√© mesmo implementar schedulers customizados para atender necessidades espec√≠ficas de agendamento. üß† Scheduler K8s = "C√©rebro de agendamento" configur√°vel.                                                                                                                                                                                                                                                            |
| 137 | `ConfigMaps` s√£o mais adequados para armazenar dados bin√°rios grandes, como imagens ou arquivos de v√≠deo, para serem consumidos por Pods.                                                                                                                                                                                                                                                                                | F        | ‚ùå Falso! `ConfigMaps` s√£o para dados de configura√ß√£o textuais (chave-valor). Embora possam armazenar pequenos bin√°rios (codificados em base64, se necess√°rio), eles t√™m limites de tamanho (geralmente 1MB no `etcd`). Para bin√°rios grandes, use `PersistentVolumes` ou um object storage externo. üì¶ `ConfigMap` ‚â† Armaz√©m de dados grandes.                                                                                                                                                                                         |
| 138 | O Rancher Continuous Delivery (baseado no Fleet) permite definir "Bundles" que agrupam m√∫ltiplos manifestos Kubernetes ou Helm charts para serem implantados como uma unidade em um ou mais clusters.                                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro! Bundles no Fleet s√£o uma forma poderosa de definir o estado de uma aplica√ß√£o ou um conjunto de aplica√ß√µes e suas configura√ß√µes, facilitando a gest√£o e implanta√ß√£o consistente via GitOps. üéÅ Fleet Bundles = "Pacotes de presente" com tudo que sua app precisa para rodar.                                                                                                                                                                                                                                             |
| 139 | Se um `PersistentVolumeClaim` (PVC) √© deletado, o `PersistentVolume` (PV) subjacente √© sempre automaticamente deletado, independentemente de sua `persistentVolumeReclaimPolicy`.                                                                                                                                                                                                                                        | F        | ‚ùå Falso! O destino do PV ap√≥s a dele√ß√£o do PVC depende da `persistentVolumeReclaimPolicy` do PV: `Retain` (PV permanece, precisa ser limpo manualmente), `Delete` (PV e storage externo s√£o deletados), `Recycle` (depreciado, limpava o PV para reutiliza√ß√£o). ‚ôªÔ∏è `ReclaimPolicy` decide o destino do "disco" (PV).                                                                                                                                                                                                                   |
| 140 | O Rancher pode impor pol√≠ticas de seguran√ßa, como `Pod Security Standards` (PSS), em clusters gerenciados, garantindo que os workloads implantados atendam a requisitos m√≠nimos de seguran√ßa.                                                                                                                                                                                                                            | V        | ‚úÖ Verdadeiro! Atrav√©s da configura√ß√£o de clusters ou projetos, o Rancher pode ajudar a aplicar os PSS (Privileged, Baseline, Restricted) ou integrar-se com ferramentas como OPA Gatekeeper para pol√≠ticas mais granulares. üëÆ Rancher como "Policial de Seguran√ßa" dos seus clusters.                                                                                                                                                                                                                                                 |
| 141 | `Admission Controllers` no Kubernetes s√£o plugins que interceptam requisi√ß√µes ao `kube-apiserver` (ap√≥s autentica√ß√£o e autoriza√ß√£o) e podem validar, modificar (mutating) ou rejeitar (validating) essas requisi√ß√µes.                                                                                                                                                                                                    | V        | ‚úÖ Correto! `Admission Controllers` s√£o uma forma poderosa de aplicar pol√≠ticas, seguran√ßa e boas pr√°ticas no cluster. Exemplos: `NamespaceLifecycle`, `LimitRanger`, `ResourceQuota`. üö™ `Admission Controllers` = "Fiscais" na porta da API K8s.                                                                                                                                                                                                                                                                                      |
| 142 | O comando `kubectl cluster-info dump --output-directory=/tmp/cluster_state` salva um dump completo do `etcd` no diret√≥rio especificado.                                                                                                                                                                                                                                                                                  | F        | ‚ùå Falso! `kubectl cluster-info dump` coleta informa√ß√µes sobre o estado *atual* dos objetos da API (como `kubectl get all -A -o yaml`) e logs de componentes do control plane. N√£o √© um dump direto do `etcd`. Para backup do `etcd`, usa-se `etcdctl snapshot save`. üîç `cluster-info dump` ‚â† Backup `etcd`.                                                                                                                                                                                                                           |
| 143 | `RKE2` (tamb√©m conhecido como RKE Government) √© uma distribui√ß√£o Kubernetes da Rancher/SUSE focada em seguran√ßa e conformidade, seguindo guias como o STIG da DISA.                                                                                                                                                                                                                                                      | V        | ‚úÖ Verdadeiro! `RKE2` √© uma evolu√ß√£o do RKE com foco em atender requisitos de seguran√ßa rigorosos, comum em ambientes governamentais e setores regulados. üîí `RKE2` = "RKE com armadura refor√ßada".                                                                                                                                                                                                                                                                                                                                     |
| 144 | Alterar o `selector` de um `Service` existente para apontar para um novo conjunto de Pods com `labels` diferentes far√° com que o `Service` pare de rotear tr√°fego para os Pods antigos e comece a rotear para os novos, sem necessidade de recriar o `Service`.                                                                                                                                                          | V        | ‚úÖ Correto! O `selector` de um `Service` √© mut√°vel. Mudar o `selector` e aplicar a altera√ß√£o far√° o Kubernetes atualizar os `Endpoints`/`EndpointSlices` do `Service` para refletir o novo conjunto de Pods. üîÑ `Service selector` = "Mira ajust√°vel" para seus Pods.                                                                                                                                                                                                                                                                   |
| 145 | O `kubelet` em um n√≥ se registra no `kube-apiserver` e periodicamente envia o estado do n√≥ e dos Pods que ele gerencia.                                                                                                                                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro! Esta √© a comunica√ß√£o fundamental entre o `kubelet` (agente do n√≥) e o `Control Plane`. O `kubelet` √© os "olhos e ouvidos" do `Control Plane` em cada n√≥. üëÄ `kubelet` reportando = "Chefe, aqui no n√≥ X est√° tudo assim...".                                                                                                                                                                                                                                                                                             |
| 146 | O Rancher n√£o permite o gerenciamento de `ServiceAccounts`, `Roles` e `RoleBindings` do Kubernetes diretamente, exigindo que toda a gest√£o de RBAC seja feita atrav√©s da UI ou API do pr√≥prio Rancher.                                                                                                                                                                                                                   | F        | ‚ùå Falso! Embora o Rancher ofere√ßa uma camada de RBAC pr√≥pria que simplifica a gest√£o de permiss√µes em clusters e projetos, ele tamb√©m permite (e muitas vezes requer para cen√°rios avan√ßados) a gest√£o direta dos recursos RBAC nativos do Kubernetes (via `kubectl` ou YAML atrav√©s da UI). üõ†Ô∏è Rancher RBAC: Camada de ajuda + Acesso direto.                                                                                                                                                                                        |
| 147 | `Taints` aplicados a um n√≥ com o efeito `NoExecute` far√£o com que os Pods j√° rodando naquele n√≥ que n√£o toleram o `taint` sejam evictados (removidos) do n√≥.                                                                                                                                                                                                                                                             | V        | ‚úÖ Correto! `NoExecute` √© mais forte que `NoSchedule`. Al√©m de impedir novos agendamentos, ele remove Pods existentes que n√£o toleram o `taint` ap√≥s um `tolerationSeconds` (ou imediatamente se n√£o especificado). Ï´ìÏïÑÎÇ¥Îã§ (jjochanaeda - expulsar em coreano) `NoExecute` = "Saia daqui agora se voc√™ n√£o tem a chave (Toleration)!".                                                                                                                                                                                                    |
| 148 | `Helmfile` √© uma ferramenta declarativa para implantar e gerenciar m√∫ltiplos Helm charts como um conjunto coeso, permitindo definir ambientes e heran√ßa de valores.                                                                                                                                                                                                                                                      | V        | ‚úÖ Verdadeiro! `Helmfile` ajuda a organizar implanta√ß√µes Helm complexas, onde voc√™ pode ter dezenas de charts. Ele age como um "orquestrador de charts Helm". üéº `Helmfile` = O "maestro" para sua "orquestra" de Helm charts.                                                                                                                                                                                                                                                                                                          |
| 149 | O `kube-proxy` em modo IPVS (IP Virtual Server) geralmente oferece melhor performance e escalabilidade para `Services` com muitos backends (Pods) em compara√ß√£o com o modo `iptables`.                                                                                                                                                                                                                                   | V        | ‚úÖ Verdadeiro! IPVS usa estruturas de hash mais eficientes para balanceamento de carga, sendo mais perform√°tico que `iptables` em clusters grandes. √â uma configura√ß√£o que pode ser feita no `kube-proxy`. üöÄ IPVS = "Modo turbo" para o `kube-proxy`.                                                                                                                                                                                                                                                                                  |
| 150 | Uma vez que um `Secret` √© criado no Kubernetes, seu conte√∫do (`data`) n√£o pode ser alterado; para atualizar um `Secret`, √© necess√°rio delet√°-lo e recri√°-lo com os novos valores.                                                                                                                                                                                                                                        | F        | ‚ùå Falso! O conte√∫do de um `Secret` (o campo `data`) *pode* ser alterado ap√≥s a cria√ß√£o. As aplica√ß√µes que consomem o `Secret` (se montado como volume) ver√£o a atualiza√ß√£o (pode haver um pequeno delay). ‚úèÔ∏è `Secrets` s√£o mut√°veis, mas trate as atualiza√ß√µes com cuidado!                                                                                                                                                                                                                                                            |
| 151 | Uma empresa enfrenta alta lat√™ncia em microsservi√ßos cr√≠ticos no Kubernetes. A solu√ß√£o imediata proposta √© aumentar o n√∫mero de r√©plicas de todos os microsservi√ßos em 50%.                                                                                                                                                                                                                                              | F        | ‚ùå Incorreto. Aumentar r√©plicas sem diagn√≥stico √© reativo. üí° **Solu√ß√£o:** Investigar a causa raiz: gargalos de rede (CNI, `Services`), limites de recursos inadequados (`requests`/`limits`), c√≥digo ineficiente, ou falta de um Service Mesh para observabilidade e controle de tr√°fego. Comece com monitoramento e tracing.                                                                                                                                                                                                          |
| 152 | A equipe de DevOps de uma startup est√° sobrecarregada gerenciando manualmente tr√™s clusters Kubernetes em diferentes provedores de nuvem. Adotar o Rancher para centralizar o gerenciamento, a autentica√ß√£o e a implanta√ß√£o de aplica√ß√µes √© uma estrat√©gia v√°lida para aliviar essa carga.                                                                                                                               | V        | ‚úÖ Correto. Rancher √© projetado para isso. üí° **Solu√ß√£o:** Rancher unifica a gest√£o de clusters heterog√™neos, simplifica RBAC, e oferece cat√°logos de apps, reduzindo a complexidade operacional e permitindo que a equipe foque em tarefas de maior valor.                                                                                                                                                                                                                                                                             |
| 153 | Uma aplica√ß√£o legada conteinerizada est√° consumindo excessivamente CPU em um n√≥ espec√≠fico do Kubernetes, impactando outros Pods. A solu√ß√£o mais robusta √© configurar `Node Affinity` para que este Pod sempre rode em um n√≥ dedicado com mais CPU.                                                                                                                                                                      | F        | ‚ùå Incompleto. Embora `Node Affinity` possa isolar, n√£o resolve a causa. üí° **Solu√ß√£o:** Primeiro, perfilar a aplica√ß√£o para entender o alto consumo de CPU. Em paralelo, definir `requests` e `limits` de CPU para o Pod, e considerar `ResourceQuotas` no `Namespace` para evitar "vizinhos barulhentos". N√≥ dedicado √© um √∫ltimo recurso.                                                                                                                                                                                            |
| 154 | A empresa "Alfa" precisa garantir que apenas imagens de cont√™iner aprovadas e escaneadas de seu registry privado sejam implantadas no cluster Kubernetes. Implementar um `Admission Controller` (como OPA Gatekeeper ou Kyverno) com pol√≠ticas para validar a origem e o status do scan da imagem √© uma abordagem eficaz.                                                                                                | V        | ‚úÖ Verdadeiro. `Admission Controllers` s√£o ideais para impor pol√≠ticas. üí° **Solu√ß√£o:** Essa abordagem permite bloquear implanta√ß√µes de imagens n√£o conformes na entrada (API Server), refor√ßando a seguran√ßa da cadeia de suprimentos de software. Integra√ß√£o com scanners de vulnerabilidade √© crucial.                                                                                                                                                                                                                               |
| 155 | Os custos com a nuvem para os clusters Kubernetes est√£o crescendo descontroladamente. A equipe sugere desabilitar o `Cluster Autoscaler` para economizar, pois ele provisiona n√≥s dinamicamente.                                                                                                                                                                                                                         | F        | ‚ùå Arriscado e contraproducente. O `Cluster Autoscaler` tamb√©m *remove* n√≥s ociosos. üí° **Solu√ß√£o:** Otimizar o `Cluster Autoscaler` (perfis, limites), usar `HorizontalPodAutoscaler` para escalar Pods eficientemente, definir `requests/limits` corretos, usar `PodDisruptionBudgets`, e investigar ferramentas de "cost management" espec√≠ficas para K8s.                                                                                                                                                                           |
| 156 | Durante um rollout de uma nova vers√£o via `Deployment` Kubernetes, a aplica√ß√£o come√ßa a apresentar erros. A equipe decide imediatamente deletar o `Deployment` e recri√°-lo com a vers√£o antiga.                                                                                                                                                                                                                          | F        | ‚ùå Ineficiente e arriscado. üí° **Solu√ß√£o:** Usar `kubectl rollout undo deployment/NOME_DO_DEPLOYMENT`. Isso reverte para a revis√£o est√°vel anterior de forma controlada, minimizando o downtime e preservando o hist√≥rico do `Deployment` para an√°lise.                                                                                                                                                                                                                                                                                 |
| 157 | Uma empresa quer adotar GitOps para gerenciar seus clusters Kubernetes. Utilizar uma ferramenta como Argo CD ou Flux, que sincroniza o estado do cluster com manifestos em um reposit√≥rio Git, √© uma pr√°tica recomendada para essa abordagem.                                                                                                                                                                            | V        | ‚úÖ Correto. Argo CD e Flux s√£o pilares do GitOps. üí° **Solu√ß√£o:** GitOps traz rastreabilidade, versionamento e automa√ß√£o para a configura√ß√£o do cluster. O Git se torna a √∫nica fonte da verdade, e as ferramentas garantem que o cluster reflita o estado desejado.                                                                                                                                                                                                                                                                    |
| 158 | Para melhorar a seguran√ßa, a empresa decidiu que todos os `Services` Kubernetes devem ser do tipo `NodePort` para evitar a complexidade de `LoadBalancers` e `Ingress`.                                                                                                                                                                                                                                                  | F        | ‚ùå Inseguro e impratic√°vel para muitos cen√°rios. `NodePort` exp√µe portas em *todos* os n√≥s, aumentando a superf√≠cie de ataque e dificultando o gerenciamento de tr√°fego HTTP/S. üí° **Solu√ß√£o:** Usar `Ingress` para tr√°fego HTTP/S (com termina√ß√£o SSL, roteamento) e `LoadBalancer` para outros protocolos TCP/UDP quando necess√°rio. `NodePort` √© mais para casos espec√≠ficos ou internos.                                                                                                                                            |
| 159 | A equipe de desenvolvimento precisa de acesso r√°pido para depurar Pods em produ√ß√£o. A solu√ß√£o √© conceder a todos os desenvolvedores permiss√µes de `cluster-admin` no cluster de produ√ß√£o.                                                                                                                                                                                                                                | F        | ‚ùå Extremamente arriscado! `cluster-admin` d√° controle total. üí° **Solu√ß√£o:** Implementar RBAC granular. Conceder permiss√µes m√≠nimas necess√°rias (ex: `get`, `logs`, `exec` em Pods de seus `Namespaces`) via `Roles` e `RoleBindings`. Usar `Ephemeral Containers` ou ferramentas de depura√ß√£o que n√£o exijam acesso excessivo.                                                                                                                                                                                                        |
| 160 | Uma aplica√ß√£o cr√≠tica precisa de armazenamento persistente com baixa lat√™ncia e alta disponibilidade no Kubernetes. A equipe prop√µe usar `hostPath` volumes em n√≥s espec√≠ficos com SSDs NVMe.                                                                                                                                                                                                                            | F        | ‚ùå Arriscado e n√£o port√°vel. `hostPath` acopla o Pod ao n√≥ e tem implica√ß√µes de seguran√ßa. üí° **Solu√ß√£o:** Usar `PersistentVolumes` (PVs) provisionados por uma `StorageClass` que utilize um driver CSI para storage de alta performance (ex: discos SSD da nuvem, storage SAN/NAS otimizado). `StatefulSets` para gerenciar Pods com PVs.                                                                                                                                                                                             |
| 161 | A empresa est√° migrando para microsservi√ßos e enfrenta dificuldades com descoberta de servi√ßo, resili√™ncia (retries, timeouts) e seguran√ßa na comunica√ß√£o inter-servi√ßos. Adotar um Service Mesh como Istio ou Linkerd pode endere√ßar esses desafios de forma centralizada.                                                                                                                                              | V        | ‚úÖ Verdadeiro. Service Meshes s√£o projetados para isso. üí° **Solu√ß√£o:** Um Service Mesh injeta sidecars (proxies) nos Pods, gerenciando o tr√°fego L7 e fornecendo observabilidade, seguran√ßa (mTLS) e controle de tr√°fego sem alterar o c√≥digo da aplica√ß√£o.                                                                                                                                                                                                                                                                            |
| 162 | Para reduzir a complexidade do Kubernetes para novas equipes, a empresa decide padronizar o uso de `kubectl run` para todas as implanta√ß√µes, em vez de arquivos YAML.                                                                                                                                                                                                                                                    | F        | ‚ùå Perda de declaratividade e versionamento. `kubectl run` √© imperativo. üí° **Solu√ß√£o:** Educar as equipes sobre a abordagem declarativa com manifestos YAML. Usar Helm charts ou Kustomize para parametrizar e gerenciar a complexidade dos YAMLs. `kubectl run` √© bom para testes r√°pidos, n√£o para produ√ß√£o.                                                                                                                                                                                                                         |
| 163 | A empresa possui m√∫ltiplos clusters Kubernetes e precisa garantir que as pol√≠ticas de seguran√ßa de rede (`NetworkPolicies`) sejam aplicadas de forma consistente em todos eles. Utilizar o Rancher com suas funcionalidades de gerenciamento de pol√≠ticas ou uma ferramenta de GitOps como o Fleet √© uma boa estrat√©gia.                                                                                                 | V        | ‚úÖ Correto. Consist√™ncia √© chave em multi-cluster. üí° **Solu√ß√£o:** Rancher pode aplicar CRDs de pol√≠tica ou integrar com OPA Gatekeeper em m√∫ltiplos clusters. Fleet (GitOps) pode garantir que os manifestos de `NetworkPolicy` sejam sincronizados de um reposit√≥rio Git para todos os clusters alvo.                                                                                                                                                                                                                                 |
| 164 | Uma aplica√ß√£o web no Kubernetes precisa escalar dinamicamente com base no n√∫mero de requisi√ß√µes por segundo (RPS). Configurar um `HorizontalPodAutoscaler` (HPA) para usar m√©tricas customizadas de RPS expostas por um adaptador (ex: Prometheus Adapter) √© a abordagem correta.                                                                                                                                        | V        | ‚úÖ Verdadeiro. HPA com m√©tricas customizadas √© flex√≠vel. üí° **Solu√ß√£o:** Expor a m√©trica de RPS via Prometheus (ou similar), instalar o `kube-state-metrics` e o `Prometheus Adapter` para que o HPA possa consumir essa m√©trica customizada (`custom.metrics.k8s.io`) e escalar os Pods da aplica√ß√£o.                                                                                                                                                                                                                                  |
| 165 | Para otimizar o build de imagens Docker, a equipe sugere copiar todo o c√≥digo-fonte para a imagem na primeira camada e depois instalar as depend√™ncias, compilar e configurar a aplica√ß√£o nas camadas seguintes.                                                                                                                                                                                                         | F        | ‚ùå Ineficiente para o cache de camadas. üí° **Solu√ß√£o:** Usar multi-stage builds. Copiar apenas o necess√°rio para cada est√°gio (ex: `pom.xml`/`package.json` primeiro, instalar depend√™ncias, *depois* copiar o c√≥digo-fonte e compilar). Isso melhora o aproveitamento do cache de camadas e reduz o tamanho final da imagem.                                                                                                                                                                                                           |
| 166 | A empresa "Beta" quer garantir que nenhum cont√™iner rode como `root` em seus clusters Kubernetes. Aplicar um `PodSecurityPolicy` (PSP) ou usar `Pod Security Admission` (PSA) com o perfil `restricted` ou `baseline` que impe√ßa `allowPrivilegeEscalation: true` e `runAsUser: 0` √© a forma de implementar isso.                                                                                                        | V        | ‚úÖ Correto. PSP (legado) ou PSA s√£o os mecanismos para isso. üí° **Solu√ß√£o:** PSA √© o mais novo e recomendado. Configurar o `Namespace` com `pod-security.kubernetes.io/enforce: restricted` (ou `baseline`) ajudar√° a prevenir cont√™ineres root e outras configura√ß√µes inseguras.                                                                                                                                                                                                                                                       |
| 167 | Os logs de todas as aplica√ß√µes rodando no Kubernetes est√£o sendo perdidos quando os Pods s√£o reiniciados. A solu√ß√£o √© configurar cada aplica√ß√£o para escrever logs em um `PersistentVolume` montado no Pod.                                                                                                                                                                                                              | F        | ‚ùå Ineficiente e n√£o escal√°vel. üí° **Solu√ß√£o:** Implementar uma arquitetura de logging centralizada. Configurar os cont√™ineres para logar em `stdout/stderr`. Usar um agente coletor de logs no n√≥ (ex: Fluentd, Filebeat) como um `DaemonSet` para enviar os logs para um backend centralizado (ex: Elasticsearch, Loki, Splunk).                                                                                                                                                                                                      |
| 168 | Uma empresa precisa expor um servi√ßo TCP n√£o-HTTP (ex: um banco de dados) para acesso externo de forma segura e resiliente no Kubernetes. Usar um `Service` do tipo `LoadBalancer` √© a abordagem padr√£o e recomendada.                                                                                                                                                                                                   | V        | ‚úÖ Verdadeiro. `LoadBalancer` √© para isso. üí° **Solu√ß√£o:** Um `Service` do tipo `LoadBalancer` provisiona um balanceador de carga L4 na nuvem (ou usa solu√ß√µes como MetalLB on-prem) que pode distribuir tr√°fego TCP/UDP para os Pods do servi√ßo. Combinar com `NetworkPolicies` para restringir o acesso.                                                                                                                                                                                                                              |
| 169 | Para realizar um teste A/B de uma nova feature, a equipe planeja duplicar o `Deployment` da aplica√ß√£o, criar um novo `Service` para a vers√£o B, e dividir o tr√°fego manualmente alterando a configura√ß√£o do DNS.                                                                                                                                                                                                         | F        | ‚ùå Complexo e propenso a erros. üí° **Solu√ß√£o:** Usar um `Ingress Controller` avan√ßado (ex: Nginx, Traefik, Istio Gateway) que suporte divis√£o de tr√°fego baseada em peso (weighted load balancing) ou headers. Isso permite direcionar uma porcentagem do tr√°fego para a vers√£o B de forma controlada e din√¢mica. Um Service Mesh tamb√©m facilita isso.                                                                                                                                                                                 |
| 170 | A empresa "Gama" est√° preocupada com a seguran√ßa dos `Secrets` Kubernetes armazenados no `etcd`. Habilitar a criptografia em repouso (encryption at rest) para `Secrets` no `etcd` √© uma medida de seguran√ßa fundamental.                                                                                                                                                                                                | V        | ‚úÖ Correto. Por padr√£o, `Secrets` s√£o apenas base64-encoded. üí° **Solu√ß√£o:** Configurar o `kube-apiserver` para usar um provedor de criptografia (ex: `aescbc`, `kms`) para que os `Secrets` sejam criptografados antes de serem escritos no `etcd`. Isso protege contra acesso direto aos dados do `etcd`.                                                                                                                                                                                                                             |
| 171 | Para acelerar o desenvolvimento, a equipe decide montar o diret√≥rio `~/.kube/config` do desenvolvedor diretamente nos Pods de desenvolvimento para que eles possam interagir com o cluster.                                                                                                                                                                                                                              | F        | ‚ùå Risco de seguran√ßa enorme! Exp√µe credenciais de admin. üí° **Solu√ß√£o:** Se Pods precisam interagir com a API K8s, eles devem usar `ServiceAccounts` com permiss√µes RBAC m√≠nimas e espec√≠ficas para suas tarefas. Jamais montar `kubeconfig` de usu√°rio em Pods.                                                                                                                                                                                                                                                                       |
| 172 | Uma empresa utiliza Kustomize para gerenciar as varia√ß√µes de seus manifestos Kubernetes entre ambientes (dev, staging, prod). Essa √© uma abordagem v√°lida para customiza√ß√£o declarativa sem a complexidade do templating do Helm.                                                                                                                                                                                        | V        | ‚úÖ Verdadeiro. Kustomize √© √≥timo para isso. üí° **Solu√ß√£o:** Kustomize permite sobrepor "patches" em uma base comum de manifestos, facilitando a gest√£o de configura√ß√µes espec√≠ficas por ambiente de forma declarativa e sem a necessidade de aprender a linguagem de templating do Helm.                                                                                                                                                                                                                                                |
| 173 | A empresa est√° enfrentando problemas de "noisy neighbor" em seus clusters K8s, onde alguns workloads consomem todos os recursos de um n√≥. A solu√ß√£o prim√°ria √© implementar `Vertical Pod Autoscaler` (VPA) para todos os Pods.                                                                                                                                                                                           | F        | ‚ùå VPA ajuda, mas n√£o √© a solu√ß√£o prim√°ria para "noisy neighbor". üí° **Solu√ß√£o:** A base √© definir `requests` e `limits` de recursos para *todos* os cont√™ineres. Em seguida, usar `ResourceQuotas` por `Namespace`. VPA pode *ajudar* a definir esses `requests/limits` de forma mais din√¢mica, mas n√£o substitui a necessidade deles.                                                                                                                                                                                                 |
| 174 | Para facilitar a gest√£o de certificados TLS para `Ingress`, a empresa instala `cert-manager` no cluster Kubernetes. `cert-manager` pode automaticamente provisionar e renovar certificados de autoridades como Let's Encrypt.                                                                                                                                                                                            | V        | ‚úÖ Correto. `cert-manager` automatiza o ciclo de vida de certificados. üí° **Solu√ß√£o:** `cert-manager` monitora recursos `Ingress` (e outros) e, baseado em annotations, solicita, valida e configura certificados TLS, armazenando-os em `Secrets` para uso pelo `Ingress Controller`.                                                                                                                                                                                                                                                  |
| 175 | A equipe de SRE quer ser alertada se o `etcd` de um cluster RKE gerenciado pelo Rancher estiver com problemas de quorum. Configurar alertas no Prometheus (integrado ao Rancher) com base nas m√©tricas de sa√∫de do `etcd` √© a abordagem correta.                                                                                                                                                                         | V        | ‚úÖ Verdadeiro. Monitorar `etcd` √© vital. üí° **Solu√ß√£o:** Rancher exp√µe m√©tricas do `etcd` (para clusters RKE) via Prometheus. Criar regras de alerta no Alertmanager para condi√ß√µes como perda de quorum, lat√™ncia alta, ou erros de escrita no `etcd` √© crucial para a estabilidade do cluster.                                                                                                                                                                                                                                        |
| 176 | Para garantir a portabilidade das aplica√ß√µes entre diferentes clusters Kubernetes (on-prem e nuvem), a empresa decide usar apenas `hostPath` volumes para persist√™ncia, mapeando para um storage NFS montado em todos os n√≥s.                                                                                                                                                                                            | F        | ‚ùå Problem√°tico. `hostPath` n√£o √© port√°vel e NFS via `hostPath` √© complexo de gerenciar e menos seguro. üí° **Solu√ß√£o:** Usar `PersistentVolumeClaims` com `StorageClasses` que apontem para drivers CSI apropriados para cada ambiente (ex: CSI driver para NFS, CSI driver para EBS na AWS). Isso abstrai a infra de storage.                                                                                                                                                                                                          |
| 177 | Uma empresa precisa realizar upgrades dos n√≥s do Kubernetes com o m√≠nimo de impacto nas aplica√ß√µes. Utilizar o processo de "cordon and drain" em cada n√≥, respeitando os `PodDisruptionBudgets` (PDBs) das aplica√ß√µes, √© a pr√°tica recomendada.                                                                                                                                                                          | V        | ‚úÖ Correto. `cordon` impede novos agendamentos, `drain` remove Pods existentes de forma segura. üí° **Solu√ß√£o:** Antes de atualizar um n√≥, marcar como `unschedulable` (`cordon`). Depois, usar `kubectl drain NOME_DO_NO --ignore-daemonsets --delete-emptydir-data`. PDBs garantem que um n√∫mero m√≠nimo de r√©plicas da aplica√ß√£o permane√ßa dispon√≠vel.                                                                                                                                                                                 |
| 178 | Os desenvolvedores reclamam que o build de imagens Docker est√° muito lento. A equipe de infraestrutura sugere usar um registry Docker local como cache (pull-through cache) para imagens base e depend√™ncias.                                                                                                                                                                                                            | V        | ‚úÖ Verdadeiro. Isso pode acelerar significativamente os builds. üí° **Solu√ß√£o:** Configurar um registry como Harbor ou Nexus para atuar como proxy e cache para registries p√∫blicos (Docker Hub, etc.). Os builds no CI/CD e nas m√°quinas dos devs usariam esse registry local, que serviria as imagens cacheadas rapidamente.                                                                                                                                                                                                           |
| 179 | A empresa quer implementar autentica√ß√£o mTLS entre todos os microsservi√ßos no Kubernetes para seguran√ßa "zero trust". A maneira mais simples √© modificar cada microsservi√ßo para lidar com certificados e handshakes TLS.                                                                                                                                                                                                | F        | ‚ùå Complexo e intrusivo. üí° **Solu√ß√£o:** Usar um Service Mesh (Istio, Linkerd). O Service Mesh injeta sidecar proxies que automaticamente gerenciam o mTLS entre os Pods, sem necessidade de alterar o c√≥digo da aplica√ß√£o. Ele tamb√©m lida com rota√ß√£o de certificados.                                                                                                                                                                                                                                                                |
| 180 | Para reduzir o tamanho das imagens de cont√™iner, a equipe decide usar imagens base "scratch" para todas as aplica√ß√µes, incluindo aquelas escritas em Java ou Python.                                                                                                                                                                                                                                                     | F        | ‚ùå "Scratch" √© para bin√°rios estaticamente compilados. Aplica√ß√µes Java/Python precisam de um runtime. üí° **Solu√ß√£o:** Usar imagens base minimalistas (ex: `alpine`, `distroless`) que contenham apenas o runtime e as depend√™ncias necess√°rias. Multi-stage builds s√£o essenciais aqui para copiar apenas o artefato final e o runtime.                                                                                                                                                                                                 |
| 181 | Uma aplica√ß√£o stateful (ex: um banco de dados customizado) precisa de nomes de rede est√°veis e armazenamento persistente √∫nico por r√©plica no Kubernetes. Utilizar um `StatefulSet` em vez de um `Deployment` √© a escolha correta para este cen√°rio.                                                                                                                                                                     | V        | ‚úÖ Correto. `StatefulSets` fornecem essas garantias. üí° **Solu√ß√£o:** `StatefulSets` d√£o aos Pods identidades ordinais e est√°veis (ex: `meu-db-0`, `meu-db-1`) e PVs est√°veis vinculados a cada identidade. Isso √© crucial para aplica√ß√µes que dependem de estado e ordem.                                                                                                                                                                                                                                                               |
| 182 | A empresa nota que, ap√≥s deletar um `Namespace` no Kubernetes, alguns recursos externos (ex: Load Balancers na nuvem criados por `Services`) n√£o s√£o removidos, gerando custos. Isso ocorre porque o Kubernetes n√£o gerencia o ciclo de vida de recursos externos criados por `Services`.                                                                                                                                | V        | ‚úÖ Verdadeiro. Este √© um comportamento conhecido. üí° **Solu√ß√£o:** Implementar `finalizers` customizados ou usar `Operators` que gerenciem o ciclo de vida completo desses recursos externos. Ao deletar o `Service` ou `Namespace`, o `finalizer`/`Operator` se encarregaria de limpar os recursos na nuvem. Ferramentas como Crossplane tamb√©m ajudam.                                                                                                                                                                                 |
| 183 | Para monitorar a performance de aplica√ß√µes Java no Kubernetes, a equipe decide apenas coletar m√©tricas de CPU e mem√≥ria dos Pods via `metrics-server`.                                                                                                                                                                                                                                                                   | F        | ‚ùå Insuficiente para insights profundos em aplica√ß√µes Java. üí° **Solu√ß√£o:** Al√©m de m√©tricas de infra (CPU/mem√≥ria), usar APM (Application Performance Monitoring) com agentes Java (ex: Prometheus JMX Exporter, OpenTelemetry agent, Dynatrace, New Relic) para coletar m√©tricas da JVM (heap, GC, threads) e tracing de transa√ß√µes.                                                                                                                                                                                                  |
| 184 | A empresa quer ter um ambiente de "disaster recovery" (DR) para seus clusters Kubernetes. A estrat√©gia √© usar o Rancher para fazer backup do `etcd` do cluster prim√°rio e restaur√°-lo em um novo conjunto de VMs no site de DR quando necess√°rio.                                                                                                                                                                        | V        | ‚úÖ Correto, para clusters RKE/K3s. üí° **Solu√ß√£o:** Para clusters provisionados pelo Rancher (RKE/K3s), o backup/restaura√ß√£o do `etcd` √© uma parte fundamental da estrat√©gia de DR. Para clusters de nuvem gerenciados (EKS, GKE), usar as ferramentas de backup/DR do provedor de nuvem. Considerar tamb√©m a replica√ß√£o de dados de volumes persistentes e configura√ß√µes de DNS.                                                                                                                                                        |
| 185 | Para melhorar a utiliza√ß√£o de recursos, a equipe configura `HorizontalPodAutoscaler` para ter `minReplicas: 1` e `maxReplicas: 100` para todas as aplica√ß√µes, independentemente de sua criticidade ou padr√£o de tr√°fego.                                                                                                                                                                                                 | F        | ‚ùå Configura√ß√£o gen√©rica demais, pode ser ineficiente ou arriscada. üí° **Solu√ß√£o:** Ajustar `minReplicas`/`maxReplicas` e as m√©tricas de HPA individualmente por aplica√ß√£o, com base em seus SLAs, padr√µes de carga e custos. Aplica√ß√µes cr√≠ticas podem precisar de `minReplicas` maior para alta disponibilidade.                                                                                                                                                                                                                      |
| 186 | Uma empresa est√° usando o `Cluster Autoscaler` no Kubernetes, mas os n√≥s demoram muito para escalar para cima (add new nodes) durante picos de carga. Aumentar o valor de `scan-interval` no `Cluster Autoscaler` resolver√° o problema.                                                                                                                                                                                  | F        | ‚ùå Diminuir, n√£o aumentar. `scan-interval` √© a frequ√™ncia com que ele verifica a necessidade de escalar. üí° **Solu√ß√£o:** *Diminuir* o `scan-interval` (com cautela, para n√£o sobrecarregar a API da nuvem). Verificar tamb√©m os "expander strategies" e os "node group auto-discovery" para otimizar a adi√ß√£o de n√≥s. Garantir que os `requests` dos Pods estejam corretos para que o autoscaler preveja a necessidade.                                                                                                                 |
| 187 | A empresa quer garantir que todos os `Deployments` no Kubernetes tenham `Liveness` e `Readiness Probes` configuradas. Criar uma pol√≠tica com OPA Gatekeeper ou Kyverno que valide a presen√ßa dessas probes nos manifestos de `Deployment` antes de permitir a cria√ß√£o/atualiza√ß√£o √© uma boa pr√°tica.                                                                                                                     | V        | ‚úÖ Verdadeiro. Isso imp√µe boas pr√°ticas de forma automatizada. üí° **Solu√ß√£o:** Usar um `Admission Controller` com essa pol√≠tica garante que nenhum `Deployment` "esque√ßa" de definir probes, melhorando a resili√™ncia e a detec√ß√£o de problemas nas aplica√ß√µes.                                                                                                                                                                                                                                                                         |
| 188 | Para debug de problemas de rede entre Pods em diferentes n√≥s, a primeira a√ß√£o da equipe √© instalar `tcpdump` em todos os cont√™ineres de aplica√ß√£o.                                                                                                                                                                                                                                                                       | F        | ‚ùå Intrusivo e nem sempre poss√≠vel (imagens scratch/minimal). üí° **Solu√ß√£o:** Usar ferramentas de debug no n√≠vel do *n√≥* (ex: `tcpdump` no host), ou `Ephemeral Containers` com ferramentas de rede. Ferramentas como `ksniff` (plugin `kubectl`) ou `Hubble` (com Cilium CNI) s√£o projetadas para observabilidade de rede em K8s.                                                                                                                                                                                                      |
| 189 | A empresa "Delta" utiliza `Secrets` Kubernetes para armazenar chaves de API de terceiros. Para maior seguran√ßa, eles decidiram usar um sistema de gerenciamento de segredos externo como HashiCorp Vault, integrado ao Kubernetes via Vault Agent Injector ou CSI driver.                                                                                                                                                | V        | ‚úÖ Correto. Solu√ß√µes externas como Vault oferecem gerenciamento de segredos mais robusto. üí° **Solu√ß√£o:** O Vault Agent injeta segredos diretamente nos Pods (ou o CSI driver monta-os como volumes), evitando que fiquem no `etcd` (mesmo criptografados). Vault oferece rota√ß√£o, leasing, auditoria granular de segredos.                                                                                                                                                                                                             |
| 190 | A empresa quer permitir que desenvolvedores criem `Namespaces` dinamicamente para seus projetos de teste. A melhor forma √© dar a eles a permiss√£o `create` no recurso `namespaces` em n√≠vel de cluster.                                                                                                                                                                                                                  | F        | ‚ùå Permiss√£o muito ampla. üí° **Solu√ß√£o:** Implementar um "Namespace-as-a-Service". Pode ser um `Operator` customizado ou um processo via pipeline de CI/CD que cria o `Namespace` e aplica `ResourceQuotas`, `NetworkPolicies` padr√£o, e `RoleBindings` para o desenvolvedor *dentro daquele `Namespace` apenas*.                                                                                                                                                                                                                       |
| 191 | Para controlar custos em clusters de desenvolvimento, a empresa implementa uma pol√≠tica para deletar automaticamente todos os `Namespaces` que n√£o tiveram atividade (novos Pods ou atualiza√ß√µes) por mais de 7 dias.                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro. Pr√°tica comum para evitar desperd√≠cio. üí° **Solu√ß√£o:** Usar um `Operator` ou script customizado que monitore a atividade nos `Namespaces` (ex: √∫ltimo `updateTimestamp` de recursos, eventos) e, ap√≥s um per√≠odo de inatividade e talvez uma notifica√ß√£o, delete o `Namespace` "abandonado".                                                                                                                                                                                                                             |
| 192 | A empresa percebe que a performance de escrita no `etcd` est√° degradando √† medida que o cluster cresce. A primeira medida deve ser aumentar a CPU e mem√≥ria dos n√≥s do `etcd`.                                                                                                                                                                                                                                           | F        | ‚ùå Nem sempre. I/O de disco √© o gargalo mais comum para `etcd`. üí° **Solu√ß√£o:** Garantir que o `etcd` esteja rodando em discos SSD r√°pidos e de baixa lat√™ncia. Monitorar as m√©tricas do `etcd` (lat√™ncias de `fsync`, `wal_fsync_duration_seconds`). Compactar e defragmentar o `etcd` regularmente. Aumentar CPU/mem√≥ria ajuda, mas disco √© fundamental.                                                                                                                                                                              |
| 193 | Uma aplica√ß√£o precisa acessar metadados do Pod onde ela est√° rodando (ex: nome do Pod, `Namespace`, IP do Pod). A forma mais segura e recomendada √© usar a `Downward API` para expor esses metadados como vari√°veis de ambiente ou arquivos no Pod.                                                                                                                                                                      | V        | ‚úÖ Correto. `Downward API` √© para isso. üí° **Solu√ß√£o:** Em vez de dar ao Pod permiss√£o para chamar a API K8s para obter seus pr√≥prios dados, a `Downward API` permite que o `kubelet` projete essas informa√ß√µes diretamente no ambiente do Pod, de forma segura e eficiente.                                                                                                                                                                                                                                                            |
| 194 | A empresa "Epsilon" tem um grande n√∫mero de Helm charts para suas aplica√ß√µes. Para simplificar o gerenciamento de vers√µes e depend√™ncias entre charts, eles decidem usar Helmfile ou um "Chart of Charts" (umbrella chart).                                                                                                                                                                                              | V        | ‚úÖ Verdadeiro. Ambas s√£o abordagens v√°lidas para gerenciar complexidade Helm. üí° **Solu√ß√£o:** Um "Chart of Charts" agrupa m√∫ltiplos charts como depend√™ncias. Helmfile oferece uma forma declarativa de gerenciar o ciclo de vida de m√∫ltiplos charts, seus valores e ambientes.                                                                                                                                                                                                                                                        |
| 195 | Para um `Deployment` com `strategy: RollingUpdate`, o par√¢metro `maxUnavailable` define o n√∫mero m√°ximo de Pods que podem ser criados acima da contagem desejada durante a atualiza√ß√£o, enquanto `maxSurge` define o n√∫mero m√°ximo de Pods que podem estar indispon√≠veis.                                                                                                                                                | F        | ‚ùå Oposto! `maxSurge` √© sobre quantos *acima* (surge = onda). `maxUnavailable` √© sobre quantos podem estar *fora*. üí° **Solu√ß√£o:** `maxSurge`: Quantos Pods extras podem ser criados (ex: "25%" significa que pode ter 125% de Pods momentaneamente). `maxUnavailable`: Quantos Pods podem estar indispon√≠veis (ex: "25%" de 4 r√©plicas significa que 1 pode estar fora, mantendo 3).                                                                                                                                                   |
| 196 | Uma empresa quer usar `Canary Deployments` para liberar novas features gradualmente. Usar um Service Mesh (como Istio) ou um `Ingress Controller` avan√ßado para controlar a porcentagem de tr√°fego direcionada para a vers√£o canary √© uma t√©cnica eficaz.                                                                                                                                                                | V        | ‚úÖ Correto. Essas ferramentas oferecem controle fino de tr√°fego. üí° **Solu√ß√£o:** Configurar regras no Service Mesh ou `Ingress` para enviar uma pequena porcentagem de tr√°fego (ex: 5%) para os Pods da vers√£o canary. Monitorar as m√©tricas. Se tudo ok, aumentar gradualmente a porcentagem at√© 100% e depois desativar a vers√£o antiga.                                                                                                                                                                                              |
| 197 | A empresa nota que alguns `PersistentVolumeClaims` (PVCs) ficam no estado "Pending" indefinidamente. Isso geralmente ocorre porque n√£o h√° `PersistentVolumes` (PVs) dispon√≠veis que satisfa√ßam os requisitos do PVC (tamanho, `accessModes`, `StorageClass`).                                                                                                                                                            | V        | ‚úÖ Verdadeiro. Causa comum de PVCs pendentes. üí° **Solu√ß√£o:** Verificar se a `StorageClass` especificada no PVC existe e est√° funcional. Verificar se h√° PVs est√°ticos dispon√≠veis que correspondam, ou se o provisionador din√¢mico da `StorageClass` est√° funcionando e tem capacidade para criar um novo PV. `kubectl describe pvc NOME_DO_PVC` mostrar√° eventos √∫teis.                                                                                                                                                               |
| 198 | Para garantir a integridade das imagens de cont√™iner, a empresa decide assinar digitalmente todas as imagens antes de envi√°-las para o registry privado e configurar o Kubernetes para verificar essas assinaturas antes de executar os Pods, usando uma ferramenta como Notary ou Sigstore.                                                                                                                             | V        | ‚úÖ Correto. Assinatura de imagens √© uma camada de seguran√ßa importante. üí° **Solu√ß√£o:** Implementar um sistema de assinatura (Notary, Sigstore Cosign). Integrar com um `Admission Controller` (ex: via Kyverno ou Gatekeeper) que valide a assinatura da imagem contra chaves p√∫blicas confi√°veis antes de permitir a execu√ß√£o do Pod.                                                                                                                                                                                                 |
| 199 | Ao usar o `Cluster Autoscaler` com v√°rios `Node Groups` (ou `Auto Scaling Groups` na AWS) com tipos de inst√¢ncia diferentes, o `Cluster Autoscaler` sempre escolher√° o tipo de inst√¢ncia mais barato que atenda aos requisitos do Pod pendente.                                                                                                                                                                          | F        | ‚ùå N√£o necessariamente "sempre o mais barato" por padr√£o. üí° **Solu√ß√£o:** O `Cluster Autoscaler` usa "expanders" para decidir qual `Node Group` escalar. O `least-waste` √© comum (escolhe o grupo que ter√° menos recursos ociosos ap√≥s escalar), mas pode haver o `most-pods` (maximiza Pods), `random`, ou `priority` (baseado em prioridades que voc√™ define). O custo pode ser um fator na defini√ß√£o de prioridades ou tipos de inst√¢ncia nos grupos.                                                                                |
| 200 | Uma empresa quer migrar seus workloads de VMs para Kubernetes. A melhor estrat√©gia √© um "lift and shift" direto, empacotando cada VM inteira como um √∫nico cont√™iner Docker gigante.                                                                                                                                                                                                                                     | F        | ‚ùå Ineficiente e n√£o aproveita os benef√≠cios dos cont√™ineres. üí° **Solu√ß√£o:** Replatforming ou Refactoring √© geralmente melhor. Quebrar as aplica√ß√µes monol√≠ticas das VMs em microsservi√ßos menores e mais gerenci√°veis. Conteinerizar esses microsservi√ßos individualmente. "Lift and shift" de VMs para cont√™ineres (se poss√≠vel) deve ser um passo intermedi√°rio, se muito.                                                                                                                                                          |
| 201 | A empresa "Zeta" est√° preocupada com a possibilidade de um `ServiceAccount` comprometido ser usado para escalar privil√©gios no cluster. A pr√°tica de "least privilege" (conceder apenas as permiss√µes estritamente necess√°rias via RBAC) e auditar regularmente as permiss√µes dos `ServiceAccounts` √© fundamental.                                                                                                       | V        | ‚úÖ Verdadeiro. `ServiceAccounts` s√£o alvos valiosos. üí° **Solu√ß√£o:** Revisar `Roles` e `ClusterRoles` vinculados a `ServiceAccounts`. Evitar usar o `default` `ServiceAccount` com permiss√µes amplas. Usar ferramentas como `kubectl-who-can` ou `rakkess` para auditar permiss√µes. Considerar tokens de `ServiceAccount` com tempo de expira√ß√£o (Bound Service Account Tokens).                                                                                                                                                        |
| 202 | Para um `StatefulSet` que gerencia um banco de dados replicado, durante uma atualiza√ß√£o (`RollingUpdate`), os Pods s√£o atualizados em ordem decrescente (ex: `db-2`, `db-1`, `db-0`) para garantir que o prim√°rio (geralmente o de menor ordinal) seja o √∫ltimo a ser atualizado.                                                                                                                                        | V        | ‚úÖ Correto. Esta √© a ordem padr√£o para `StatefulSets`. üí° **Solu√ß√£o:** A ordem inversa (do maior ordinal para o menor) √© o padr√£o para `RollingUpdate` em `StatefulSets`. Isso √© geralmente mais seguro para sistemas com um l√≠der/prim√°rio no Pod de menor ordinal, permitindo que as r√©plicas sejam atualizadas primeiro.                                                                                                                                                                                                             |
| 203 | Uma empresa usa `kube-prometheus-stack` (que inclui Prometheus, Grafana, Alertmanager) para monitoramento. Para reduzir o "alert fatigue", a equipe decide agrupar alertas relacionados e definir tempos de `repeat_interval` mais longos no Alertmanager.                                                                                                                                                               | V        | ‚úÖ Verdadeiro. Estrat√©gias importantes para gerenciar alertas. üí° **Solu√ß√£o:** Usar `group_by` no Alertmanager para agrupar alertas (ex: por cluster, `Namespace`, app). Definir `group_wait`, `group_interval` e `repeat_interval` apropriadamente para evitar notifica√ß√µes excessivas sobre o mesmo problema persistente. Criar silenciamentos para manuten√ß√µes planejadas.                                                                                                                                                           |
| 204 | Ao configurar `ReadinessProbes` para uma aplica√ß√£o que depende de um servi√ßo externo, √© uma boa pr√°tica fazer a probe verificar a conectividade com esse servi√ßo externo. Se o servi√ßo externo estiver fora, a probe falhar√° e o Pod ser√° reiniciado.                                                                                                                                                                    | F        | ‚ùå Falha na `ReadinessProbe` *n√£o* reinicia o Pod; ela o marca como "NotReady". Falha na `LivenessProbe` reinicia. Verificar depend√™ncias externas na `ReadinessProbe` pode causar "cascading failures" se o externo estiver inst√°vel. üí° **Solu√ß√£o:** `ReadinessProbe` deve focar na sa√∫de da *inst√¢ncia local*. Gerenciar depend√™ncias externas com circuit breakers na aplica√ß√£o ou monitoramento de sa√∫de do endpoint externo.                                                                                                      |
| 205 | A empresa quer usar `Ephemeral Storage` (armazenamento tempor√°rio do n√≥) para seus Pods de forma controlada. Definir `requests` e `limits` para `ephemeral-storage` nos manifestos dos cont√™ineres e `ResourceQuotas` no `Namespace` √© a forma correta de gerenciar isso.                                                                                                                                                | V        | ‚úÖ Correto. `Ephemeral Storage` tamb√©m √© um recurso gerenci√°vel. üí° **Solu√ß√£o:** Sem limites, um Pod pode consumir todo o disco do n√≥ usado para `emptyDir`, logs de cont√™iner, e camadas de imagem, causando instabilidade no n√≥. Definir `requests.ephemeral-storage` e `limits.ephemeral-storage` previne isso.                                                                                                                                                                                                                      |
| 206 | A empresa "√îmega" est√° implantando um `Operator` Kubernetes para gerenciar uma aplica√ß√£o customizada. O `Operator` precisa de permiss√µes para criar, ler, atualizar e deletar os `CustomResources` (CRs) que ele define, al√©m de outros recursos Kubernetes que ele orquestra (ex: `Deployments`, `Services`).                                                                                                           | V        | ‚úÖ Verdadeiro. O `Operator` age em nome do usu√°rio para gerenciar esses recursos. üí° **Solu√ß√£o:** Criar um `ServiceAccount` para o `Operator`. Definir `Roles` ou `ClusterRoles` com as permiss√µes necess√°rias nos CRs e outros recursos. Vincular com `RoleBindings` ou `ClusterRoleBindings`. Seguir o princ√≠pio de menor privil√©gio.                                                                                                                                                                                                 |
| 207 | Para otimizar o agendamento de Pods que t√™m requisitos de localidade de dados (data locality) com `PersistentVolumes`, a funcionalidade `VolumeBindingMode: WaitForFirstConsumer` na `StorageClass` √© recomendada.                                                                                                                                                                                                       | V        | ‚úÖ Correto. Isso atrasa o binding do PV at√© que um Pod seja agendado. üí° **Solu√ß√£o:** Com `WaitForFirstConsumer`, o `Scheduler` considera a topologia do n√≥ (ex: zona) ao escolher um n√≥ para o Pod, e *s√≥ ent√£o* o PV √© provisionado (se din√¢mico) ou vinculado (se est√°tico) naquela zona, garantindo que o Pod e seu storage estejam pr√≥ximos.                                                                                                                                                                                       |
| 208 | Uma empresa quer usar o Rancher para provisionar clusters K3s em dispositivos de borda (edge). O K3s, por ser leve e ter requisitos reduzidos, √© uma escolha adequada para esses cen√°rios.                                                                                                                                                                                                                               | V        | ‚úÖ Verdadeiro. K3s foi projetado com a borda em mente. üí° **Solu√ß√£o:** K3s √© uma distribui√ß√£o Kubernetes CNCF-certified, mas com bin√°rio √∫nico, depend√™ncias reduzidas (usa `containerd`, SQLite por padr√£o), e menor consumo de recursos, tornando-o ideal para Raspberry Pis, dispositivos IoT, e outros cen√°rios de borda. Rancher pode gerenciar esses clusters K3s centralmente.                                                                                                                                                   |
| 209 | Para troubleshooting, um engenheiro executa `kubectl delete pod MEU_POD --force --grace-period=0` para remover rapidamente um Pod problem√°tico. Esta √© a forma mais segura de garantir que o Pod seja removido sem efeitos colaterais.                                                                                                                                                                                   | F        | ‚ùå Extremamente perigoso! Isso pode causar corrup√ß√£o de dados ou deixar recursos √≥rf√£os. üí° **Solu√ß√£o:** Evitar `--force` e `--grace-period=0` a menos que seja o √∫ltimo recurso e se entenda as consequ√™ncias. Permitir que o Pod termine graciosamente (respeitando o `terminationGracePeriodSeconds`) permite que ele limpe conex√µes, salve estado, etc. Se um Pod est√° preso em `Terminating`, investigue a causa (ex: `finalizers`, problemas no n√≥).                                                                              |
| 210 | A empresa usa o `Ingress-Nginx Controller`. Para proteger contra ataques de nega√ß√£o de servi√ßo (DoS) na camada de aplica√ß√£o, eles podem configurar limites de taxa (`rate limiting`) diretamente nas annotations do recurso `Ingress`.                                                                                                                                                                                   | V        | ‚úÖ Correto. `Ingress-Nginx` suporta isso via annotations. üí° **Solu√ß√£o:** Usar annotations como `nginx.ingress.kubernetes.io/limit-rps` (requests per second) ou `nginx.ingress.kubernetes.io/limit-rpm` (requests per minute) no `Ingress` para instruir o Nginx a aplicar limites de taxa, ajudando a mitigar DoS e tr√°fego abusivo.                                                                                                                                                                                                  |
| 211 | Uma empresa precisa de uma forma de expor m√∫ltiplos servi√ßos gRPC rodando no Kubernetes atrav√©s de um √∫nico endere√ßo IP externo e porta, com termina√ß√£o TLS. Um `Ingress Controller` tradicional focado em HTTP/1.1 √© a melhor solu√ß√£o para isso.                                                                                                                                                                        | F        | ‚ùå `Ingress` padr√£o √© primariamente para HTTP/S. gRPC sobre HTTP/2 tem requisitos diferentes. üí° **Solu√ß√£o:** Alguns `Ingress Controllers` mais modernos (ex: Nginx com configura√ß√£o espec√≠fica, Traefik, Contour, Emissary-ingress) ou Gateways de API (como os de Service Meshes tipo Istio Gateway) suportam gRPC e HTTP/2 de forma mais nativa, incluindo multiplexa√ß√£o de streams em uma √∫nica conex√£o.                                                                                                                            |
| 212 | Os `HorizontalPodAutoscaler` (HPA) e `VerticalPodAutoscaler` (VPA) podem ser usados simultaneamente para o mesmo workload no Kubernetes, com o HPA gerenciando o n√∫mero de r√©plicas e o VPA ajustando os `requests` de CPU/mem√≥ria das r√©plicas.                                                                                                                                                                         | V        | ‚úÖ Verdadeiro, com ressalvas e em certas configura√ß√µes. üí° **Solu√ß√£o:** Historicamente, havia conflitos. Vers√µes mais recentes do VPA podem operar em modo "Recommendation" (apenas sugere `requests`) enquanto o HPA atua. A integra√ß√£o est√° melhorando, mas exige configura√ß√£o cuidadosa e entendimento de como eles interagem, especialmente se o VPA estiver no modo "Auto" ou "Update".                                                                                                                                            |
| 213 | Para garantir alta disponibilidade do Control Plane do Kubernetes em um cluster auto-gerenciado, √© suficiente ter m√∫ltiplos `kube-apiserver` e `kube-controller-manager` rodando, mesmo que usando um √∫nico n√≥ `etcd`.                                                                                                                                                                                                   | F        | ‚ùå `etcd` tamb√©m precisa ser HA. `etcd` √© o "c√©rebro". üí° **Solu√ß√£o:** Para um Control Plane HA, voc√™ precisa de um cluster `etcd` com m√∫ltiplos membros (idealmente 3 ou 5), e m√∫ltiplos `kube-apiserver`, `kube-scheduler`, e `kube-controller-manager` distribu√≠dos entre os n√≥s master, geralmente com um balanceador de carga na frente dos `api-servers`.                                                                                                                                                                         |
| 214 | A empresa quer usar `Jobs` Kubernetes para processar arquivos enviados para um bucket S3. Um `Job` pode ser configurado para rodar, processar um arquivo, e terminar. Para processar m√∫ltiplos arquivos de forma cont√≠nua, um `CronJob` que verifica o bucket periodicamente e cria `Jobs` √© uma abordagem inadequada.                                                                                                   | F        | ‚ùå Um `CronJob` *pode* ser uma abordagem, mas pode n√£o ser a mais eficiente ou reativa. üí° **Solu√ß√£o:** `CronJob` √© vi√°vel se um pequeno delay √© aceit√°vel. Alternativas mais reativas incluem: usar um sistema de filas (ex: SQS, Kafka) onde um evento de novo arquivo dispara um processador (que pode ser um `Job` K8s escalado por KEDA), ou um `Operator` que monitora o bucket.                                                                                                                                                  |
| 215 | Uma empresa precisa que certos Pods de administra√ß√£o (ex: para executar backups ou diagn√≥sticos) possam ser agendados mesmo em n√≥s que est√£o com `taints` `NoSchedule` ou que est√£o sob press√£o de recursos. Configurar esses Pods com alta `PriorityClass` e `Tolerations` apropriadas √© a solu√ß√£o.                                                                                                                     | V        | ‚úÖ Correto. `PriorityClass` e `Tolerations` d√£o "superpoderes" de agendamento. üí° **Solu√ß√£o:** Uma `PriorityClass` alta permite que o Pod preempte (remova) Pods de prioridade mais baixa se necess√°rio. `Tolerations` permitem que ele seja agendado em n√≥s com `taints` correspondentes. Essencial para workloads cr√≠ticos do sistema ou de administra√ß√£o.                                                                                                                                                                            |
| 216 | Durante a an√°lise de um incidente em um cluster Kubernetes, um perito descobre um `Pod` suspeito. A primeira a√ß√£o deve ser deletar o `Pod` imediatamente usando `kubectl delete pod NOME_DO_POD --force` para conter a amea√ßa.                                                                                                                                                                                           | F        | ‚ùå Conten√ß√£o √© importante, mas a dele√ß√£o imediata e for√ßada destr√≥i evid√™ncias vol√°teis cruciais. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Isolar o `Pod` (ex: com `NetworkPolicy` agressiva), realizar um dump de mem√≥ria do cont√™iner (se poss√≠vel), coletar logs do `Pod` (`kubectl logs`), descrever o `Pod` (`kubectl describe`), e obter seu YAML. Apenas depois da coleta preliminar, considerar a conten√ß√£o mais dr√°stica, preservando o estado do n√≥ se poss√≠vel.                                                                             |
| 217 | Um perito investiga um acesso n√£o autorizado ao `kube-apiserver`. A an√°lise dos logs de auditoria do `kube-apiserver` √© fundamental para identificar o IP de origem, o `User-Agent`, as a√ß√µes realizadas e as contas de usu√°rio ou `ServiceAccount` utilizadas pelo atacante.                                                                                                                                            | V        | ‚úÖ Verdadeiro. Logs de auditoria s√£o ouro para o perito. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Coletar e analisar os logs de auditoria do K8s (se habilitados e configurados corretamente). Focar em eventos de autentica√ß√£o, cria√ß√£o/modifica√ß√£o de `Secrets`, `Roles`, `RoleBindings`, e `Pods` suspeitos. Correlacionar com timestamps de outros logs (ex: firewall, VPN).                                                                                                                                                                       |
| 218 | Em um caso de exfiltra√ß√£o de dados via um cont√™iner comprometido, o perito foca apenas nos logs da aplica√ß√£o dentro do cont√™iner. Os logs do `kubelet` e do `container runtime` (Docker, containerd) no n√≥ hospedeiro s√£o irrelevantes, pois o cont√™iner √© isolado.                                                                                                                                                      | F        | ‚ùå Logs do n√≥ s√£o muito relevantes. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Logs do `kubelet` podem mostrar eventos de cria√ß√£o/parada do cont√™iner, falhas, e montagem de volumes. Logs do `container runtime` podem detalhar intera√ß√µes de baixo n√≠vel do cont√™iner. Estes, junto com logs de rede do n√≥ e `syslog`, podem revelar comunica√ß√µes externas n√£o registradas pela aplica√ß√£o.                                                                                                                                                             |
| 219 | Um perito suspeita que um `Deployment` foi modificado para incluir um cont√™iner malicioso com um crypto miner. Comparar o manifesto YAML atual do `Deployment` (obtido com `kubectl get deployment NOME -o yaml`) com vers√µes anteriores armazenadas em um sistema de controle de vers√£o (Git) √© uma t√©cnica v√°lida para identificar a altera√ß√£o maliciosa.                                                              | V        | ‚úÖ Correto. Versionamento √© chave para detectar altera√ß√µes. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se a empresa usa GitOps ou versiona manifestos, a compara√ß√£o (diff) revelar√° a adi√ß√£o do cont√™iner, a imagem usada, e possivelmente o autor do commit. Se n√£o houver versionamento, analisar os `managedFields` ou logs de auditoria do `kube-apiserver` para o `Deployment` pode ajudar a rastrear a mudan√ßa.                                                                                                                                    |
| 220 | Foi identificado um `Secret` Kubernetes contendo credenciais expostas. A a√ß√£o pericial correta √© alterar o valor do `Secret` diretamente no `etcd` usando `etcdctl` para invalidar as credenciais imediatamente.                                                                                                                                                                                                         | F        | ‚ùå Alterar evid√™ncias diretamente √© problem√°tico e arriscado. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Preservar o estado atual do `Secret` (ex: `kubectl get secret NOME -o yaml`). Identificar onde ele √© usado. Coordenar com a equipe de seguran√ßa/opera√ß√µes para *revogar as credenciais no sistema de origem* (ex: API de terceiro, banco de dados) e *depois* atualizar o `Secret` via `kubectl` (ou o m√©todo padr√£o de gest√£o de segredos da empresa, ex: Vault) com novas credenciais. A integridade do `etcd` deve ser mantida para an√°lise. |
| 221 | Um perito precisa analisar o tr√°fego de rede de um `Pod` espec√≠fico sem instalar ferramentas no cont√™iner (que √© `scratch`). Capturar o tr√°fego na interface de rede do *n√≥* hospedeiro e filtrar pelo IP do `Pod` √© uma abordagem forense v√°lida.                                                                                                                                                                       | V        | ‚úÖ Verdadeiro. An√°lise no n√≠vel do n√≥ √© menos intrusiva. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Identificar o IP do `Pod` e a interface de rede virtual do `Pod` no n√≥ (ex: `vethXXX`). Usar `tcpdump` ou `tshark` no n√≥, filtrando pelo IP do `Pod` ou pela interface `veth`. Considerar o CNI usado, pois ele influencia como o tr√°fego √© roteado.                                                                                                                                                                                                 |
| 222 | Durante uma resposta a incidente, um perito decide fazer um snapshot de um `PersistentVolume` (PV) associado a um `Pod` comprometido. Se o PV for um disco EBS na AWS, o snapshot criado via API da AWS ou console √© forensemente aceit√°vel, mesmo que o `Pod` ainda esteja escrevendo no volume.                                                                                                                        | F        | ‚ùå Snapshot "a quente" pode ter dados inconsistentes. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Idealmente, "congelar" o `Pod` (parar escritas, ex: `kubectl scale deployment --replicas=0` se for um `Deployment`, ou pausar o processo no cont√™iner) *antes* de tirar o snapshot para garantir consist√™ncia do filesystem. Se n√£o for poss√≠vel, documentar que o snapshot foi "crash-consistent" e pode exigir ferramentas de recupera√ß√£o de filesystem. A cadeia de cust√≥dia do snapshot √© vital.                                                    |
| 223 | Um atacante obteve acesso a um `ServiceAccount` com permiss√µes para listar `Secrets` em todos os `Namespaces`. O perito deve focar a investiga√ß√£o nos logs de auditoria do `kube-apiserver` procurando por requisi√ß√µes de `list` ou `get` no recurso `secrets` originadas por esse `ServiceAccount`.                                                                                                                     | V        | ‚úÖ Correto. Rastrear o uso de permiss√µes √© fundamental. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Filtrar os logs de auditoria pelo `username` do `ServiceAccount` (ex: `system:serviceaccount:NAMESPACE:NOME_SA`) e pelos `verb` (`list`, `get`) e `resource` (`secrets`). Analisar os IPs de origem e timestamps para correlacionar com outras atividades.                                                                                                                                                                                            |
| 224 | Um perito encontra um arquivo YAML suspeito em um reposit√≥rio Git usado para GitOps com Argo CD/Flux. O arquivo define um `CronJob` que executa um script para exfiltrar dados. Apenas analisar o c√≥digo do script √© suficiente; o `CronJob` em si n√£o oferece informa√ß√µes forenses relevantes.                                                                                                                          | F        | ‚ùå O `CronJob` YAML √© muito relevante. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Analisar o YAML do `CronJob` para entender: a `schedule` (quando executa), a imagem do cont√™iner usada, os `ServiceAccount` montados (permiss√µes!), volumes montados (acesso a dados/`Secrets`), e vari√°veis de ambiente que podem ser usadas pelo script. Isso contextualiza a amea√ßa do script.                                                                                                                                                                      |
| 225 | Em um cluster Rancher, o perito suspeita que um usu√°rio do Rancher abusou de suas permiss√µes para acessar um cluster downstream. Os logs de auditoria do *Rancher Server* e os logs de auditoria do `kube-apiserver` do *cluster downstream* devem ser coletados e correlacionados.                                                                                                                                      | V        | ‚úÖ Verdadeiro. Investiga√ß√£o em duas camadas. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Logs do Rancher Server mostrar√£o as a√ß√µes do usu√°rio na UI/API do Rancher (ex: quem acessou qual cluster, quem executou `kubectl` via UI). Logs do `kube-apiserver` do cluster downstream mostrar√£o as a√ß√µes efetivas realizadas no cluster, possivelmente por um `ServiceAccount` que o Rancher usa para se comunicar com o cluster, mas o log de auditoria do K8s pode ter o `impersonatedUser` do Rancher.                                                    |
| 226 | Um cont√™iner Docker foi usado para um ataque e depois removido (`docker rm`). O perito pode recuperar facilmente o conte√∫do completo do filesystem do cont√™iner removido a partir das camadas de imagem armazenadas no n√≥ Docker.                                                                                                                                                                                        | F        | ‚ùå Dif√≠cil e nem sempre completo. A camada de escrita (read-write layer) do cont√™iner √© perdida. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** As camadas da *imagem* base podem ser recuperadas. A camada de escrita espec√≠fica do cont√™iner (onde as modifica√ß√µes e novos arquivos foram criados) √© geralmente removida. Pode haver resqu√≠cios no filesystem do n√≥ (ex: `overlay2` diffs n√£o totalmente limpos), mas n√£o √© garantido. Foco em logs e metadados do runtime.                                                                                |
| 227 | Um perito est√° analisando um dump de mem√≥ria de um n√≥ Kubernetes onde um cont√™iner malicioso rodou. Procurar por processos do cont√™iner, conex√µes de rede ativas no momento do dump, e strings relacionadas a comandos maliciosos ou IPs de C2 s√£o passos v√°lidos na an√°lise de mem√≥ria.                                                                                                                                 | V        | ‚úÖ Correto. An√°lise de mem√≥ria pode revelar artefatos vol√°teis. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Usar ferramentas como Volatility ou Rekall. Identificar processos no `namespace` do cont√™iner. Extrair informa√ß√µes de rede (`netscan`). Procurar por m√≥dulos de kernel suspeitos (rootkits). Analisar hist√≥rico de comandos se presente na mem√≥ria.                                                                                                                                                                                           |
| 228 | Um atacante modificou um `ValidatingAdmissionWebhook` para permitir a cria√ß√£o de `Pods` privilegiados. O perito deve apenas analisar os logs do webhook em si, pois ele √© o respons√°vel pela decis√£o final.                                                                                                                                                                                                              | F        | ‚ùå Logs do `kube-apiserver` s√£o cruciais. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Logs do `kube-apiserver` mostrar√£o as requisi√ß√µes chegando, a decis√£o do webhook (permitir/negar), e se houve erros na chamada ao webhook. Logs do webhook podem estar adulterados ou incompletos. Comparar o comportamento esperado do webhook com as decis√µes registradas no `api-server`. Verificar a configura√ß√£o do `ValidatingWebhookConfiguration`.                                                                                                          |
| 229 | Um perito identifica um `Pod` com `hostPID: true` e `hostNetwork: true`. Isso indica uma tentativa de escalada de privil√©gios, pois permite ao `Pod` ver todos os processos do n√≥ e usar a interface de rede do n√≥ diretamente, bypassando `NetworkPolicies` do `Pod`.                                                                                                                                                   | V        | ‚úÖ Verdadeiro. Configura√ß√µes de alto risco. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Investigar o prop√≥sito desse `Pod`. Analisar a imagem do cont√™iner e os comandos executados. Verificar se o `ServiceAccount` usado tem permiss√µes excessivas. Coletar logs do `Pod` e do n√≥ para atividade suspeita. Tais configura√ß√µes s√£o leg√≠timas apenas para alguns agentes de sistema (ex: CNI, monitoramento de n√≥).                                                                                                                                       |
| 230 | Ao coletar logs de um cluster Kubernetes para an√°lise forense, √© suficiente copiar os arquivos de log de `/var/log` nos n√≥s master e worker.                                                                                                                                                                                                                                                                             | F        | ‚ùå Incompleto. Logs de componentes K8s podem estar em outros locais ou gerenciados pelo systemd/journald. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Al√©m de `/var/log`, usar `journalctl` para logs de `kubelet`, `container runtime`. Para o Control Plane, logs de `kube-apiserver`, `kube-scheduler`, `kube-controller-manager`, `etcd` (se n√£o centralizados). Logs de auditoria do K8s (se habilitados) s√£o um stream separado, geralmente configurado para um arquivo ou webhook.                                                                 |
| 231 | Um perito suspeita que um `ConfigMap` foi usado para armazenar um script malicioso que √© depois executado por um `Pod`. Analisar o conte√∫do do `ConfigMap` (`kubectl get configmap NOME -o yaml`) e identificar `Pods` que o montam como volume ou usam seus dados como vari√°veis de ambiente √© um passo crucial.                                                                                                        | V        | ‚úÖ Correto. `ConfigMaps` podem ser vetores. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Decodificar os dados do `ConfigMap` (se base64). Procurar por scripts, bin√°rios ou configura√ß√µes suspeitas. Usar `kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.namespace}{"\t"}{.metadata.name}{"\t"}{.spec.volumes[*].configMap.name}{"\n"}{end}'` (ou similar) para listar `Pods` que montam `ConfigMaps`.                                                                                                                                      |
| 232 | Um atacante ganhou acesso ao `etcd` de um cluster Kubernetes e modificou diretamente um `Deployment` para usar uma imagem maliciosa. Os logs de auditoria do `kube-apiserver` n√£o mostrar√£o essa modifica√ß√£o, pois o `api-server` n√£o foi envolvido.                                                                                                                                                                     | V        | ‚úÖ Verdadeiro, se a modifica√ß√£o foi *diretamente* no `etcd`. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Este √© um cen√°rio avan√ßado. Se o `etcd` foi comprometido e alterado por baixo dos panos, o `api-server` pode n√£o registrar a mudan√ßa em seus logs de auditoria. A detec√ß√£o viria de: monitoramento de integridade do `etcd`, compara√ß√£o do estado no `etcd` com o estado esperado (GitOps), ou comportamento an√¥malo do `Deployment`. An√°lise forense do `etcd` (se houver logs ou snapshots) seria necess√°ria.                                  |
| 233 | Um perito precisa determinar se um `ServiceAccount` token foi usado externamente para acessar o cluster. Analisar logs de acesso √† rede para o `kube-apiserver` (ex: logs de firewall, load balancer na frente do API) correlacionados com os IPs de origem nos logs de auditoria do K8s pode ajudar a identificar acessos de fora da rede do cluster.                                                                   | V        | ‚úÖ Correto. Correlacionar logs de rede e K8s. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Filtrar logs de auditoria do K8s por `authenticationInfo.username` do `ServiceAccount`. Extrair os `sourceIPs`. Comparar esses IPs com faixas de IP internas conhecidas. IPs externos indicam poss√≠vel uso externo do token. Verificar os logs de rede do `api-server` para esses IPs e os `User-Agents` associados.                                                                                                                                            |
| 234 | Durante a an√°lise de um n√≥ comprometido, o perito encontra v√°rios cont√™ineres parados, mas n√£o removidos. A melhor forma de obter informa√ß√µes sobre eles √© executar `docker start` nesses cont√™ineres para inspecionar seu estado.                                                                                                                                                                                       | F        | ‚ùå Iniciar cont√™ineres parados pode alterar evid√™ncias (ex: timestamps, estado do filesystem) ou reativar malware. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Usar `docker inspect NOME_OU_ID_CONTAINER_PARADO` para obter metadados. Se poss√≠vel, exportar o filesystem do cont√™iner parado (`docker export`) para an√°lise offline. Se o runtime for `containerd`, usar `ctr` para inspecionar.                                                                                                                                                         |
| 235 | Um perito investiga um ataque de "supply chain" onde uma imagem popular do Docker Hub foi comprometida e continha malware. Verificar os `imageID` (digest SHA256) das imagens rodando no cluster e compar√°-los com os `imageID` conhecidos da imagem leg√≠tima e da comprometida √© uma t√©cnica v√°lida.                                                                                                                    | V        | ‚úÖ Verdadeiro. O `imageID` (digest) √© um identificador √∫nico do conte√∫do da imagem. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Obter a lista de `imageID` de todos os Pods (`kubectl get pods -A -o jsonpath='{range .items[*].status.containerStatuses[*]}{.imageID}{"\n"}{end}'`). Comparar com os digests conhecidos. Ferramentas de scan de vulnerabilidade que tamb√©m checam digests de imagens conhecidas como maliciosas podem ajudar.                                                                                                            |
| 236 | Um atacante usou `kubectl exec` para executar comandos em um `Pod`. Os logs de auditoria do `kube-apiserver` mostrar√£o uma requisi√ß√£o para criar uma sub-recurso `exec` no `Pod`, incluindo os comandos exatos que foram executados como parte da requisi√ß√£o.                                                                                                                                                            | F        | ‚ùå Os comandos exatos *n√£o* s√£o logados na requisi√ß√£o `exec` em si nos logs de auditoria por padr√£o. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** O log de auditoria mostrar√° *que* uma sess√£o `exec` foi iniciada, por quem, e para qual `Pod`/cont√™iner. Para os comandos *dentro* da sess√£o `exec`, seria necess√°rio: logs do shell no cont√™iner (se houver e n√£o foram apagados), `tty` logging (raro), ou EDR/monitoramento de processos no n√≥ que capture execu√ß√µes dentro do cont√™iner.                                                             |
| 237 | Um perito encontra um `Pod` configurado com um `securityContext.privileged: true`. Isso, por si s√≥, n√£o √© uma vulnerabilidade, pois o Kubernetes gerencia o isolamento mesmo para `Pods` privilegiados.                                                                                                                                                                                                                  | F        | ‚ùå `privileged: true` √© uma vulnerabilidade significativa. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Um `Pod` privilegiado tem acesso a todos os dispositivos do n√≥ e bypassa muitas das prote√ß√µes do Docker/kernel. √â quase como rodar como `root` no n√≥. Investigar por que ele √© privilegiado, o que ele faz, e se foi explorado para escapar do cont√™iner ou atacar o n√≥.                                                                                                                                                                           |
| 238 | Para preservar a evid√™ncia de um `Pod` Kubernetes que precisa ser parado, a melhor abordagem √© usar `kubectl drain NOME_DO_NO_DO_POD` para que o `Pod` seja terminado graciosamente e suas informa√ß√µes preservadas pelo `kubelet`.                                                                                                                                                                                       | F        | ‚ùå `drain` √© para manuten√ß√£o de n√≥, n√£o para preserva√ß√£o forense de um `Pod` espec√≠fico. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** `drain` vai deletar o `Pod` (ap√≥s `grace period`). Para preservar: isolar, coletar logs/YAML/mem√≥ria do `Pod`, snapshot de PVs. Se for preciso parar o `Pod` (ap√≥s coleta), `kubectl scale deployment/statefulset SELETOR_DO_POD --replicas=0` √© mais controlado do que um `drain` no n√≥ inteiro.                                                                                                                    |
| 239 | Um perito analisa um incidente onde um atacante explorou uma vulnerabilidade SSRF em uma aplica√ß√£o rodando em um `Pod` para interagir com o `kubelet` API do n√≥ onde o `Pod` estava. Os logs do `kubelet` no n√≥ s√£o a principal fonte para investigar essa atividade.                                                                                                                                                    | V        | ‚úÖ Correto. `kubelet` API pode ser um alvo. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Analisar logs do `kubelet` (geralmente via `journalctl -u kubelet`) por requisi√ß√µes suspeitas (ex: para `/pods`, `/run`, `/exec`, `/attach`). Verificar se o `kubelet` estava configurado para permitir acesso an√¥nimo ou se o `ServiceAccount` do `Pod` tinha permiss√µes para falar com o `kubelet` (menos comum, mas poss√≠vel).                                                                                                                                 |
| 240 | Em um cluster com Rancher, um perito precisa verificar quais usu√°rios do Rancher t√™m permiss√£o para acessar um cluster Kubernetes espec√≠fico e quais pap√©is (roles) eles possuem nesse cluster. Consultar a configura√ß√£o de "Cluster Members" na UI do Rancher ou via API do Rancher √© o caminho.                                                                                                                        | V        | ‚úÖ Verdadeiro. Rancher gerencia seu pr√≥prio RBAC sobre o K8s. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** A UI/API do Rancher mostrar√° os usu√°rios/grupos do Rancher e os pap√©is de cluster (ex: Cluster Owner, Cluster Member, ou pap√©is customizados) atribu√≠dos a eles para o cluster em quest√£o. Isso √© mapeado para `RoleBindings` / `ClusterRoleBindings` no K8s, mas a vis√£o do Rancher √© o ponto de partida.                                                                                                                                      |
| 241 | Um atacante criou um `RoleBinding` que concede ao `ServiceAccount` `default` do `Namespace` `kube-system` o papel `cluster-admin`. Para detectar isso, o perito deve procurar por eventos de cria√ß√£o de `RoleBinding` ou `ClusterRoleBinding` nos logs de auditoria do K8s que referenciem `system:serviceaccount:kube-system:default` e `cluster-admin`.                                                                | V        | ‚úÖ Correto. Escalada de privil√©gios cl√°ssica. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Essa √© uma t√©cnica de persist√™ncia/escalada perigosa. Filtrar logs de auditoria por: `objectRef.resource: "rolebindings" OR objectRef.resource: "clusterrolebindings"`, `verb: "create" OR verb: "update"`. Inspecionar o `requestObject` para o `subjects[].name` e `roleRef.name`.                                                                                                                                                                            |
| 242 | Um perito precisa analisar o conte√∫do de um `etcd` comprometido. A melhor forma √© conectar-se diretamente ao `etcd` em produ√ß√£o com `etcdctl` e listar todas as chaves.                                                                                                                                                                                                                                                  | F        | ‚ùå Arriscado e pode n√£o ser forensemente s√£o. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se o `etcd` est√° comprometido, o ideal √© obter um *snapshot* do `etcd` (`etcdctl snapshot save`). Analisar esse snapshot *offline* em um ambiente seguro. Conectar-se ao `etcd` vivo pode alterar timestamps de acesso ou perder dados se o atacante estiver ativo. A cadeia de cust√≥dia do snapshot √© crucial.                                                                                                                                                 |
| 243 | Um `Pod` foi observado fazendo conex√µes de sa√≠da para um IP malicioso conhecido. O perito pode usar `NetworkPolicies` para bloquear imediatamente essa comunica√ß√£o de sa√≠da do `Pod` (ou de todos os `Pods` no `Namespace`) como medida de conten√ß√£o, enquanto continua a investiga√ß√£o.                                                                                                                                  | V        | ‚úÖ Verdadeiro. `NetworkPolicies` s√£o para isso. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Criar uma `NetworkPolicy` de `egress` que negue tr√°fego para o IP/CIDR malicioso, aplicada ao `Pod` (via `podSelector`) ou ao `Namespace`. Isso ajuda a conter a amea√ßa. Continuar a an√°lise de logs de rede, `Pod`, e n√≥ para entender a origem e o escopo do comprometimento.                                                                                                                                                                               |
| 244 | Durante a an√°lise de um ataque a um `Ingress Controller`, o perito foca apenas nos logs de acesso do `Ingress Controller` (ex: logs do Nginx). Os logs dos `Services` e `Pods` backend n√£o s√£o importantes, pois o `Ingress` √© o ponto de entrada.                                                                                                                                                                       | F        | ‚ùå Logs backend s√£o muito importantes. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Logs do `Ingress` mostram o que chegou na borda. Logs dos `Services`/`Pods` backend mostram o que foi efetivamente processado, se houve erros, ou se a requisi√ß√£o maliciosa atingiu a aplica√ß√£o. Correlacionar os dois (ex: por `x-request-id` ou timestamps) √© essencial.                                                                                                                                                                                             |
| 245 | Um perito suspeita que um atacante est√° usando `kubectl port-forward` para exfiltrar dados de um `Pod`. Os logs de auditoria do `kube-apiserver` registrar√£o a cria√ß√£o da conex√£o `port-forward` (sub-recurso `portforward` do `Pod`), mas n√£o o volume de dados transferido.                                                                                                                                            | V        | ‚úÖ Correto. Auditoria K8s foca no controle, n√£o no data plane. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** O log de auditoria mostrar√° o evento de `port-forward`. Para o volume de dados, seria necess√°rio analisar logs de rede do *n√≥* onde o `Pod` est√° rodando, ou do firewall/IDS na borda, se o `port-forward` foi exposto externamente ou se o tr√°fego foi roteado para fora.                                                                                                                                                                     |
| 246 | Um atacante deletou os logs de um cont√™iner (`/var/log/app.log` dentro do cont√™iner) antes de remover o `Pod`. Se a empresa n√£o tem coleta de logs centralizada, a evid√™ncia desses logs est√° permanentemente perdida.                                                                                                                                                                                                   | V        | ‚úÖ Verdadeiro, na maioria dos casos. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se os logs estavam apenas no filesystem ef√™mero do cont√™iner e foram deletados *antes* da remo√ß√£o do `Pod` (e o `Pod` foi ent√£o removido, destruindo sua camada de escrita), e n√£o h√° coleta centralizada (Fluentd, etc.), a recupera√ß√£o √© improv√°vel. Isso refor√ßa a import√¢ncia da coleta centralizada e em tempo real.                                                                                                                                                |
| 247 | Um perito est√° investigando um cluster K3s. Como K3s usa SQLite por padr√£o em vez de `etcd`, n√£o h√° logs de transa√ß√£o ou snapshots compar√°veis aos do `etcd` para an√°lise forense do estado do cluster.                                                                                                                                                                                                                  | F        | ‚ùå SQLite tem mecanismos de log e backup. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** K3s com SQLite ainda mant√©m o estado do cluster. O arquivo SQLite (`/var/lib/rancher/k3s/server/db/state.db`) e seus logs de write-ahead (`-wal`, `-shm`) podem ser coletados. Backups do SQLite (se configurados pelo K3s ou manualmente) tamb√©m s√£o fontes de evid√™ncia. A an√°lise √© diferente do `etcd`, mas n√£o inexistente.                                                                                                                                    |
| 248 | Foi detectada uma imagem de cont√™iner em um registry privado da empresa que foi substitu√≠da (mesma tag, digest diferente) por uma vers√£o maliciosa. O perito deve analisar os logs de auditoria do *registry de cont√™ineres* (ex: Harbor, Artifactory) para identificar quem e quando realizou o push da imagem maliciosa.                                                                                               | V        | ‚úÖ Correto. O registry √© a fonte da verdade para pushes de imagem. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Registries modernos t√™m logs de auditoria que registram pushes, pulls, dele√ß√µes de tags, etc., incluindo o usu√°rio, IP de origem, e timestamp. Isso √© crucial para rastrear a origem do comprometimento da imagem.                                                                                                                                                                                                                         |
| 249 | Um perito precisa analisar um `PersistentVolume` (PV) do tipo `hostPath` que foi usado por um `Pod` malicioso. A an√°lise deve se restringir ao diret√≥rio especificado no `hostPath` no n√≥, pois o `Pod` n√£o tem acesso a outras √°reas do filesystem do n√≥.                                                                                                                                                               | F        | ‚ùå Se o `Pod` era privilegiado ou escapou, pode ter acessado mais. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** O diret√≥rio `hostPath` √© o ponto de partida. No entanto, se o `Pod` rodou como `root` no `Namespace` do host (ex: com `hostPID: true`, `privileged: true`) ou explorou uma vulnerabilidade de kernel, ele poderia ter acessado/modificado outras partes do filesystem do n√≥. Analisar logs do sistema do n√≥ e procurar por outros indicadores de comprometimento no n√≥ √© essencial.                                                        |
| 250 | Um atacante usou um `ServiceAccount` token v√°lido, obtido de um `Pod` comprometido, para criar um novo `Pod` em um `Namespace` diferente. Os logs de auditoria do `kube-apiserver` mostrar√£o a cria√ß√£o do `Pod` com o `username` do `ServiceAccount` usado, e o `requestObject` mostrar√° o `Namespace` alvo do novo `Pod`.                                                                                               | V        | ‚úÖ Verdadeiro. Auditoria registra esses detalhes. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Verificar se o `ServiceAccount` original *tinha* permiss√£o (via `Roles`/`ClusterRoles`) para criar `Pods` no `Namespace` alvo. Se n√£o, isso indica uma falha no RBAC ou que o atacante encontrou outra forma de escalar permiss√µes ou usar um token mais privilegiado.                                                                                                                                                                                      |
| 251 | Um perito examina um `Pod` com um `initContainer` que baixa um bin√°rio da internet e o executa. Para an√°lise forense, apenas o bin√°rio baixado precisa ser analisado; o `initContainer` em si n√£o deixa rastros significativos ap√≥s sua conclus√£o.                                                                                                                                                                       | F        | ‚ùå O `initContainer` pode deixar rastros. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Analisar a defini√ß√£o do `initContainer` no YAML do `Pod`: qual imagem ele usou, de onde baixou o bin√°rio (URL), quais comandos executou. Se o `initContainer` escreveu em um `emptyDir` volume compartilhado com o cont√™iner principal, artefatos podem estar l√°. Logs do `initContainer` (`kubectl logs NOME_POD -c NOME_INIT_CONTAINER --previous` se ele falhou/reiniciou) tamb√©m podem ser √∫teis.                                                               |
| 252 | Em um incidente de ransomware que afetou volumes persistentes de um cluster Kubernetes, a primeira a√ß√£o do perito √© tentar montar os snapshots dos volumes (se existirem) em um ambiente de an√°lise isolado para verificar o estado dos arquivos e a extens√£o da criptografia, antes de qualquer tentativa de restaura√ß√£o em produ√ß√£o.                                                                                   | V        | ‚úÖ Correto. An√°lise antes da restaura√ß√£o. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Montar snapshots em um ambiente "limpo" e isolado. Usar ferramentas forenses para examinar os arquivos, identificar o tipo de ransomware, verificar se h√° "chaves" ou notas de resgate. Documentar tudo. Isso informa a estrat√©gia de recupera√ß√£o e ajuda a entender o ataque.                                                                                                                                                                                      |
| 253 | Um atacante explorou uma vulnerabilidade em um `Admission Controller` customizado para bypassar pol√≠ticas de seguran√ßa. O perito deve focar em obter o c√≥digo-fonte e a configura√ß√£o do `Admission Controller`, al√©m de analisar os logs do `kube-apiserver` para identificar requisi√ß√µes que foram indevidamente permitidas por ele.                                                                                    | V        | ‚úÖ Verdadeiro. Entender o mecanismo de falha. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** An√°lise do c√≥digo do `Admission Controller` pode revelar a vulnerabilidade (ex: l√≥gica falha, input n√£o sanitizado). A configura√ß√£o (`ValidatingWebhookConfiguration` ou `MutatingWebhookConfiguration`) mostra como ele √© invocado. Logs do `kube-apiserver` (com `audit-level` adequado) podem mostrar as requisi√ß√µes e as decis√µes do webhook.                                                                                                               |
| 254 | Um `Pod` est√° se comunicando com um C2 server usando DNS tunneling. A an√°lise dos logs do CoreDNS (ou kube-dns) no cluster Kubernetes, procurando por um volume an√¥malo de queries DNS para subdom√≠nios suspeitos ou queries com payloads estranhos (ex: TXT records longos), pode revelar essa atividade.                                                                                                               | V        | ‚úÖ Correto. DNS logs s√£o cruciais para DNS tunneling. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Habilitar e coletar logs do CoreDNS (aumentar o n√≠vel de log se necess√°rio). Procurar por padr√µes anormais: queries muito frequentes do mesmo `Pod`, queries para dom√≠nios inexistentes ou com nomes gerados algoritmicamente, queries TXT/CNAME com dados codificados. Correlacionar com logs de rede do `Pod`/n√≥.                                                                                                                                     |
| 255 | Um perito encontra um `DaemonSet` que executa um `Pod` em cada n√≥ do cluster. Se esse `DaemonSet` for malicioso (ex: para persist√™ncia ou coleta de credenciais do n√≥), o impacto √© limitado ao `Namespace` do `DaemonSet`.                                                                                                                                                                                              | F        | ‚ùå `DaemonSets` podem ter impacto em todo o cluster. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** `Pods` de `DaemonSets` frequentemente rodam com privil√©gios elevados (ex: acesso ao filesystem do n√≥, `hostNetwork`) para realizar suas tarefas. Se malicioso, um `DaemonSet` pode comprometer *todos os n√≥s* do cluster. Analisar as permiss√µes do `ServiceAccount` do `DaemonSet` e o `securityContext` de seus `Pods`.                                                                                                                                |
| 256 | Ao investigar um cluster gerenciado pelo Rancher, o perito nota que o `cattle-cluster-agent` (agente do Rancher no cluster gerenciado) est√° consumindo muitos recursos e fazendo muitas chamadas ao `kube-apiserver`. Isso √© sempre um comportamento normal e n√£o indica um problema de seguran√ßa ou comprometimento do Rancher.                                                                                         | F        | ‚ùå Pode indicar um problema. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Embora o agente do Rancher se comunique com o `api-server`, um consumo excessivo ou atividade an√¥mala pode indicar: um problema de configura√ß√£o no Rancher, uma vers√£o bugada do agente, ou que o pr√≥prio Rancher Server (ou o agente) foi comprometido e est√° sendo usado para atividades maliciosas no cluster downstream. Analisar logs do agente e do Rancher Server.                                                                                                        |
| 257 | Um perito precisa determinar a hora exata em que um `Pod` foi criado. O campo `metadata.creationTimestamp` no YAML do `Pod` (obtido via `kubectl get pod NOME -o yaml`) √© a fonte mais confi√°vel e precisa para isso, representando o tempo UTC.                                                                                                                                                                         | V        | ‚úÖ Verdadeiro. `creationTimestamp` √© fundamental. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Este timestamp √© registrado pelo `kube-apiserver` no momento da cria√ß√£o do objeto. √â crucial para construir timelines de incidentes. Verificar tamb√©m os `status.conditions` (ex: `Initialized`, `ContainersReady`, `Ready`) e seus `lastTransitionTime` para entender o ciclo de vida do `Pod`.                                                                                                                                                            |
| 258 | Um atacante conseguiu executar um `Pod` usando a imagem `nginx` oficial, mas modificou o `command` do cont√™iner para executar um script malicioso em vez do Nginx. O perito deve analisar o `spec.containers[].command` e `spec.containers[].args` no manifesto do `Pod` para identificar o comando real executado.                                                                                                      | V        | ‚úÖ Correto. O `command`/`args` sobrescreve o `ENTRYPOINT`/`CMD` da imagem. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Mesmo que a `image` pare√ßa benigna, o `command` e `args` no manifesto do `Pod` (ou `Deployment`, `StatefulSet`, etc.) definem o que realmente √© executado. Isso √© uma t√©cnica comum para rodar c√≥digo arbitr√°rio usando imagens confi√°veis.                                                                                                                                                                                        |
| 259 | Em um caso de comprometimento de um `Sidecar Injector Webhook` (como o do Istio), um perito nota que `Pods` que deveriam ter o sidecar injetado n√£o o possuem. A an√°lise dos logs do `kube-apiserver` e do pr√≥prio `Sidecar Injector Webhook` √© crucial para determinar se o webhook falhou, foi bypassado, ou desconfigurado.                                                                                           | V        | ‚úÖ Verdadeiro. Falha na inje√ß√£o de sidecar pode ter implica√ß√µes de seguran√ßa/observabilidade. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Verificar logs do `api-server` para erros ao chamar o webhook. Logs do webhook podem indicar por que ele n√£o injetou o sidecar (ex: falha na l√≥gica, erro de configura√ß√£o, `Namespace` n√£o habilitado para inje√ß√£o). Comparar `Pods` com e sem sidecar para identificar o ponto de diverg√™ncia.                                                                                                                 |
| 260 | Um perito est√° analisando um `PersistentVolumeClaim` (PVC) e o `PersistentVolume` (PV) associado. Apenas o `PV` cont√©m informa√ß√µes sobre o tipo de storage real (ex: `awsElasticBlockStore`, `azureDisk`, `nfs`); o `PVC` √© apenas uma requisi√ß√£o abstrata.                                                                                                                                                              | V        | ‚úÖ Correto. O `PV` detalha a implementa√ß√£o do storage. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** O `PV` (`spec`) mostrar√° o plugin de volume usado e seus par√¢metros (ex: ID do disco EBS, servidor NFS). O `PVC` (`spec`) mostra a `StorageClass` solicitada, `accessModes` e tamanho. Para an√°lise forense do *conte√∫do* do volume, ser√° necess√°rio acessar o storage backend referenciado no `PV`.                                                                                                                                                   |
| 261 | Um atacante obteve um `kubeconfig` com acesso a m√∫ltiplos clusters gerenciados por um Rancher Server. O perito deve assumir que todos os clusters listados no `kubeconfig` podem ter sido comprometidos e inclu√≠-los no escopo da investiga√ß√£o.                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. Um `kubeconfig` comprometido √© uma chave mestra. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Analisar os contextos no `kubeconfig`. Para cada cluster, obter logs de auditoria do `kube-apiserver` e procurar por atividades usando as credenciais do `kubeconfig` (usu√°rio, token). O Rancher Server em si tamb√©m deve ser investigado para ver como o `kubeconfig` foi gerado/acessado.                                                                                                                                                  |
| 262 | Um perito identifica um `Pod` com `NET_RAW` capability. Isso √© altamente suspeito, pois permite ao `Pod` criar sockets raw, o que pode ser usado para spoofing de pacotes, sniffing (se combinado com `NET_ADMIN` ou `hostNetwork`), e outras atividades de rede maliciosas.                                                                                                                                             | V        | ‚úÖ Correto. `NET_RAW` √© uma capability perigosa. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Investigar por que o `Pod` precisa de `NET_RAW`. Analisar a imagem e o comportamento do `Pod`. Coletar tr√°fego de rede do n√≥/`Pod`. Poucas aplica√ß√µes leg√≠timas precisam disso (ex: `ping`). Em conjunto com outras capabilities ou configura√ß√µes de rede permissivas, aumenta o risco.                                                                                                                                                                      |
| 263 | Um atacante usou `kubectl cp` para copiar arquivos de um `Pod` para sua m√°quina. Os logs de auditoria do `kube-apiserver` n√£o registrar√£o esta atividade `cp` diretamente, pois `kubectl cp` usa um mecanismo similar ao `tar` sobre uma conex√£o `exec`.                                                                                                                                                                 | V        | ‚úÖ Verdadeiro. `cp` √© um pouco opaco na auditoria. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** O log de auditoria mostrar√° uma sess√£o `exec` sendo iniciada no `Pod` pelo `kubectl cp`. Os comandos `tar` dentro dessa sess√£o `exec` n√£o s√£o logados na auditoria do K8s. Evid√™ncia da c√≥pia viria de: logs do shell/comandos no cont√™iner (se houver), an√°lise do filesystem do `Pod` (timestamps de acesso), ou logs de rede se os dados foram enviados para fora.                                                                                      |
| 264 | Um perito encontra um `EndpointSlice` para um `Service` que aponta para IPs de `Pods` que n√£o existem mais ou para IPs fora do CIDR do cluster. Isso pode indicar uma tentativa de sequestro de tr√°fego ou um `Service` mal configurado/comprometido.                                                                                                                                                                    | V        | ‚úÖ Correto. `EndpointSlices` devem refletir `Pods` v√°lidos. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Investigar como os `EndpointSlices` foram criados/modificados. Se o `Service` tem um `selector`, os `EndpointSlices` s√£o gerenciados pelo `endpointslice-controller`. Se n√£o tem `selector` (headless ou `ExternalName` mal usado), os `Endpoints`/`EndpointSlices` podem ser gerenciados manualmente. Verificar logs de auditoria para cria√ß√£o/modifica√ß√£o de `EndpointSlices` e `Endpoints`.                                                    |
| 265 | Ao analisar um `Pod` que usa um `emptyDir` volume, o perito sabe que os dados nesse volume s√£o persistentes e sobreviver√£o a reinicializa√ß√µes do `Pod` e do n√≥.                                                                                                                                                                                                                                                          | F        | ‚ùå `emptyDir` √© ef√™mero. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Dados em `emptyDir` existem enquanto o `Pod` estiver rodando *no mesmo n√≥*. Se o `Pod` for deletado e recriado (mesmo no mesmo n√≥), o `emptyDir` √© novo/vazio. Se o n√≥ reiniciar, o `emptyDir` √© perdido. √â para dados tempor√°rios.                                                                                                                                                                                                                                                  |
| 266 | Um atacante comprometeu um `CustomResourceDefinition` (CRD) e seu `Operator` associado. O perito deve analisar n√£o apenas as inst√¢ncias do `CustomResource` (CRs), mas tamb√©m o c√≥digo do `Operator` e as permiss√µes RBAC concedidas ao `ServiceAccount` do `Operator`, pois o `Operator` pode ter sido modificado para realizar a√ß√µes maliciosas ao processar os CRs.                                                   | V        | ‚úÖ Verdadeiro. `Operator` comprometido √© um vetor poderoso. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** O `Operator` √© um software que reage a CRs. Se o `Operator` ou suas permiss√µes s√£o comprometidos, ele pode abusar de suas permiss√µes para criar `Pods` privilegiados, `Secrets`, ou exfiltrar dados em resposta a um CR (mesmo que o CR pare√ßa benigno). An√°lise de c√≥digo do `Operator`, seus logs, e suas permiss√µes RBAC √© crucial.                                                                                                            |
| 267 | Um perito investiga um cluster onde o `kube-scheduler` foi desabilitado ou comprometido. `Pods` rec√©m-criados ficar√£o no estado `Pending` indefinidamente, pois n√£o haver√° componente para atribu√≠-los a um n√≥.                                                                                                                                                                                                          | V        | ‚úÖ Correto. `Scheduler` √© essencial para o ciclo de vida do `Pod`. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Verificar o status dos componentes do Control Plane (`kubectl get componentstatuses` - embora depreciado, pode dar uma pista; ou logs dos componentes). Se `Pods` est√£o `Pending` com o evento "FailedScheduling" e mensagens indicando que o scheduler n√£o est√° rodando, investigar o `kube-scheduler`.                                                                                                                                   |
| 268 | Um `Pod` malicioso est√° usando um `Service` do tipo `ExternalName` para se comunicar com um servidor de C2, usando o nome DNS interno do `Service` como um alias para o C2. A an√°lise dos logs DNS do cluster (CoreDNS) procurando queries para o FQDN do `Service` `ExternalName` pode revelar o FQDN real do C2.                                                                                                       | V        | ‚úÖ Verdadeiro. `ExternalName` pode ofuscar destinos. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Logs do CoreDNS mostrar√£o a query para `NOME_SERVICO.NOME_NAMESPACE.svc.cluster.local` e a resposta CNAME para o FQDN externo (o C2). Isso revela o destino real.                                                                                                                                                                                                                                                                                        |
| 269 | Um perito precisa coletar evid√™ncias de um n√≥ Kubernetes que ser√° desligado permanentemente. A prioridade √© desligar o n√≥ imediatamente para evitar altera√ß√£o de dados e depois usar ferramentas forenses de disco para analisar a imagem do disco do n√≥.                                                                                                                                                                | F        | ‚ùå Desligamento imediato perde dados vol√°teis. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se poss√≠vel e seguro, realizar coleta de dados vol√°teis *antes* de desligar: dump de mem√≥ria RAM, listar processos, conex√µes de rede, logs recentes do sistema e `kubelet`/`container runtime`. *Depois* disso, desligar o n√≥ (idealmente de forma controlada) e prosseguir com a imagem do disco para an√°lise "post mortem".                                                                                                                                  |
| 270 | Em um ataque que explorou a API do `containerd` diretamente em um n√≥ (bypassando o `kubelet`), os logs de auditoria do `kube-apiserver` n√£o ter√£o registros dessa atividade, mas os logs do `containerd` no n√≥ e possivelmente `sysmon`/`auditd` (se configurados) podem conter evid√™ncias.                                                                                                                              | V        | ‚úÖ Correto. Ataque direto ao runtime foge da auditoria K8s. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se o atacante interage diretamente com o socket do `containerd` (ex: se ele tem acesso root no n√≥), o `kube-apiserver` n√£o v√™. Investigar logs do `containerd` (`journalctl -u containerd`), logs de auditoria do sistema operacional do n√≥ (`auditd` para syscalls, `sysmon` no Windows se aplic√°vel), e procurar por cont√™ineres "√≥rf√£os" ou n√£o gerenciados pelo K8s.                                                                          |
| 271 | Um perito encontra um `Pod` com a annotation `kubectl.kubernetes.io/last-applied-configuration`. Essa annotation cont√©m o manifesto YAML completo do `Pod` como foi aplicado pela √∫ltima vez via `kubectl apply`, o que pode ser √∫til para entender sua configura√ß√£o original se o `Pod` foi modificado dinamicamente.                                                                                                   | V        | ‚úÖ Verdadeiro. √â um "backup" da √∫ltima configura√ß√£o aplicada. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se houver suspeita de que o `Pod` em execu√ß√£o difere do que foi intencionalmente aplicado (ex: por um `Admission Controller` mutante ou altera√ß√£o direta), esta annotation pode servir como refer√™ncia. Compar√°-la com o `kubectl get pod NOME -o yaml` atual.                                                                                                                                                                                  |
| 272 | Um atacante criou um `NetworkPolicy` muito permissivo (`podSelector: {}`, `ingress: [{}]`) em um `Namespace` para permitir todo o tr√°fego de entrada para todos os `Pods` naquele `Namespace`, bypassando pol√≠ticas mais restritivas que poderiam existir. O perito deve procurar por eventos de cria√ß√£o/atualiza√ß√£o de `NetworkPolicy` nos logs de auditoria.                                                           | V        | ‚úÖ Correto. `NetworkPolicy` pode ser usada para abrir brechas. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** `podSelector: {}` seleciona todos os `Pods` no `Namespace`. `ingress: [{}]` (sem `from` ou `ports`) permite todo tr√°fego de entrada de qualquer origem. Se essa pol√≠tica foi criada por um atacante, ela pode anular outras pol√≠ticas mais espec√≠ficas dependendo da ordem de avalia√ß√£o do CNI. Verificar logs de auditoria por `NetworkPolicy` com seletores e regras suspeitamente amplos.                                                   |
| 273 | Um perito est√° analisando um arquivo `kubeconfig` comprometido. O `kubeconfig` cont√©m apenas o endere√ßo do `kube-apiserver` e o nome do usu√°rio; as credenciais reais (token ou certificado) s√£o sempre buscadas dinamicamente.                                                                                                                                                                                          | F        | ‚ùå `kubeconfig` *pode* embutir tokens ou certs/chaves. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Inspecionar o `kubeconfig`. Ele pode conter um token de `ServiceAccount` ou de usu√°rio (ex: `users[].user.token`), ou caminhos para arquivos de certificado do cliente e chave (`users[].user.client-certificate` e `users[].user.client-key`). Se esses estiverem embutidos ou os arquivos referenciados forem acess√≠veis, o `kubeconfig` √© uma credencial completa.                                                                                  |
| 274 | Um perito suspeita que um `Pod` est√° exfiltrando dados lentamente atrav√©s de m√∫ltiplas pequenas queries DNS para um dom√≠nio controlado pelo atacante. Analisar a frequ√™ncia e o tamanho das queries DNS originadas do `Pod` nos logs do CoreDNS, mesmo que para um dom√≠nio leg√≠timo, pode ser um indicador se o volume for an√¥malo.                                                                                      | V        | ‚úÖ Verdadeiro. Exfiltra√ß√£o via DNS pode ser sutil. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Procurar por um n√∫mero de queries DNS de um `Pod` para um mesmo dom√≠nio (ou subdom√≠nios) que seja estatisticamente incomum em compara√ß√£o com o baseline. Analisar o tamanho das queries/respostas (especialmente TXT records). Ferramentas de an√°lise de tr√°fego de rede (PCAP) podem ser necess√°rias para inspecionar o payload DNS real.                                                                                                                 |
| 275 | Em um cluster com Istio Service Mesh, um atacante modifica uma `VirtualService` ou `DestinationRule` para rotear tr√°fego destinado a um servi√ßo leg√≠timo para um `Pod` malicioso. O perito deve analisar os logs de auditoria do `kube-apiserver` para modifica√ß√µes nesses CRDs do Istio e os logs do Istio control plane (istiod) e data plane (envoy proxies).                                                         | V        | ‚úÖ Correto. Configura√ß√£o do Service Mesh √© um ponto de controle cr√≠tico. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** CRDs do Istio (como `VirtualService`, `DestinationRule`, `Gateway`) definem o roteamento de tr√°fego. Modifica√ß√µes neles podem sequestrar tr√°fego. Logs de auditoria do K8s rastreiam mudan√ßas nos CRDs. Logs do `istiod` podem mostrar reconfigura√ß√µes. Logs dos Envoy sidecars podem mostrar para onde o tr√°fego est√° sendo efetivamente roteado.                                                                                   |
| 276 | Um perito investiga um cluster Kubernetes onde os `Node` status est√£o `NotReady`. A primeira a√ß√£o √© reiniciar todos os `kubelet` nos n√≥s afetados, pois isso geralmente resolve o problema de comunica√ß√£o.                                                                                                                                                                                                               | F        | ‚ùå Reiniciar sem diagn√≥stico pode piorar ou ocultar a causa. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Antes de reiniciar, investigar *por que* os n√≥s est√£o `NotReady`. Verificar logs do `kubelet` no n√≥. Verificar conectividade de rede do n√≥ para o `kube-apiserver`. Verificar recursos do n√≥ (disco, mem√≥ria, PIDs). Verificar o status do `container runtime`. O problema pode ser de rede, recursos, ou configura√ß√£o, n√£o apenas um `kubelet` "preso".                                                                                         |
| 277 | Um atacante obteve acesso a um n√≥ e est√° usando `crictl` (ferramenta CLI para CRI runtimes como `containerd`) para interagir com cont√™ineres, bypassando o `kubelet` e `kubectl`. As a√ß√µes realizadas com `crictl` n√£o ser√£o vis√≠veis nos logs de auditoria do `kube-apiserver`.                                                                                                                                         | V        | ‚úÖ Verdadeiro. `crictl` fala direto com o runtime. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se o atacante tem acesso ao n√≥ e ao socket do CRI, ele pode manipular cont√™ineres diretamente. Logs de auditoria do K8s n√£o ver√£o isso. Evid√™ncias estariam nos logs do *container runtime* (`containerd`, CRI-O), logs de auditoria do sistema operacional do n√≥ (`auditd`), e comportamento an√¥malo dos cont√™ineres/n√≥.                                                                                                                                  |
| 278 | Um perito encontra um `Pod` com um volume montado de um `Secret` que n√£o existe mais (foi deletado). O `Pod` continuar√° rodando com a √∫ltima vers√£o conhecida do `Secret` que foi montada, at√© que o `Pod` seja reiniciado.                                                                                                                                                                                              | V        | ‚úÖ Correto. O `kubelet` monta o `Secret` quando o `Pod` inicia. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se um `Secret` √© deletado, os `Pods` que j√° o montaram como volume continuar√£o a ter acesso aos dados daquele `Secret` *no momento em que foi montado*. Novos `Pods` n√£o conseguir√£o montar o `Secret` deletado e falhar√£o ao iniciar. Isso pode ser relevante se o `Secret` continha credenciais que foram revogadas ao deletar o `Secret`, mas `Pods` antigos ainda as usam.                                                                |
| 279 | Um perito analisa um caso de uso indevido de recursos em um cluster e suspeita que um usu√°rio est√° criando `Pods` com `requests` de CPU muito baixos mas `limits` muito altos (ou ausentes), permitindo que seus `Pods` consumam muitos recursos se dispon√≠veis ("bursting"), potencialmente impactando outros. Verificar a configura√ß√£o de `LimitRange` no `Namespace` e os `requests`/`limits` dos `Pods` √© essencial. | V        | ‚úÖ Verdadeiro. Abuso de bursting pode ser um problema. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial/Administrativa:** `LimitRange` pode impor `requests` e `limits` m√≠nimos/m√°ximos e a raz√£o entre eles. Analisar os `requests` vs. `limits` dos `Pods` do usu√°rio. Monitorar o uso real de CPU (`kubectl top pods`) vs. `requests`. Se `requests` s√£o muito baixos, o `Scheduler` pode colocar muitos `Pods` em um n√≥, e se todos tentarem "burst" ao mesmo tempo, causa conten√ß√£o.                                                                         |
| 280 | Um atacante criou um `ValidatingAdmissionWebhook` malicioso que *sempre* rejeita a cria√ß√£o de `Pods` por administradores, causando uma nega√ß√£o de servi√ßo (DoS) no gerenciamento do cluster. O perito deve verificar as `ValidatingWebhookConfiguration` ativas e os logs do `kube-apiserver` para erros de admiss√£o.                                                                                                    | V        | ‚úÖ Correto. Webhooks podem ser abusados para DoS. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Listar `ValidatingWebhookConfiguration` (`kubectl get validatingwebhookconfigurations`). Inspecionar suas `rules` (para quais opera√ß√µes/recursos se aplicam) e `clientConfig` (onde o webhook est√°). Logs do `api-server` mostrar√£o as requisi√ß√µes sendo rejeitadas pelo webhook e qual webhook as rejeitou.                                                                                                                                                |
| 281 | Ao investigar um `Pod` que foi terminado, o comando `kubectl logs NOME_POD_TERMINADO --previous` sempre mostrar√° os logs da √∫ltima execu√ß√£o do cont√™iner principal antes de terminar.                                                                                                                                                                                                                                    | F        | ‚ùå `--previous` √© para cont√™ineres que *reiniciaram* e foram substitu√≠dos por uma nova inst√¢ncia *no mesmo Pod*. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se o `Pod` inteiro foi terminado e um novo `Pod` (com nome diferente, mesmo que parte do mesmo `Deployment`) foi criado, `--previous` no `Pod` antigo n√£o se aplica ao novo. Para `Pods` terminados, se os logs n√£o foram coletados centralmente, eles podem estar perdidos ou apenas acess√≠veis nos logs do `container runtime`/`kubelet` no n√≥ por um curto per√≠odo.                       |
| 282 | Um perito precisa verificar se um `HorizontalPodAutoscaler` (HPA) est√° tomando decis√µes corretas de escalonamento. `kubectl describe hpa NOME_HPA` mostrar√° as m√©tricas atuais observadas pelo HPA, as m√©tricas alvo, e os eventos de escalonamento.                                                                                                                                                                     | V        | ‚úÖ Verdadeiro. `describe hpa` √© a primeira parada. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial/Diagn√≥stica:** `describe hpa` mostra o `currentReplicas`, `desiredReplicas`, e as `Conditions` (ex: `AbleToScale`, `ScalingActive`). Os `Events` no final mostrar√£o quando o HPA escalou para cima ou para baixo e por qu√™ (ex: "New size: 5; reason: cpu resource utilization (percentage of request) above target").                                                                                                                                        |
| 283 | Um perito forense digital, ao analisar um sistema Kubernetes, deve priorizar a coleta de dados da mem√≥ria vol√°til dos n√≥s (RAM) em detrimento da an√°lise de logs do `kube-apiserver`, pois a mem√≥ria RAM cont√©m o estado mais atual e completo do sistema.                                                                                                                                                               | F        | ‚ùå Ambas s√£o importantes, mas t√™m prop√≥sitos e desafios diferentes. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Logs de auditoria do `kube-apiserver` fornecem um registro hist√≥rico de *a√ß√µes de controle* e s√£o persistentes (se bem configurados). Mem√≥ria RAM √© vol√°til e cont√©m o estado *em execu√ß√£o*, √∫til para malware e processos ef√™meros, mas sua coleta em m√∫ltiplos n√≥s √© complexa e intrusiva. A escolha depende do cen√°rio; geralmente, logs s√£o o primeiro passo para entender o "o qu√™ e quando" das a√ß√µes no cluster.                   |
| 284 | Um atacante com acesso ao `kubelet` API de um n√≥ pode criar um "mirror pod" (Pod espelho) que n√£o √© gerenciado pelo `kube-apiserver`. Este `Pod` n√£o aparecer√° em `kubectl get pods`, mas estar√° rodando no n√≥.                                                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. Mirror pods s√£o uma forma de bypassar o `api-server`. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se o `kubelet` API (porta 10250 por padr√£o, requer autentica√ß√£o/autoriza√ß√£o) for acessado, um atacante pode instruir o `kubelet` a rodar um `Pod` diretamente. Isso √© como `static pods` (definidos por arquivos no n√≥), mas din√¢mico. Evid√™ncia estaria nos logs do `kubelet`, `container runtime`, e na lista de cont√™ineres/processos do n√≥.                                                                                          |
| 285 | Um perito est√° investigando a exfiltra√ß√£o de dados de um `PersistentVolume`. A an√°lise dos `accessModes` do `PV` (ex: `ReadWriteOnce`, `ReadOnlyMany`, `ReadWriteMany`) pode ajudar a entender quantos `Pods`/n√≥s poderiam ter acessado o volume simultaneamente, auxiliando a delimitar o escopo de `Pods` suspeitos.                                                                                                   | V        | ‚úÖ Verdadeiro. `accessModes` definem a sem√¢ntica de montagem. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se um `PV` √© `ReadWriteOnce` (RWO), ele s√≥ pode ser montado por um n√≥ por vez. Se for `ReadWriteMany` (RWX, ex: NFS), m√∫ltiplos `Pods` em m√∫ltiplos n√≥s podem acess√°-lo. Isso ajuda a focar a an√°lise nos `Pods` que *poderiam* ter montado o volume durante o per√≠odo do incidente.                                                                                                                                                            |
| 286 | Um perito encontra um arquivo de `kubeconfig` que usa um `exec plugin` para autentica√ß√£o. Para usar este `kubeconfig`, o perito precisa ter o plugin bin√°rio referenciado no `kubeconfig` instalado e funcionando em sua m√°quina de an√°lise, e entender que este plugin pode executar c√≥digo arbitr√°rio.                                                                                                                 | V        | ‚úÖ Correto. `exec plugins` s√£o poderosos e podem ser um risco. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** `exec plugins` s√£o usados para integrar com sistemas de auth customizados. O `kubeconfig` especifica um comando a ser executado, e a sa√≠da (`stdout`) desse comando √© usada para obter credenciais (ex: token). O perito deve analisar o plugin (se dispon√≠vel) antes de execut√°-lo, pois ele pode ser malicioso ou ter vulnerabilidades.                                                                                                      |
| 287 | Em um incidente envolvendo o Rancher, um perito precisa determinar quais a√ß√µes um usu√°rio espec√≠fico do Rancher realizou atrav√©s da UI do Rancher. A an√°lise dos logs do `rancher-server` (cont√™iner do Rancher) √© o local prim√°rio para encontrar essa informa√ß√£o, pois ele registra as chamadas de API do frontend.                                                                                                    | V        | ‚úÖ Verdadeiro. Logs do Rancher Server s√£o chave para auditoria da UI. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** A UI do Rancher faz chamadas para a API do Rancher Server. Os logs do `rancher-server` (se configurados com verbosidade adequada) registrar√£o essas chamadas, incluindo o usu√°rio autenticado no Rancher, a a√ß√£o realizada, e os recursos afetados (ex: qual cluster, qual projeto).                                                                                                                                                    |
| 288 | Um atacante criou um `Pod` que monta o Docker socket do n√≥ (`/var/run/docker.sock`) como um volume. Isso permite ao `Pod` controlar o Docker daemon no n√≥, podendo listar, criar, e remover quaisquer cont√™ineres no n√≥, efetivamente escapando do isolamento do seu pr√≥prio cont√™iner.                                                                                                                                  | V        | ‚úÖ Verdadeiro. Montar o Docker socket √© equivalente a ter acesso root no Docker do n√≥. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Esta √© uma t√©cnica de escape de cont√™iner cl√°ssica. Investigar por que o `Pod` precisa disso. Analisar os comandos executados pelo `Pod` (logs, `exec` se poss√≠vel). Verificar se o `Pod` criou outros cont√™ineres privilegiados ou exfiltrou dados.                                                                                                                                                                   |
| 289 | Um perito est√° analisando um cluster Kubernetes onde o `etcd` √© externo ao cluster (n√£o gerenciado pelo K8s). Os logs de auditoria do `kube-apiserver` ainda ser√£o a fonte prim√°ria para atividades de controle no cluster, mas a seguran√ßa e os logs do *cluster etcd externo* tamb√©m precisam ser investigados separadamente.                                                                                          | V        | ‚úÖ Correto. `etcd` externo tem sua pr√≥pria superf√≠cie de ataque. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Mesmo com `etcd` externo, o `kube-apiserver` audita as requisi√ß√µes que ele processa. No entanto, se o cluster `etcd` externo for comprometido diretamente (sem passar pelo `api-server`), investigar seus logs de acesso, logs do sistema dos n√≥s do `etcd`, e suas configura√ß√µes de seguran√ßa (TLS, autentica√ß√£o de cliente) √© crucial.                                                                                                     |
| 290 | Um `Pod` malicioso est√° usando `hostAliases` em sua especifica√ß√£o para redirecionar o tr√°fego destinado a um servi√ßo interno leg√≠timo (ex: `servico-interno.prod.svc.cluster.local`) para um IP controlado pelo atacante. O perito deve inspecionar o `spec.hostAliases` no manifesto do `Pod`.                                                                                                                          | V        | ‚úÖ Verdadeiro. `hostAliases` manipula a resolu√ß√£o DNS dentro do `Pod`. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** `hostAliases` adiciona entradas ao `/etc/hosts` do cont√™iner. Se um atacante controla um `Pod`, ele pode usar isso para fazer o `Pod` se comunicar com um endpoint malicioso em vez do real, mesmo que a aplica√ß√£o use o nome DNS correto. Verificar o YAML do `Pod` por `hostAliases` suspeitos.                                                                                                                                      |
| 291 | Um perito investiga um ataque que explorou uma vulnerabilidade de "Remote Code Execution" (RCE) em uma aplica√ß√£o web rodando em um `Pod`. Ap√≥s obter um shell no cont√™iner, o atacante n√£o conseguiu escalar privil√©gios ou acessar outros `Pods` porque o `ServiceAccount` padr√£o (`default`) usado pelo `Pod` n√£o tinha nenhuma permiss√£o RBAC e `NetworkPolicies` bloqueavam o acesso lateral.                        | V        | ‚úÖ Verdadeiro. "Defense in depth" funcionando. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Este √© um exemplo de boas pr√°ticas de seguran√ßa limitando o "blast radius". Mesmo com RCE no `Pod`, se o `ServiceAccount` n√£o tem permiss√µes e `NetworkPolicies` s√£o restritivas, o dano √© contido. Investigar como o RCE ocorreu na aplica√ß√£o, mas reconhecer que as defesas do K8s funcionaram.                                                                                                                                                              |
| 292 | Durante a an√°lise de um dump de mem√≥ria de um cont√™iner, o perito encontra um bin√°rio "packed" (ofuscado/comprimido). A melhor abordagem √© executar o bin√°rio em um sandbox isolado e monitorar seu comportamento (syscalls, conex√µes de rede, arquivos criados) para entender sua real funcionalidade, em vez de tentar desempacot√°-lo manualmente de imediato.                                                         | V        | ‚úÖ Correto. An√°lise din√¢mica em sandbox √© mais segura e eficiente para packers. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Desempacotar manualmente pode ser complexo e demorado. Executar em um ambiente de an√°lise din√¢mica (sandbox como Cuckoo, ou um ambiente K8s/Docker isolado com `strace`, `tcpdump`, `sysdig`) pode revelar rapidamente o comportamento do malware ap√≥s ele se desempacotar na mem√≥ria.                                                                                                                                        |
| 293 | Um atacante obteve acesso ao Rancher Server e criou um "API Key" global com altas permiss√µes. O perito pode identificar a cria√ß√£o dessa API Key nos logs de auditoria do Rancher Server e, em seguida, usar a API do Rancher (ou a UI) para listar e revogar a chave suspeita.                                                                                                                                           | V        | ‚úÖ Verdadeiro. API Keys do Rancher s√£o credenciais poderosas. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Logs do Rancher devem mostrar a cria√ß√£o da API Key (quem, quando). A UI/API do Rancher permite listar as API Keys existentes (embora o token em si n√£o seja vis√≠vel ap√≥s a cria√ß√£o) e revog√°-las. Investigar quais a√ß√µes foram realizadas usando essa API Key nos logs do Rancher e nos clusters downstream.                                                                                                                                    |
| 294 | Um perito encontra um `Pod` cujo `imagePullPolicy` est√° configurada como `Never`. Isso significa que o `kubelet` nunca tentar√° baixar a imagem do registry e usar√° apenas uma imagem que j√° exista localmente no n√≥. Se a imagem local for maliciosa ou desatualizada, o `Pod` a usar√°.                                                                                                                                  | V        | ‚úÖ Correto. `imagePullPolicy: Never` depende da imagem local. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Esta pol√≠tica √© geralmente usada em desenvolvimento ou para imagens pr√©-carregadas. Se um atacante pode colocar uma imagem maliciosa no n√≥ com o nome e tag esperados, e `imagePullPolicy` √© `Never` (ou `IfNotPresent` e a imagem existe localmente), o K8s a usar√°. Verificar a integridade das imagens nos n√≥s.                                                                                                                              |
| 295 | Um atacante est√° usando um `Pod` para escanear a rede interna do cluster Kubernetes em busca de outros `Services` vulner√°veis. Os logs de fluxo de rede (netflow) gerados pelo CNI (se o CNI suportar e estiver configurado para export√°-los) ou por um Service Mesh podem ajudar a identificar essa atividade de scan.                                                                                                  | V        | ‚úÖ Verdadeiro. Logs de fluxo de rede mostram padr√µes de comunica√ß√£o. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Um `Pod` fazendo muitas conex√µes para diferentes IPs/portas internas em um curto per√≠odo √© um forte indicador de scan. CNIs avan√ßados (como Cilium) ou Service Meshes (Istio, Linkerd) podem fornecer visibilidade detalhada do tr√°fego L3/L4/L7 entre `Pods`.                                                                                                                                                                           |
| 296 | Um perito suspeita que um `MutatingAdmissionWebhook` est√° modificando `Pods` na cria√ß√£o para adicionar um sidecar malicioso. Comparar o YAML do `Pod` enviado ao `kube-apiserver` (se logado no n√≠vel de auditoria `Request` ou `RequestResponse`) com o YAML do `Pod` efetivamente criado (obtido via `kubectl get pod ... -o yaml`) pode revelar a muta√ß√£o.                                                            | V        | ‚úÖ Correto. Mutating webhooks alteram objetos. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Se os logs de auditoria do K8s est√£o no n√≠vel `RequestResponse`, eles mostrar√£o o objeto antes e depois da muta√ß√£o. Caso contr√°rio, comparar a inten√ß√£o original (ex: de um arquivo YAML no Git) com o `Pod` resultante no cluster. Analisar a configura√ß√£o do `MutatingWebhookConfiguration` para identificar o webhook respons√°vel.                                                                                                                          |
| 297 | Um perito est√° analisando um `Pod` que foi parte de um ataque e precisa verificar se o `Pod` tinha acesso ao `ServiceAccount` token. Se o `PodSpec` n√£o especifica `automountServiceAccountToken: false`, ent√£o o token do `ServiceAccount` padr√£o (ou o especificado em `serviceAccountName`) √© automaticamente montado em `/var/run/secrets/kubernetes.io/serviceaccount/token` dentro do cont√™iner.                   | V        | ‚úÖ Verdadeiro. Montagem autom√°tica √© o padr√£o. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Verificar o `automountServiceAccountToken` no `PodSpec` e no `ServiceAccount` em si. Se montado, um atacante que compromete o `Pod` pode usar esse token para interagir com a API K8s com as permiss√µes do `ServiceAccount`.                                                                                                                                                                                                                                   |
| 298 | Um atacante comprometeu um n√≥ e instalou um "kernel module rootkit". A an√°lise forense dos cont√™ineres rodando nesse n√≥ ser√° suficiente para detectar o rootkit, pois os cont√™ineres compartilham o kernel do n√≥.                                                                                                                                                                                                        | F        | ‚ùå Cont√™ineres t√™m vis√£o limitada do kernel. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** O rootkit opera no n√≠vel do kernel do *n√≥*. A detec√ß√£o requer an√°lise do *n√≥* em si: dump de mem√≥ria do n√≥, verifica√ß√£o de m√≥dulos de kernel carregados (`lsmod`), ferramentas de detec√ß√£o de rootkit no n√≠vel do host, an√°lise do `dmesg` e logs do sistema do n√≥. Cont√™ineres podem ser afetados, mas n√£o necessariamente revelar√£o o rootkit diretamente.                                                                                                     |
| 299 | Um perito encontra um `CronJob` suspeito que executa um `Pod` a cada hora. Para entender o que o `Pod` fez em execu√ß√µes passadas, o perito pode listar os `Jobs` criados pelo `CronJob` e ent√£o inspecionar os `Pods` associados a esses `Jobs` (e seus logs), assumindo que eles n√£o foram deletados pela pol√≠tica de reten√ß√£o do `CronJob`.                                                                            | V        | ‚úÖ Correto. `CronJob` cria `Jobs`, que criam `Pods`. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** `kubectl get jobs -l cronjob-name=NOME_CRONJOB` (ou similar, dependendo dos labels). Para cada `Job`, `kubectl get pods -l job-name=NOME_JOB` e `kubectl logs POD_DO_JOB`. Verificar `successfulJobsHistoryLimit` e `failedJobsHistoryLimit` no `CronJob` para ver quantos `Jobs` antigos s√£o mantidos.                                                                                                                                                  |
| 300 | Um perito est√° analisando logs de auditoria do `kube-apiserver` e nota muitas requisi√ß√µes com `responseStatus.code` 403 (Forbidden) de um `ServiceAccount` espec√≠fico. Isso indica que o `ServiceAccount` est√° tentando realizar a√ß√µes para as quais n√£o tem permiss√£o e pode ser um sinal de atividade maliciosa ou de uma aplica√ß√£o mal configurada tentando escalar privil√©gios.                                      | V        | ‚úÖ Verdadeiro. Muitos 403s s√£o um alerta. üïµÔ∏è‚Äç‚ôÇÔ∏è **A√ß√£o Pericial:** Investigar quais a√ß√µes (`verb`, `resource`, `namespace`) o `ServiceAccount` est√° tentando realizar. Comparar com suas permiss√µes RBAC (`Roles`/`RoleBindings`). Se as tentativas s√£o para recursos sens√≠veis ou fora de seu escopo normal, pode ser um atacante usando o token do `ServiceAccount` ou um malware no `Pod`.                                                                                                                                          |
