| id  | afirma√ß√£o                                                                                                                                                                                                                                                                                                                                                       | resposta | explica√ß√£o                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|-----|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1   | O Scikit-learn √© uma biblioteca Python focada exclusivamente em algoritmos de aprendizado profundo (deep learning).                                                                                                                                                                                                                                             | F        | ‚ùå Falso. Scikit-learn √© vastamente conhecido por seus algoritmos de Machine Learning cl√°ssicos (regress√£o, classifica√ß√£o, clusteriza√ß√£o, etc.). Embora possua um `MLPClassifier` (Multi-layer Perceptron), seu foco principal n√£o √© deep learning, para o qual TensorFlow e PyTorch s√£o mais especializados. üß† **Dica:** Associe Scikit-learn a ML tradicional e ferramentas de pr√©-processamento.                                                                   |
| 2   | No TensorFlow, a execu√ß√£o √°vida (eager execution) √© o padr√£o desde a vers√£o 2.x, eliminando a necessidade de definir grafos est√°ticos explicitamente para a maioria dos casos de uso.                                                                                                                                                                           | V        | ‚úÖ Verdadeiro. A execu√ß√£o √°vida permite uma experi√™ncia mais "pyth√¥nica" e interativa, onde as opera√ß√µes s√£o executadas imediatamente. Grafos ainda podem ser usados para otimiza√ß√£o com `tf.function`. üí° **Dica:** "√Åvida" = imediata. Lembre-se que TF 1.x era "define-and-run" (grafo est√°tico).                                                                                                                                                                   |
| 3   | PyTorch utiliza exclusivamente grafos computacionais est√°ticos, semelhantes ao TensorFlow 1.x, para otimiza√ß√£o de performance.                                                                                                                                                                                                                                  | F        | ‚ùå Falso. PyTorch √© conhecido por seus grafos computacionais din√¢micos ("define-by-run"), que oferecem grande flexibilidade, especialmente para modelos com estruturas vari√°veis. Embora `torch.jit` permita compilar para um formato de grafo mais est√°tico. üîÑ **Dica:** PyTorch = Din√¢mico. TensorFlow 2.x = √Åvido (din√¢mico) por padr√£o, com op√ß√£o de grafo est√°tico (`tf.function`).                                                                              |
| 4   | O NLTK (Natural Language Toolkit) requer o download manual de corpora e modelos espec√≠ficos, como o `punkt` para tokeniza√ß√£o, atrav√©s do comando `nltk.download()`.                                                                                                                                                                                             | V        | ‚úÖ Verdadeiro. Diferentemente de algumas bibliotecas que j√° v√™m com tudo, NLTK possui um sistema modular onde voc√™ baixa apenas os recursos necess√°rios. üì¶ **Dica:** Se uma quest√£o NLTK mencionar um recurso espec√≠fico, lembre que provavelmente precisa ser baixado via `nltk.download('nome_do_recurso')`.                                                                                                                                                        |
| 5   | Matplotlib √© uma biblioteca de visualiza√ß√£o de dados que gera apenas gr√°ficos est√°ticos, n√£o sendo capaz de produzir visualiza√ß√µes interativas.                                                                                                                                                                                                                 | F        | ‚ùå Falso. Embora seja famosa por gr√°ficos est√°ticos de alta qualidade, Matplotlib pode gerar gr√°ficos interativos usando diferentes backends (e.g., `%matplotlib notebook` no Jupyter, ou com integra√ß√µes Qt, Tkinter).  interactivity üìà **Dica:** Cuidado com absolutos como "apenas" ou "nunca". Matplotlib √© mais vers√°til do que parece.                                                                                                                          |
| 6   | Jupyter Notebooks permitem a execu√ß√£o de c√©lulas de c√≥digo fora de ordem, o que invariavelmente garante a reprodutibilidade da an√°lise.                                                                                                                                                                                                                         | F        | ‚ùå Falso. A execu√ß√£o fora de ordem √© uma _causa_ comum de problemas de reprodutibilidade, pois o estado do kernel pode se tornar inconsistente. Para reprodutibilidade, o notebook deve rodar linearmente de cima a baixo. ‚û°Ô∏è **Dica:** "Garante" √© uma palavra forte. A flexibilidade do Jupyter √© √≥tima, mas exige disciplina para reprodutibilidade.                                                                                                                |
| 7   | O m√©todo `fit_transform()` do Scikit-learn √© aplic√°vel a todos os estimadores, incluindo classificadores e regressores, para treinar o modelo e fazer predi√ß√µes simultaneamente.                                                                                                                                                                                | F        | ‚ùå Falso. O `fit_transform()` √© t√≠pico de transformadores (e.g., `StandardScaler`, `PCA`), que aprendem par√¢metros dos dados (`fit`) e os aplicam para transformar os dados (`transform`). Estimadores como classificadores usam `fit()` para treinar e `predict()` para predizer. üõ†Ô∏è **Dica:** `fit_transform` = pr√©-processamento. `fit` + `predict` = modelagem.                                                                                                   |
| 8   | `tf.Variable` no TensorFlow √© usado para representar tensores cujos valores podem ser alterados durante a execu√ß√£o do grafo, como os pesos de uma rede neural.                                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro. `tf.Variable` √© fundamental para armazenar e atualizar os par√¢metros trein√°veis de um modelo durante o processo de otimiza√ß√£o. üèãÔ∏è‚Äç‚ôÇÔ∏è **Dica:** Vari√°veis = Pesos do modelo. Elas "variam" durante o treinamento.                                                                                                                                                                                                                                       |
| 9   | Em PyTorch, o m√≥dulo `torch.nn` √© respons√°vel exclusivamente pela defini√ß√£o de fun√ß√µes de perda (loss functions).                                                                                                                                                                                                                                               | F        | ‚ùå Falso. `torch.nn` √© muito mais abrangente. Ele cont√©m blocos de constru√ß√£o para redes neurais, como camadas (lineares, convolucionais), fun√ß√µes de ativa√ß√£o, cont√™ineres de modelos (`nn.Module`), e tamb√©m fun√ß√µes de perda. üß± **Dica:** `nn` = Neural Networks. Pense em todos os tijolos para construir uma rede.                                                                                                                                               |
| 10  | A t√©cnica de Stemming no NLTK sempre reduz as palavras √† sua forma dicionarizada correta (lema).                                                                                                                                                                                                                                                                | F        | ‚ùå Falso. Stemming √© um processo mais r√∫stico que remove sufixos, podendo resultar em radicais que n√£o s√£o palavras v√°lidas (e.g., "studi" de "studies"). Lematiza√ß√£o visa encontrar o lema correto. ‚úÇÔ∏è **Dica:** Stemming = "cortar" sufixos. Lematiza√ß√£o = "dicion√°rio" (mais inteligente).                                                                                                                                                                          |
| 11  | A interface `pyplot` do Matplotlib √© considerada a √∫nica forma de criar gr√°ficos, n√£o existindo uma abordagem orientada a objetos.                                                                                                                                                                                                                              | F        | ‚ùå Falso. Matplotlib possui uma poderosa interface orientada a objetos, onde se manipula explicitamente objetos `Figure` e `Axes`. `pyplot` √© uma interface de scripting (estado) mais conveniente para plots r√°pidos. üñºÔ∏è **Dica:** `pyplot` = r√°pido e f√°cil. OO = controle total e complexidade.                                                                                                                                                                    |
| 12  | Magic commands no Jupyter Notebook, como `%timeit`, s√£o parte da sintaxe padr√£o do Python e funcionam em qualquer interpretador Python.                                                                                                                                                                                                                         | F        | ‚ùå Falso. Magic commands s√£o espec√≠ficos do kernel IPython (usado pelo Jupyter). Eles n√£o s√£o reconhecidos por um interpretador Python padr√£o fora desse ambiente. ‚ú® **Dica:** O `%` ou `%%` √© o sinal de que √© algo "m√°gico", espec√≠fico do IPython/Jupyter.                                                                                                                                                                                                          |
| 13  | A valida√ß√£o cruzada (Cross-Validation) no Scikit-learn tem como principal objetivo acelerar o tempo de treinamento dos modelos.                                                                                                                                                                                                                                 | F        | ‚ùå Falso. O principal objetivo da valida√ß√£o cruzada √© obter uma estimativa mais robusta da performance do modelo em dados n√£o vistos e ajudar na sele√ß√£o de hiperpar√¢metros, reduzindo o risco de overfitting ao conjunto de treino original. ‚è±Ô∏è **Dica:** Valida√ß√£o Cruzada = avalia√ß√£o mais confi√°vel, n√£o velocidade.                                                                                                                                               |
| 14  | O TensorFlow Hub (`tfhub.dev`) √© uma plataforma para compartilhar exclusivamente c√≥digo-fonte de modelos, sem incluir pesos pr√©-treinados.                                                                                                                                                                                                                      | F        | ‚ùå Falso. TensorFlow Hub √© um reposit√≥rio de m√≥dulos de machine learning reutiliz√°veis, que incluem tanto a arquitetura do modelo quanto os pesos pr√©-treinados, facilitando o transfer learning.  pretrained weights üöÄ **Dica:** Hub = Modelos prontos para usar ou adaptar.                                                                                                                                                                                         |
| 15  | O tensor `grad_fn` em um tensor PyTorch indica a fun√ß√£o que ser√° usada para calcular o gradiente desse tensor na pr√≥xima itera√ß√£o de treinamento.                                                                                                                                                                                                               | F        | ‚ùå Falso. O atributo `grad_fn` de um tensor que resulta de uma opera√ß√£o rastreada pelo Autograd aponta para a fun√ß√£o que *criou* esse tensor (a opera√ß√£o no grafo reverso). Ele √© usado para a retropropaga√ß√£o. Os gradientes s√£o armazenados em `.grad`. üîô **Dica:** `grad_fn` = fun√ß√£o que gerou o tensor, parte do hist√≥rico para o backward pass.                                                                                                                 |
| 16  | NLTK fornece ferramentas para Part-of-Speech (POS) tagging, que consiste em classificar palavras em categorias como substantivo, verbo, adjetivo, etc.                                                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. NLTK oferece taggers como `nltk.pos_tag` que utilizam modelos pr√©-treinados para realizar a etiquetagem gramatical das palavras em um texto. üè∑Ô∏è **Dica:** POS tagging = etiquetas gramaticais.                                                                                                                                                                                                                                                         |
| 17  | Ao usar `plt.subplots()` no Matplotlib, √© obrigat√≥rio especificar o n√∫mero de linhas e colunas, mesmo para criar um √∫nico subplot.                                                                                                                                                                                                                              | F        | ‚ùå Falso. Chamar `plt.subplots()` sem argumentos (ou `plt.subplots(1,1)`) cria uma figura com um √∫nico subplot (eixo) por padr√£o. üéØ **Dica:** `plt.subplots()` √© flex√≠vel. Sem args = 1 figura, 1 eixo.                                                                                                                                                                                                                                                               |
| 18  | O JupyterLab √© uma vers√£o mais antiga e simplificada do Jupyter Notebook, com menos funcionalidades.                                                                                                                                                                                                                                                            | F        | ‚ùå Falso. JupyterLab √© considerado a pr√≥xima gera√ß√£o da interface Jupyter, oferecendo um ambiente de desenvolvimento mais integrado e flex√≠vel, com suporte a m√∫ltiplos notebooks, editores de texto, terminais, etc., em uma interface de abas. üåü **Dica:** Lab = Laborat√≥rio (mais completo). Notebook = Caderno (mais simples, mas ainda poderoso).                                                                                                                |
| 19  | O `GridSearchCV` do Scikit-learn testa apenas um subconjunto aleat√≥rio de combina√ß√µes de hiperpar√¢metros para encontrar o melhor modelo.                                                                                                                                                                                                                        | F        | ‚ùå Falso. `GridSearchCV` realiza uma busca exaustiva por todas as combina√ß√µes de hiperpar√¢metros fornecidas na grade. `RandomizedSearchCV` √© que testa um subconjunto aleat√≥rio.  exhaustive search üåê **Dica:** Grid = Grade (todos os pontos). Randomized = Aleat√≥rio.                                                                                                                                                                                               |
| 20  | Tensores no TensorFlow s√£o imut√°veis, o que significa que seu valor n√£o pode ser alterado ap√≥s a cria√ß√£o, exceto para `tf.Variable`.                                                                                                                                                                                                                            | V        | ‚úÖ Verdadeiro. `tf.Tensor` objetos s√£o imut√°veis. Se voc√™ precisa de um tensor mut√°vel (e.g., para acumular valores ou para par√¢metros trein√°veis), voc√™ deve usar `tf.Variable`. üîí **Dica:** Tensor = constante (como uma tupla Python). Variable = mut√°vel (como uma lista Python, mas para o grafo).                                                                                                                                                               |
| 21  | Em PyTorch, para mover um tensor para a GPU, utiliza-se o m√©todo `tensor.gpu()`.                                                                                                                                                                                                                                                                                | F        | ‚ùå Falso. O m√©todo correto √© `tensor.to('cuda')` ou `tensor.cuda()`. √â importante verificar a disponibilidade da GPU com `torch.cuda.is_available()`. üíª‚û°Ô∏èüîå **Dica:** Pense em "to device" ou a abrevia√ß√£o `cuda`.                                                                                                                                                                                                                                                    |
| 22  | WordNet, acess√≠vel via NLTK, √© um grande banco de dados l√©xico do ingl√™s, agrupando palavras em conjuntos de sin√¥nimos chamados synsets.                                                                                                                                                                                                                        | V        | ‚úÖ Verdadeiro. WordNet √© um recurso crucial para tarefas de PLN que envolvem sem√¢ntica, como desambigua√ß√£o de sentido de palavra e encontrar sin√¥nimos/ant√¥nimos. üìö **Dica:** WordNet = Rede de palavras (e seus significados/rela√ß√µes).                                                                                                                                                                                                                              |
| 23  | `fig.add_subplot()` e `plt.subplot()` no Matplotlib s√£o funcionalmente id√™nticos e podem ser usados intercambiavelmente em qualquer contexto.                                                                                                                                                                                                                   | F        | ‚ùå Falso. `fig.add_subplot()` √© um m√©todo de um objeto `Figure` (interface OO), enquanto `plt.subplot()` opera na figura ativa implicitamente (interface `pyplot`). Embora possam alcan√ßar resultados semelhantes, s√£o de paradigmas diferentes. üñºÔ∏èüÜöüé® **Dica:** `fig.` = Orientado a Objeto. `plt.` = Pyplot (scripting).                                                                                                                                           |
| 24  | √â poss√≠vel converter um Jupyter Notebook (`.ipynb`) para um script Python (`.py`) usando a funcionalidade nativa do Jupyter ou ferramentas como `nbconvert`.                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro. `jupyter nbconvert --to script meu_notebook.ipynb` √© um comando comum para isso, removendo as sa√≠das e formata√ß√£o Markdown, mantendo apenas o c√≥digo. üìú **Dica:** `nbconvert` √© o canivete su√≠√ßo para transformar notebooks.                                                                                                                                                                                                                           |
| 25  | O par√¢metro `random_state` em muitos estimadores Scikit-learn serve para controlar a aleatoriedade, garantindo reprodutibilidade nos resultados.                                                                                                                                                                                                                | V        | ‚úÖ Verdadeiro. Definir um `random_state` com um inteiro fixo garante que opera√ß√µes estoc√°sticas (como divis√µes de dados, inicializa√ß√µes, etc.) produzam o mesmo resultado a cada execu√ß√£o. üé≤ **Dica:** `random_state` = Semente para aleatoriedade. Essencial para debug e artigos cient√≠ficos.                                                                                                                                                                       |
| 26  | TensorFlow Lite (TFLite) √© projetado para treinar modelos de deep learning em dispositivos com recursos limitados, como smartphones.                                                                                                                                                                                                                            | F        | ‚ùå Falso. TFLite √© primariamente para *implantar* (infer√™ncia) modelos j√° treinados em dispositivos de borda. O treinamento geralmente ocorre em m√°quinas mais potentes e o modelo √© ent√£o convertido para o formato TFLite. üì± **Dica:** TFLite = Leve (Lite) para rodar, n√£o para treinar.                                                                                                                                                                           |
| 27  | O m√©todo `backward()` em um tensor PyTorch √© chamado para iniciar o processo de retropropaga√ß√£o e calcular os gradientes a partir daquele tensor (geralmente a perda).                                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. Chamar `loss.backward()` calcula os gradientes de `loss` em rela√ß√£o a todos os tensores no grafo que t√™m `requires_grad=True` e contribu√≠ram para `loss`. üìâ **Dica:** `backward()` = "andar para tr√°s" no grafo para calcular gradientes.                                                                                                                                                                                                              |
| 28  | A tokeniza√ß√£o em NLTK sempre divide o texto em palavras individuais, nunca em senten√ßas ou outras unidades.                                                                                                                                                                                                                                                     | F        | ‚ùå Falso. NLTK possui diferentes tokenizadores, como `sent_tokenize` para dividir em senten√ßas e `word_tokenize` para palavras. Existem tamb√©m tokenizadores mais complexos (e.g., baseados em regex). üó£Ô∏è **Dica:** Token = "peda√ßo". Pode ser palavra, senten√ßa, etc.                                                                                                                                                                                                |
| 29  | Para adicionar uma legenda a um gr√°fico Matplotlib, basta chamar `plt.legend()` ap√≥s plotar os dados, sem necessidade de especificar r√≥tulos nos plots.                                                                                                                                                                                                         | F        | ‚ùå Falso. Para que `plt.legend()` funcione corretamente, voc√™ precisa fornecer r√≥tulos (labels) aos seus plots, e.g., `plt.plot(x, y, label='Meus Dados')`. Sem labels, a legenda pode ficar vazia ou com nomes gen√©ricos. üè∑Ô∏è **Dica:** Legenda precisa de `label='...'` no plot.                                                                                                                                                                                     |
| 30  | Kernels em Jupyter Notebooks s√£o respons√°veis apenas por executar c√≥digo Python, n√£o suportando outras linguagens como R ou Julia.                                                                                                                                                                                                                              | F        | ‚ùå Falso. Jupyter √© agn√≥stico √† linguagem. Seu nome (Julia, Python, R) reflete seu suporte inicial, mas existem kernels para dezenas de linguagens. üåê **Dica:** A for√ßa do Jupyter est√° em suportar m√∫ltiplos kernels.                                                                                                                                                                                                                                                |
| 31  | `StandardScaler` do Scikit-learn transforma os dados para que tenham m√©dia zero e vari√¢ncia unit√°ria, sendo robusto a outliers.                                                                                                                                                                                                                                 | F        | ‚ùå Falso. `StandardScaler` calcula m√©dia e desvio padr√£o, o que o torna sens√≠vel a outliers (eles influenciam essas estat√≠sticas). `RobustScaler` usa mediana e intervalo interquartil, sendo mais robusto a outliers. üõ°Ô∏è **Dica:** Standard = M√©dia/DP (sens√≠vel). Robust = Mediana/IQR (robusto).                                                                                                                                                                   |
| 32  | Keras, uma API de alto n√≠vel para constru√ß√£o de redes neurais, √© uma biblioteca completamente independente e n√£o possui integra√ß√£o com TensorFlow.                                                                                                                                                                                                              | F        | ‚ùå Falso. Keras foi profundamente integrado ao TensorFlow (como `tf.keras`) e √© a API de alto n√≠vel recomendada para a maioria dos usu√°rios de TensorFlow. Originalmente era uma biblioteca separada que podia rodar sobre m√∫ltiplos backends. ü§ù **Dica:** Hoje, Keras e TensorFlow s√£o "melhores amigos".                                                                                                                                                            |
| 33  | Em PyTorch, tensores criados a partir de arrays NumPy sempre compartilham a mesma mem√≥ria, de modo que altera√ß√µes em um refletem no outro.                                                                                                                                                                                                                      | F        | ‚ùå Falso. Por padr√£o, `torch.from_numpy()` cria um tensor que compartilha mem√≥ria com o array NumPy (se o array for do tipo certo e estiver na CPU). Contudo, se uma c√≥pia for feita (e.g. `torch.tensor(numpy_array)`) ou se o tipo de dado precisar ser alterado, eles n√£o compartilhar√£o mem√≥ria. √â crucial verificar `is_shared()`. üîó **Dica:** Cuidado com c√≥pias vs. views. `from_numpy` tenta compartilhar.                                                    |
| 34  | A frequ√™ncia de termos (TF - Term Frequency) por si s√≥ √© sempre a melhor representa√ß√£o para a import√¢ncia de uma palavra em um documento em PLN.                                                                                                                                                                                                                | F        | ‚ùå Falso. Palavras muito comuns (stop words) podem ter TF alta mas pouca import√¢ncia sem√¢ntica. TF-IDF (Term Frequency-Inverse Document Frequency) √© uma m√©trica mais robusta que pondera a frequ√™ncia do termo pela sua raridade na cole√ß√£o de documentos. ‚öñÔ∏è **Dica:** TF-IDF > TF para import√¢ncia.                                                                                                                                                                 |
| 35  | Matplotlib permite a cria√ß√£o de gr√°ficos 3D utilizando o toolkit `mplot3d`, que precisa ser importado explicitamente.                                                                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro. Para plotagem 3D, √© comum importar `Axes3D` de `mpl_toolkits.mplot3d`, e.g., `from mpl_toolkits.mplot3d import Axes3D`. üåå **Dica:** 3D n√£o √© padr√£o, precisa de um "empurr√£ozinho" com `mplot3d`.                                                                                                                                                                                                                                                      |
| 36  | C√©lulas Markdown no Jupyter Notebook suportam apenas texto simples, sem formata√ß√£o como t√≠tulos, listas ou inclus√£o de imagens.                                                                                                                                                                                                                                 | F        | ‚ùå Falso. C√©lulas Markdown s√£o poderosas e suportam uma rica sintaxe para formata√ß√£o de texto, incluindo t√≠tulos, listas, negrito, it√°lico, links, imagens, e at√© equa√ß√µes LaTeX. üìù **Dica:** Markdown √© o que torna os notebooks narrativas ricas.                                                                                                                                                                                                                   |
| 37  | O `OneHotEncoder` do Scikit-learn √© utilizado para converter vari√°veis num√©ricas cont√≠nuas em representa√ß√µes categ√≥ricas.                                                                                                                                                                                                                                       | F        | ‚ùå Falso. `OneHotEncoder` √© para converter vari√°veis *categ√≥ricas* (nominais ou ordinais j√° transformadas em inteiros) em uma representa√ß√£o bin√°ria "one-hot", onde cada categoria vira uma nova coluna com 0s e um 1. üî¢‚û°Ô∏èüìä **Dica:** OneHot = Categ√≥rico para Bin√°rio. Para discretizar cont√≠nuas, use `KBinsDiscretizer`.                                                                                                                                          |
| 38  | TensorBoard √© uma ferramenta de visualiza√ß√£o exclusiva para PyTorch, utilizada para monitorar m√©tricas de treinamento e arquiteturas de modelos.                                                                                                                                                                                                                | F        | ‚ùå Falso. TensorBoard foi originalmente desenvolvido para TensorFlow e √© sua principal ferramenta de visualiza√ß√£o. PyTorch tamb√©m pode usar TensorBoard atrav√©s de `torch.utils.tensorboard`. üìä **Dica:** TensorBoard nasceu no TF, mas PyTorch "pegou emprestado".                                                                                                                                                                                                   |
| 39  | Em PyTorch, `model.eval()` e `model.train()` s√£o usados para alternar o comportamento de camadas como Dropout e BatchNorm durante a infer√™ncia e treinamento, respectivamente.                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro. `model.train()` ativa o comportamento de treinamento (e.g., dropout aplica-se, batchnorm atualiza estat√≠sticas). `model.eval()` desativa o dropout e usa estat√≠sticas fixas de batchnorm, crucial para infer√™ncia consistente. üé≠ **Dica:** `eval()` = modo avalia√ß√£o/teste. `train()` = modo treinamento.                                                                                                                                              |
| 40  | A lematiza√ß√£o no NLTK, usando o `WordNetLemmatizer`, n√£o requer a especifica√ß√£o da classe gramatical (POS tag) da palavra para obter o lema correto.                                                                                                                                                                                                            | F        | ‚ùå Falso. Para obter o lema mais preciso (especialmente para palavras amb√≠guas como "playing" - verbo ou substantivo), √© altamente recomend√°vel fornecer a POS tag ao lematizador. Sem ela, ele pode assumir uma POS padr√£o (geralmente substantivo). üßê **Dica:** Lematizador + POS tag = Melhor resultado.                                                                                                                                                           |
| 41  | Para salvar uma figura Matplotlib em um arquivo, o √∫nico m√©todo dispon√≠vel √© clicar com o bot√£o direito na imagem e usar "Salvar imagem como...".                                                                                                                                                                                                               | F        | ‚ùå Falso. Programaticamente, usa-se `plt.savefig('nome_arquivo.png')` (ou `fig.savefig()`). Isso oferece controle sobre formato, resolu√ß√£o (DPI), etc. üíæ **Dica:** `savefig` √© o comando para salvar via c√≥digo.                                                                                                                                                                                                                                                      |
| 42  | Jupyter Notebooks armazenam tanto o c√≥digo quanto suas sa√≠das (gr√°ficos, texto) no mesmo arquivo `.ipynb`, que √© um formato JSON.                                                                                                                                                                                                                               | V        | ‚úÖ Verdadeiro. Arquivos `.ipynb` s√£o documentos JSON que cont√™m o c√≥digo das c√©lulas, as sa√≠das, texto Markdown e metadados do notebook. üìÑ **Dica:** `.ipynb` = JSON. √â por isso que versionar notebooks no Git pode ser chato com as sa√≠das mudando.                                                                                                                                                                                                                 |
| 43  | `PCA (Principal Component Analysis)` no Scikit-learn √© um algoritmo de clusteriza√ß√£o usado para agrupar dados similares.                                                                                                                                                                                                                                        | F        | ‚ùå Falso. PCA √© uma t√©cnica de redu√ß√£o de dimensionalidade. Ele encontra os componentes principais (dire√ß√µes de maior vari√¢ncia) para projetar os dados em um espa√ßo de menor dimens√£o. Algoritmos de clusteriza√ß√£o incluem K-Means, DBSCAN. üìâ **Dica:** PCA = Redu√ß√£o de Dimens√£o, n√£o agrupamento.                                                                                                                                                                  |
| 44  | `tf.data.Dataset` no TensorFlow √© uma API para construir pipelines de entrada de dados eficientes e complexos, permitindo opera√ß√µes como `map`, `batch`, `shuffle`.                                                                                                                                                                                             | V        | ‚úÖ Verdadeiro. A API `tf.data` √© crucial para carregar e pr√©-processar dados de forma perform√°tica, especialmente para grandes datasets e treinamento distribu√≠do. üçΩÔ∏è **Dica:** `tf.data` = "Gar√ßom" dos dados para seu modelo TensorFlow.                                                                                                                                                                                                                            |
| 45  | Todos os tensores em PyTorch requerem explicitamente que `requires_grad=True` seja definido para que seus gradientes sejam calculados durante a retropropaga√ß√£o.                                                                                                                                                                                                | F        | ‚ùå Falso. Por padr√£o, tensores criados pelo usu√°rio t√™m `requires_grad=False`. No entanto, os par√¢metros de um `nn.Module` (as camadas da rede) t√™m `requires_grad=True` por padr√£o quando o m√≥dulo √© instanciado. üî• **Dica:** Pesos do modelo (`nn.Parameter`) j√° v√™m "prontos" para gradientes. Inputs precisam ser configurados se voc√™ quiser gradientes em rela√ß√£o a eles.                                                                                       |
| 46  | NLTK √© primariamente uma biblioteca para tarefas de Vis√£o Computacional, com funcionalidades limitadas para Processamento de Linguagem Natural.                                                                                                                                                                                                                 | F        | ‚ùå Falso. NLTK √© uma das bibliotecas mais conhecidas e abrangentes para Processamento de Linguagem Natural (PLN). Seu foco √© em tarefas de texto. üëÅÔ∏è‚û°Ô∏èüó£Ô∏è **Dica:** NLTK = Natural Language Toolkit. O nome j√° diz tudo.                                                                                                                                                                                                                                              |
| 47  | A fun√ß√£o `plt.show()` no Matplotlib √© sempre necess√°ria ao final de um script para exibir os gr√°ficos, mesmo em ambientes como Jupyter Notebook com `%matplotlib inline`.                                                                                                                                                                                       | F        | ‚ùå Falso. Com `%matplotlib inline` (ou `%matplotlib notebook`) no Jupyter, os gr√°ficos s√£o renderizados automaticamente ap√≥s a c√©lula de plotagem ser executada. `plt.show()` pode ser omitido. Em scripts Python comuns (.py), ele √© geralmente necess√°rio. üñºÔ∏è **Dica:** Em Jupyter com magic, `show()` √© muitas vezes impl√≠cito.                                                                                                                                    |
| 48  | Exportar um Jupyter Notebook para PDF requer sempre a instala√ß√£o de LaTeX e Pandoc no sistema.                                                                                                                                                                                                                                                                  | F        | ‚ùå Falso. Embora a convers√£o via LaTeX (usando Pandoc) produza PDFs de alta qualidade, Jupyter tamb√©m oferece uma op√ß√£o de "Print to PDF" (via navegador) que n√£o tem essas depend√™ncias, mas pode ter limita√ß√µes na formata√ß√£o. üìÑ **Dica:** LaTeX = Melhor qualidade, mais depend√™ncias. Navegador = Mais simples, pode perder formata√ß√£o.                                                                                                                           |
| 49  | Em Scikit-learn, o m√©todo `predict_proba()` de um classificador retorna as probabilidades de pertencimento a cada classe, enquanto `predict()` retorna a classe predita diretamente.                                                                                                                                                                            | V        | ‚úÖ Verdadeiro. `predict_proba()` √© √∫til quando se precisa da confian√ßa da predi√ß√£o (e.g., para definir um limiar customizado), retornando um array com probabilidades por classe. `predict()` retorna a classe com maior probabilidade. üé≤üéØ **Dica:** `proba` = probabilidade. `predict` = classe final.                                                                                                                                                              |
| 50  | O conceito de "grafo est√°tico" no TensorFlow implica que o grafo computacional n√£o pode ser modificado ap√≥s sua cria√ß√£o, oferecendo vantagens de otimiza√ß√£o.                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro. Em um paradigma de grafo est√°tico (predominante no TF 1.x ou com `tf.function`), o grafo √© constru√≠do uma vez e depois executado repetidamente. Isso permite otimiza√ß√µes globais pelo compilador. üèõÔ∏è **Dica:** Est√°tico = Fixo, imut√°vel, bom para otimizar.                                                                                                                                                                                           |
| 51  | PyTorch n√£o possui um mecanismo nativo para treinamento distribu√≠do, sendo necess√°rio recorrer a bibliotecas de terceiros para tal funcionalidade.                                                                                                                                                                                                              | F        | ‚ùå Falso. PyTorch tem suporte robusto para treinamento distribu√≠do atrav√©s dos m√≥dulos `torch.nn.parallel.DistributedDataParallel` (DDP) e `torch.distributed`. üåêü§ù **Dica:** PyTorch escala bem com `DistributedDataParallel`.                                                                                                                                                                                                                                       |
| 52  | A remo√ß√£o de stop words (palavras comuns como "o", "a", "de") √© uma etapa de pr√©-processamento em PLN que sempre melhora o desempenho de todos os modelos.                                                                                                                                                                                                      | F        | ‚ùå Falso. Embora frequentemente √∫til, a remo√ß√£o de stop words pode ser prejudicial em algumas tarefas, como an√°lise de sentimento onde "n√£o" √© crucial, ou em modelos que capturam nuances contextuais (e.g., Transformers). A decis√£o depende da tarefa. üóëÔ∏èü§î **Dica:** Nem sempre "limpar" √© melhor. Teste e avalie o impacto.                                                                                                                                      |
| 53  | No Matplotlib, `Axes` e `Axis` referem-se ao mesmo objeto, sendo termos intercambi√°veis para a √°rea de plotagem.                                                                                                                                                                                                                                                | F        | ‚ùå Falso. Um objeto `Axes` (plural) √© a regi√£o de plotagem onde os dados s√£o exibidos (o "gr√°fico" em si, com seus eixos x e y). Um objeto `Axis` (singular) refere-se a um eixo individual (e.g., o eixo X ou o eixo Y), com seus ticks, r√≥tulos, etc. üìêüîç **Dica:** `Axes` (com 'e') = √°rea do gr√°fico. `Axis` (com 'i') = linha do eixo.                                                                                                                           |
| 54  | `ipywidgets` √© uma biblioteca que permite adicionar controles interativos (sliders, bot√µes) a Jupyter Notebooks, tornando-os mais din√¢micos.                                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro. `ipywidgets` enriquece a experi√™ncia do Jupyter ao permitir a cria√ß√£o de interfaces de usu√°rio simples diretamente no notebook para manipular par√¢metros e visualiza√ß√µes. üéõÔ∏è **Dica:** Widgets = Controles interativos no seu notebook.                                                                                                                                                                                                                |
| 55  | O `LabelEncoder` do Scikit-learn √© ideal para transformar features nominais com muitas categorias √∫nicas para uso em modelos baseados em √°rvores.                                                                                                                                                                                                               | F        | ‚ùå Falso. `LabelEncoder` converte categorias em inteiros (0, 1, 2...). Se usado em features com muitas categorias e aplicado a modelos lineares ou baseados em dist√¢ncia, pode introduzir uma ordem artificial. Para √°rvores, pode ser aceit√°vel, mas `OneHotEncoder` ou `TargetEncoder` s√£o frequentemente melhores ou mais seguros, dependendo do caso. üè∑Ô∏è‚ö†Ô∏è **Dica:** Cuidado com `LabelEncoder` em features nominais; ele implica uma ordem que pode n√£o existir. |
| 56  | O TensorFlow Serving √© uma ferramenta para realizar o versionamento de c√≥digo de modelos TensorFlow, similar ao Git.                                                                                                                                                                                                                                            | F        | ‚ùå Falso. TensorFlow Serving √© um sistema flex√≠vel e de alta performance para *servir* (implantar em produ√ß√£o) modelos de machine learning. Para versionamento de c√≥digo, usa-se Git. Para versionamento de modelos/dados, ferramentas como DVC ou MLflow. üöÄ‚òÅÔ∏è **Dica:** Serving = Colocar o modelo "no ar" para fazer predi√ß√µes.                                                                                                                                     |
| 57  | O otimizador `torch.optim.SGD` em PyTorch implementa apenas o algoritmo Gradiente Descendente Estoc√°stico puro, sem suporte a momento ou Nesterov.                                                                                                                                                                                                              | F        | ‚ùå Falso. `torch.optim.SGD` √© bastante flex√≠vel e suporta momento (momentum), amortecimento (dampening), e acelera√ß√£o de Nesterov atrav√©s de seus par√¢metros. üèéÔ∏è **Dica:** O SGD do PyTorch √© mais que o b√°sico.                                                                                                                                                                                                                                                      |
| 58  | NLTK oferece funcionalidades para an√°lise sint√°tica (parsing), permitindo a constru√ß√£o de √°rvores que representam a estrutura gramatical de senten√ßas.                                                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. NLTK inclui v√°rios algoritmos de parsing e gram√°ticas (e.g., CFGs - Context-Free Grammars) para analisar a estrutura sint√°tica das frases. üå≥üó£Ô∏è **Dica:** Parsing = √Årvore da senten√ßa.                                                                                                                                                                                                                                                                |
| 59  | √â poss√≠vel criar subplots em Matplotlib que compartilham um eixo X ou Y usando os par√¢metros `sharex` ou `sharey` na fun√ß√£o `plt.subplots()`.                                                                                                                                                                                                                   | V        | ‚úÖ Verdadeiro. `fig, axs = plt.subplots(2, 1, sharex=True)` criar√° dois subplots empilhados verticalmente que compartilham o mesmo eixo X, √∫til para comparar s√©ries temporais, por exemplo. üîóüìä **Dica:** `sharex`/`sharey` = Eixos conectados, bom para compara√ß√µes.                                                                                                                                                                                                |
| 60  | O comando `jupyter notebook` e `jupyter lab` iniciam interfaces fundamentalmente diferentes, sendo imposs√≠vel abrir arquivos `.ipynb` criados em um no outro.                                                                                                                                                                                                   | F        | ‚ùå Falso. Ambos `jupyter notebook` (interface cl√°ssica) e `jupyter lab` (interface mais moderna e integrada) trabalham com os mesmos arquivos `.ipynb`. JupyterLab pode abrir e gerenciar notebooks da interface cl√°ssica e vice-versa. ‚ÜîÔ∏èüìí **Dica:** O formato do arquivo √© o mesmo, s√≥ a "roupa" (interface) muda.                                                                                                                                                  |
| 61  | A m√©trica de Acur√°cia √© sempre a mais indicada para avaliar modelos de classifica√ß√£o, independentemente do balanceamento das classes.                                                                                                                                                                                                                           | F        | ‚ùå Falso. Acur√°cia pode ser enganosa em datasets desbalanceados. Por exemplo, se 99% dos dados s√£o da classe A, um modelo que sempre prev√™ A ter√° 99% de acur√°cia, mas ser√° in√∫til para prever a classe B. M√©tricas como Precis√£o, Recall, F1-score ou AUC-ROC s√£o mais informativas nesses casos. ‚öñÔ∏èüìâ **Dica:** Dataset desbalanceado? Esque√ßa (s√≥) a acur√°cia!                                                                                                      |
| 62  | `tf.GradientTape` no TensorFlow s√≥ pode ser usado uma vez para calcular gradientes; para calcular m√∫ltiplos gradientes, √© necess√°rio criar m√∫ltiplas inst√¢ncias de `GradientTape`.                                                                                                                                                                              | F        | ‚ùå Falso. Por padr√£o, um `GradientTape` libera seus recursos ap√≥s a primeira chamada a `.gradient()`. No entanto, ele pode ser configurado como "persistente" (`persistent=True`) para permitir m√∫ltiplas chamadas a `.gradient()`. üîÑüìº **Dica:** `persistent=True` para usar a "fita" de gradientes mais de uma vez.                                                                                                                                                 |
| 63  | PyTorch Lightning √© uma biblioteca de alto n√≠vel constru√≠da sobre PyTorch que visa organizar o c√≥digo de treinamento, tornando-o mais modular e reduzindo boilerplate.                                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. PyTorch Lightning abstrai grande parte do loop de treinamento, valida√ß√£o e teste, permitindo que os pesquisadores foquem na pesquisa e na defini√ß√£o do modelo (`LightningModule`). ‚ö°Ô∏èüìù **Dica:** Lightning = PyTorch organizado e menos repetitivo.                                                                                                                                                                                                    |
| 64  | TF-IDF (Term Frequency-Inverse Document Frequency) atribui um peso maior a palavras que s√£o frequentes em um √∫nico documento, mas raras em toda a cole√ß√£o de documentos.                                                                                                                                                                                        | V        | ‚úÖ Verdadeiro. Essa √© a ess√™ncia do TF-IDF: valorizar termos que s√£o distintivos para um documento espec√≠fico dentro de um corpus maior. üéØüìÑ **Dica:** TF-IDF = Import√¢ncia = Frequente no doc / Raro no geral.                                                                                                                                                                                                                                                       |
| 65  | O m√©todo `set_title()` em um objeto `Axes` do Matplotlib √© usado para definir o t√≠tulo da figura inteira, n√£o apenas do subplot espec√≠fico.                                                                                                                                                                                                                     | F        | ‚ùå Falso. `ax.set_title()` define o t√≠tulo do subplot (eixo) `ax`. Para o t√≠tulo da figura inteira, usa-se `fig.suptitle()`. üñºÔ∏èüÜöüìú **Dica:** `ax.set_title()` = t√≠tulo do subplot. `fig.suptitle()` = t√≠tulo da figura (geral).                                                                                                                                                                                                                                      |
| 66  | O kernel de um Jupyter Notebook mant√©m seu estado (vari√°veis, importa√ß√µes) entre as execu√ß√µes de diferentes c√©lulas, a menos que seja reiniciado.                                                                                                                                                                                                               | V        | ‚úÖ Verdadeiro. Esse estado compartilhado √© uma caracter√≠stica central, permitindo um fluxo de trabalho iterativo. √â tamb√©m uma fonte potencial de erros se n√£o gerenciado com cuidado (e.g., ordem de execu√ß√£o). üß†‚öôÔ∏è **Dica:** Kernel = C√©rebro do notebook, com mem√≥ria. Reiniciar limpa a mem√≥ria.                                                                                                                                                                  |
| 67  | O `DBSCAN` √© um algoritmo de clusteriza√ß√£o em Scikit-learn que requer a especifica√ß√£o do n√∫mero de clusters (k) como hiperpar√¢metro.                                                                                                                                                                                                                            | F        | ‚ùå Falso. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) √© baseado em densidade e n√£o requer que o n√∫mero de clusters seja pr√©-definido. Ele encontra clusters de formatos arbitr√°rios e identifica outliers. Seus principais hiperpar√¢metros s√£o `eps` e `min_samples`.  densit√© üåå **Dica:** K-Means precisa de `k`. DBSCAN n√£o.                                                                                                               |
| 68  | A fun√ß√£o `tf.function` no TensorFlow √© usada principalmente para converter c√≥digo Python em JavaScript para execu√ß√£o no navegador via TensorFlow.js.                                                                                                                                                                                                            | F        | ‚ùå Falso. `tf.function` √© um decorador que compila uma fun√ß√£o Python em um grafo TensorFlow cham√°vel e otimizado, visando melhor performance e portabilidade. Para TensorFlow.js, existe um processo de convers√£o de modelos separado. üêç‚û°Ô∏è‚öôÔ∏è **Dica:** `tf.function` = Python para Grafo Otimizado de TF.                                                                                                                                                             |
| 69  | Em PyTorch, `torch.no_grad()` √© um gerenciador de contexto usado para desabilitar o c√°lculo de gradientes, √∫til durante a infer√™ncia para economizar mem√≥ria e tempo.                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro. Envolver c√≥digo de infer√™ncia com `with torch.no_grad():` garante que o Autograd n√£o rastreie opera√ß√µes, o que √© mais eficiente. üö´üìâ **Dica:** `no_grad()` = Sem gradientes = Infer√™ncia mais r√°pida.                                                                                                                                                                                                                                                  |
| 70  | O NLTK n√£o oferece suporte a modelos pr√©-treinados para Named Entity Recognition (NER), sendo necess√°rio treinar um modelo do zero para essa tarefa.                                                                                                                                                                                                            | F        | ‚ùå Falso. NLTK fornece um classificador NER pr√©-treinado (`nltk.ne_chunk` ap√≥s POS tagging) que pode identificar entidades como pessoas, organiza√ß√µes e localiza√ß√µes, embora modelos mais modernos (e.g., de spaCy ou Transformers) costumem ser mais precisos. üè∑Ô∏èüåç **Dica:** NLTK tem NER b√°sico. Para SOTA (state-of-the-art), procure em spaCy/Hugging Face.                                                                                                      |
| 71  | Para criar um histograma no Matplotlib, a fun√ß√£o `plt.hist()` √© a mais indicada, e ela automaticamente calcula as faixas (bins) se n√£o especificadas.                                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro. `plt.hist(dados)` gera um histograma. O n√∫mero de bins pode ser especificado com o par√¢metro `bins=`, mas se omitido, Matplotlib escolhe um valor padr√£o. üìä **Dica:** `hist()` = Distribui√ß√£o de frequ√™ncia.                                                                                                                                                                                                                                           |
| 72  | O servidor JupyterHub permite que m√∫ltiplos usu√°rios acessem e executem seus pr√≥prios Jupyter Notebooks em um ambiente compartilhado, gerenciado centralmente.                                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro. JupyterHub √© ideal para equipes, salas de aula ou organiza√ß√µes que precisam fornecer acesso a ambientes Jupyter para v√°rios usu√°rios. üë•üíª **Dica:** Hub = Central para m√∫ltiplos usu√°rios.                                                                                                                                                                                                                                                             |
| 73  | O `Pipeline` do Scikit-learn s√≥ pode conter transformadores; um estimador final (como um classificador) deve ser treinado separadamente ap√≥s o pipeline.                                                                                                                                                                                                        | F        | ‚ùå Falso. O `Pipeline` √© projetado para encadear m√∫ltiplos transformadores *e* um estimador final. Ex: `Pipeline([('scaler', StandardScaler()), ('svc', SVC())])`. Isso garante que todo o fluxo seja tratado como uma unidade. üîó‚û°Ô∏èüéØ **Dica:** Pipeline = Pr√©-processamento + Modelo Final, tudo junto.                                                                                                                                                              |
| 74  | TensorFlow e Keras n√£o oferecem suporte nativo para o treinamento de modelos em TPUs (Tensor Processing Units), sendo estas exclusivas para uso interno do Google.                                                                                                                                                                                              | F        | ‚ùå Falso. TensorFlow (com Keras) tem excelente suporte para treinamento em TPUs, especialmente atrav√©s do Google Cloud. APIs como `TPUStrategy` facilitam a distribui√ß√£o do treinamento. ‚òÅÔ∏èüöÄ **Dica:** Google fez TPUs, Google fez TensorFlow. Eles se d√£o bem!                                                                                                                                                                                                       |
| 75  | O m√©todo `item()` em um tensor PyTorch de um √∫nico elemento √© usado para obter o valor Python escalar contido no tensor.                                                                                                                                                                                                                                        | V        | ‚úÖ Verdadeiro. Se `x` √© um tensor com um √∫nico valor (e.g., resultado de uma fun√ß√£o de perda), `x.item()` retorna esse valor como um n√∫mero Python padr√£o. √ötil para logging ou compara√ß√µes.  ‡§Ö‡§ï‡•á‡§≤‡§æ ‚û°Ô∏è üî¢ **Dica:** `item()` = Valor Python de um tensor de 1 elemento.                                                                                                                                                                                                |
| 76  | A an√°lise de sentimento utilizando NLTK e Scikit-learn normalmente envolve a transforma√ß√£o de texto em vetores num√©ricos antes de alimentar um classificador.                                                                                                                                                                                                   | V        | ‚úÖ Verdadeiro. Algoritmos de ML cl√°ssicos (como os do Scikit-learn) esperam entrada num√©rica. T√©cnicas como Bag-of-Words ou TF-IDF (usando `CountVectorizer` ou `TfidfVectorizer` do Scikit-learn) s√£o usadas para converter texto em vetores. üìú‚û°Ô∏èüî¢ü§ñ **Dica:** Texto para n√∫meros √© um passo crucial em PLN com ML cl√°ssico.                                                                                                                                        |
| 77  | Alterar o backend do Matplotlib (e.g., de `inline` para `notebook` no Jupyter) n√£o tem impacto na interatividade ou na forma como os gr√°ficos s√£o renderizados.                                                                                                                                                                                                 | F        | ‚ùå Falso. O backend define como e onde os gr√°ficos s√£o renderizados. Mudar para `notebook` habilita interatividade (zoom, pan) que n√£o existe no `inline` (est√°tico). Outros backends (Qt, Tk) abrem janelas separadas. üñºÔ∏è‚öôÔ∏è **Dica:** Backend = Motor de renderiza√ß√£o. Diferentes motores, diferentes comportamentos.                                                                                                                                                |
| 78  | Todos os "magic commands" do Jupyter s√£o precedidos por um √∫nico sinal de porcentagem (`%`).                                                                                                                                                                                                                                                                    | F        | ‚ùå Falso. Existem "line magics" (prefixo `%`, aplicam-se a uma linha) e "cell magics" (prefixo `%%`, aplicam-se √† c√©lula inteira). Ex: `%timeit` (linha) vs. `%%timeit` (c√©lula). ü™Ñ‚ú® **Dica:** `%` = linha, `%%` = c√©lula.                                                                                                                                                                                                                                            |
| 79  | Em Scikit-learn, o atributo `coef_` de um modelo de Regress√£o Linear treinado cont√©m os coeficientes angulares (pesos) para cada feature, e `intercept_` cont√©m o coeficiente linear.                                                                                                                                                                           | V        | ‚úÖ Verdadeiro. Ap√≥s o `fit()`, esses atributos armazenam os par√¢metros aprendidos pelo modelo de regress√£o linear. üìà‚öñÔ∏è **Dica:** `coef_` = inclina√ß√£o das retas/planos. `intercept_` = onde cruza o eixo y.                                                                                                                                                                                                                                                           |
| 80  | O formato SavedModel do TensorFlow √© um formato universal que permite que modelos sejam usados tanto no TensorFlow quanto no PyTorch sem convers√£o.                                                                                                                                                                                                             | F        | ‚ùå Falso. SavedModel √© o formato de serializa√ß√£o nativo do TensorFlow. Para usar um modelo TensorFlow no PyTorch (ou vice-versa), geralmente √© necess√°ria uma convers√£o para um formato intermedi√°rio como ONNX (Open Neural Network Exchange). üîÑüåâ **Dica:** ONNX √© a "ponte" entre frameworks.                                                                                                                                                                      |
| 81  | `torch.nn.Module` √© a classe base para todos os m√≥dulos de redes neurais em PyTorch. Qualquer modelo customizado deve herdar desta classe.                                                                                                                                                                                                                      | V        | ‚úÖ Verdadeiro. Para criar modelos ou camadas customizadas, voc√™ herda de `nn.Module` e implementa o construtor (`__init__`) para definir as sub-camadas e o m√©todo `forward()` para definir o fluxo de dados. üèóÔ∏èüß† **Dica:** `nn.Module` = Molde para seus modelos PyTorch.                                                                                                                                                                                           |
| 82  | NLTK e spaCy s√£o bibliotecas concorrentes que oferecem funcionalidades muito similares, sendo NLTK geralmente preferida para aplica√ß√µes em produ√ß√£o devido √† sua performance superior.                                                                                                                                                                          | F        | ‚ùå Falso. Embora ambas sejam para PLN, spaCy √© frequentemente preferida para produ√ß√£o devido √† sua performance, modelos pr√©-treinados otimizados e design focado em efici√™ncia. NLTK √© excelente para aprendizado e pesquisa, com uma gama mais ampla de algoritmos e recursos l√©xicos. üöÄüÜöüìö **Dica:** spaCy = Produ√ß√£o/Performance. NLTK = Ensino/Flexibilidade.                                                                                                    |
| 83  | `matplotlib.pyplot.scatter()` √© usado para criar gr√°ficos de dispers√£o, onde cada ponto representa um par de valores de duas vari√°veis.                                                                                                                                                                                                                         | V        | ‚úÖ Verdadeiro. Gr√°ficos de dispers√£o s√£o ideais para visualizar a rela√ß√£o entre duas vari√°veis num√©ricas. ‚ú® **Dica:** `scatter` = Pontinhos para ver correla√ß√£o.                                                                                                                                                                                                                                                                                                       |
| 84  | Uma vez que uma c√©lula de c√≥digo √© executada em um Jupyter Notebook, seu output √© fixo e n√£o pode ser limpo ou alterado sem reexecutar a c√©lula.                                                                                                                                                                                                                | F        | ‚ùå Falso. Voc√™ pode limpar o output de uma c√©lula espec√≠fica (ou de todas as c√©lulas) atrav√©s do menu do Jupyter ("Cell > Current Outputs > Clear" ou "Cell > All Output > Clear") sem precisar reexecutar o c√≥digo. üßπüìÑ **Dica:** Limpar outputs √© √∫til antes de commitar notebooks no Git.                                                                                                                                                                          |
| 85  | A t√©cnica de "Bag of Words" (BoW) preserva a ordem original das palavras no texto ao criar a representa√ß√£o vetorial.                                                                                                                                                                                                                                            | F        | ‚ùå Falso. Bag of Words trata o texto como um "saco de palavras", desconsiderando a ordem e a gram√°tica, focando apenas na contagem de ocorr√™ncias de cada palavra. üõçÔ∏èüìú **Dica:** BoW = Ordem n√£o importa, s√≥ a contagem.                                                                                                                                                                                                                                             |
| 86  | `tf.distribute.Strategy` √© uma API no TensorFlow para treinar modelos em uma √∫nica GPU, otimizando o uso da mem√≥ria.                                                                                                                                                                                                                                            | F        | ‚ùå Falso. `tf.distribute.Strategy` √© projetada para distribuir o treinamento em M√öLTIPLAS GPUs, M√öLTIPLAS m√°quinas ou TPUs. Para uma √∫nica GPU, TensorFlow geralmente a utiliza por padr√£o se dispon√≠vel. üåçü§ù **Dica:** `distribute` = distribuir em v√°rios dispositivos.                                                                                                                                                                                             |
| 87  | A fun√ß√£o `torch.save()` em PyTorch pode ser usada para salvar tanto o modelo inteiro (arquitetura e pesos) quanto apenas o dicion√°rio de estado do modelo (`state_dict`).                                                                                                                                                                                       | V        | ‚úÖ Verdadeiro. Salvar o `state_dict` √© geralmente recomendado por ser mais flex√≠vel para carregar. `torch.save(model.state_dict(), PATH)` salva os pesos, enquanto `torch.save(model, PATH)` tenta salvar o objeto modelo inteiro via pickle. üíæ‚öôÔ∏è **Dica:** `state_dict` √© o jeito preferido para salvar/carregar pesos.                                                                                                                                              |
| 88  | O algoritmo de Naive Bayes, dispon√≠vel no NLTK e Scikit-learn, assume que as features s√£o condicionalmente independentes dada a classe.                                                                                                                                                                                                                         | V        | ‚úÖ Verdadeiro. Essa √© a suposi√ß√£o "naive" (ing√™nua) fundamental do Naive Bayes, que simplifica o c√°lculo de probabilidades e torna o algoritmo eficiente, mesmo que a suposi√ß√£o nem sempre seja verdadeira na pr√°tica. üîó‚õìÔ∏è **Dica:** Naive Bayes = Independ√™ncia condicional das features.                                                                                                                                                                            |
| 89  | Matplotlib n√£o permite a personaliza√ß√£o de cores, estilos de linha ou marcadores nos gr√°ficos; utiliza-se sempre as configura√ß√µes padr√£o.                                                                                                                                                                                                                       | F        | ‚ùå Falso. Matplotlib √© altamente personaliz√°vel. Par√¢metros como `color`, `linestyle`, `marker`, `linewidth`, etc., permitem controle granular sobre a apar√™ncia dos plots. üé®üñåÔ∏è **Dica:** Matplotlib pode ser t√£o feio ou bonito quanto sua paci√™ncia para customizar.                                                                                                                                                                                               |
| 90  | Arquivos `.py` podem ser diretamente abertos e executados como notebooks no JupyterLab sem qualquer convers√£o pr√©via.                                                                                                                                                                                                                                           | F        | ‚ùå Falso. JupyterLab pode *editar* arquivos `.py` em seu editor de texto, mas para execut√°-los como um notebook (com c√©lulas e outputs), eles precisam estar no formato `.ipynb` ou serem importados/copiados para c√©lulas de um notebook. üìú‚û°Ô∏èüìí **Dica:** `.py` √© script, `.ipynb` √© notebook. Precisa de "m√°gica" (ou `nbconvert`) para um virar o outro.                                                                                                           |
| 91  | O `KMeans` do Scikit-learn sempre converge para a mesma solu√ß√£o de clusteriza√ß√£o, independentemente da inicializa√ß√£o dos centr√≥ides.                                                                                                                                                                                                                            | F        | ‚ùå Falso. A inicializa√ß√£o dos centr√≥ides no K-Means pode afetar o resultado final, pois o algoritmo pode convergir para m√≠nimos locais. Por isso, executa-se m√∫ltiplas vezes com diferentes inicializa√ß√µes (par√¢metro `n_init`). üé≤üîÑ **Dica:** K-Means √© sens√≠vel √† inicializa√ß√£o. `n_init` > 1 √© recomendado.                                                                                                                                                        |
| 92  | Checkpoints durante o treinamento de modelos TensorFlow/Keras s√£o usados exclusivamente para restaurar o treinamento em caso de falha, n√£o para salvar o melhor modelo.                                                                                                                                                                                         | F        | ‚ùå Falso. Callbacks como `ModelCheckpoint` podem ser configurados para salvar o modelo (ou apenas os pesos) periodicamente e/ou quando uma m√©trica monitorada (e.g., `val_loss`) melhora, permitindo salvar o "melhor" modelo encontrado durante o treinamento. üíæüèÜ **Dica:** Checkpoints = Salvar progresso E/OU melhor vers√£o.                                                                                                                                      |
| 93  | O PyTorch Autograd n√£o consegue calcular gradientes de segunda ordem (gradientes de gradientes), limitando sua aplica√ß√£o em certas pesquisas avan√ßadas.                                                                                                                                                                                                         | F        | ‚ùå Falso. PyTorch Autograd suporta o c√°lculo de gradientes de ordem superior (e.g., Hessianas) usando `torch.autograd.grad` com `create_graph=True`. ü§Øüìà **Dica:** PyTorch √© poderoso o suficiente para gradientes de gradientes.                                                                                                                                                                                                                                     |
| 94  | Todas as funcionalidades do NLTK, incluindo seus corpora e modelos, s√£o automaticamente instaladas quando a biblioteca √© instalada via `pip install nltk`.                                                                                                                                                                                                      | F        | ‚ùå Falso. `pip install nltk` instala o framework da biblioteca. Os dados adicionais (corpora, modelos pr√©-treinados como `punkt`, `wordnet`, `averaged_perceptron_tagger`) precisam ser baixados separadamente usando `nltk.download()`. üì¶üì• **Dica:** Instalar NLTK √© s√≥ o come√ßo. `nltk.download()` √© o pr√≥ximo passo para usar muitos recursos.                                                                                                                    |
| 95  | Em Matplotlib, `plt.figure(figsize=(width, height))` √© usado para definir o tamanho da figura em pixels.                                                                                                                                                                                                                                                        | F        | ‚ùå Falso. O par√¢metro `figsize` em `plt.figure()` define o tamanho da figura em *polegadas* (inches), n√£o pixels. A resolu√ß√£o em pixels depender√° do DPI (dots per inch) da figura. üìèüñºÔ∏è **Dica:** `figsize` = Polegadas. DPI afeta o tamanho final em pixels.                                                                                                                                                                                                        |
| 96  | Jupyter Notebooks s√£o limitados a executar c√≥digo em um √∫nico kernel por vez; n√£o √© poss√≠vel ter c√©lulas de Python e R no mesmo notebook e execut√°-las com seus respectivos kernels.                                                                                                                                                                            | V        | ‚úÖ Verdadeiro. Um notebook est√° associado a *um* kernel espec√≠fico (e.g., Python 3, R, Julia). Para usar m√∫ltiplas linguagens com seus pr√≥prios kernels no mesmo "projeto", voc√™ usaria m√∫ltiplos notebooks ou ferramentas que permitem interopera√ß√£o entre linguagens dentro de um kernel (menos comum). ‚ÜîÔ∏èüó£Ô∏è **Dica:** Um notebook = Um c√©rebro (kernel) por vez.                                                                                                   |
| 97  | A regulariza√ß√£o L1 (Lasso) no Scikit-learn tende a zerar os coeficientes de features menos importantes, realizando uma sele√ß√£o de features impl√≠cita.                                                                                                                                                                                                           | V        | ‚úÖ Verdadeiro. A penalidade L1 adiciona o valor absoluto dos coeficientes √† fun√ß√£o de custo, o que pode encolher alguns coeficientes exatamente para zero, efetivamente removendo-os do modelo. üéØüóëÔ∏è **Dica:** L1 = Lasso = Sparsity (coeficientes zero) = Sele√ß√£o de features.                                                                                                                                                                                       |
| 98  | Modelos Keras salvos no formato HDF5 (`.h5`) s√£o completamente auto-contidos e n√£o requerem que a defini√ß√£o da arquitetura do modelo em c√≥digo Python esteja dispon√≠vel para carreg√°-los.                                                                                                                                                                       | V        | ‚úÖ Verdadeiro. O formato HDF5 para modelos Keras (usando `model.save()`) salva a arquitetura do modelo, os pesos, a configura√ß√£o de treinamento (loss, optimizer) e o estado do otimizador, permitindo recriar o modelo a partir do arquivo. üì¶üß† **Dica:** `.h5` (Keras) = Modelo completo em um arquivo.                                                                                                                                                             |
| 99  | Em PyTorch, o m√©todo `apply(fn)` de um `nn.Module` aplica recursivamente uma fun√ß√£o `fn` a todos os subm√≥dulos contidos no m√≥dulo, incluindo ele mesmo.                                                                                                                                                                                                         | V        | ‚úÖ Verdadeiro. Isso √© √∫til para inicializar pesos de forma customizada em todas as camadas de um modelo, ou para aplicar outras transforma√ß√µes a subm√≥dulos.  recursively üîÑüõ†Ô∏è **Dica:** `apply` √© como um "for each" para os m√≥dulos dentro do seu modelo.                                                                                                                                                                                                           |
| 100 | A biblioteca Matplotlib √© a √∫nica op√ß√£o para visualiza√ß√£o de dados em Python, n√£o existindo alternativas como Seaborn ou Plotly.                                                                                                                                                                                                                                | F        | ‚ùå Falso. Existem muitas bibliotecas de visualiza√ß√£o em Python. Seaborn (baseada em Matplotlib) √© popular para gr√°ficos estat√≠sticos. Plotly √© conhecida por gr√°ficos interativos e dashboards. Bokeh, Altair s√£o outras. üìäüé®üåü **Dica:** Matplotlib √© a "m√£e", mas tem muitos "filhos" e "primos" com especialidades diferentes.                                                                                                                                     |
| 101 | Considere o c√≥digo Scikit-learn: `from sklearn.preprocessing import StandardScaler; import numpy as np; data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]]); scaler = StandardScaler(); transformed_data = scaler.fit_transform(data)`. Ap√≥s a execu√ß√£o, a m√©dia de cada coluna em `transformed_data` ser√° aproximadamente 0 e o desvio padr√£o aproximadamente 1. | V        | ‚úÖ Verdadeiro. `StandardScaler` subtrai a m√©dia e divide pelo desvio padr√£o de cada feature (coluna). `fit_transform` calcula essas estat√≠sticas de `data` e aplica a transforma√ß√£o. üìä **Dica:** Lembre-se que `StandardScaler` centraliza na m√©dia 0 e escala para vari√¢ncia unit√°ria.                                                                                                                                                                               |
| 102 | No TensorFlow, o c√≥digo `import tensorflow as tf; @tf.function def f(x): return x + tf.constant(1); print(f(tf.constant(2)))` imprimir√° `tf.Tensor(3, shape=(), dtype=int32)` na primeira execu√ß√£o e `3` nas execu√ß√µes subsequentes devido ao cache do grafo.                                                                                                   | F        | ‚ùå Falso. `tf.function` retorna um `tf.Tensor`. A impress√£o ser√° `tf.Tensor(3, shape=(), dtype=int32)` (ou similar, dependendo da vers√£o exata do TF) em todas as execu√ß√µes, pois o tipo de retorno da fun√ß√£o tracejada √© um tensor, n√£o um escalar Python. üîÑ **Dica:** `tf.function` opera com tensores. A convers√£o para escalar Python (`.numpy()`) seria expl√≠cita.                                                                                               |
| 103 | Dado o c√≥digo PyTorch: `import torch; x = torch.tensor([1., 2.], requires_grad=True); y = x * 2; z = y.mean(); z.backward()`. Ap√≥s a execu√ß√£o, `x.grad` ser√° `torch.tensor([1., 1.])`.                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. `z = (2*x[0] + 2*x[1])/2 = x[0] + x[1]`. `dz/dx[0] = 1`, `dz/dx[1] = 1`. ü§ì **Dica:** Calcule a derivada parcial de `z` em rela√ß√£o a cada elemento de `x`. `y.mean()` resulta em `(y1+y2)/2`.                                                                                                                                                                                                                                                           |
| 104 | O c√≥digo NLTK `from nltk.stem import PorterStemmer; ps = PorterStemmer(); print(ps.stem("studies"))` resultar√° na sa√≠da `study`.                                                                                                                                                                                                                                | F        | ‚ùå Falso. O PorterStemmer √© um algoritmo de stemming que remove sufixos de forma agressiva. Para "studies", ele provavelmente retornar√° "studi". Lematizadores visam a forma dicionarizada "study". ‚úÇÔ∏è **Dica:** Stemmer = "cortar". `PorterStemmer` √© conhecido por ser mais "bruto".                                                                                                                                                                                 |
| 105 | No Matplotlib, o c√≥digo `import matplotlib.pyplot as plt; plt.plot([1, 2, 3], [4, 5, 1]); plt.title("Meu Gr√°fico"); plt.show()` exibir√° um gr√°fico de linhas com o t√≠tulo "Meu Gr√°fico" na parte superior da figura.                                                                                                                                            | V        | ‚úÖ Verdadeiro. `plt.plot()` cria o gr√°fico de linhas e `plt.title()` define o t√≠tulo da figura atual (ou eixo, dependendo do contexto `pyplot`). `plt.show()` exibe a figura. üñºÔ∏è **Dica:** `plt.title()` afeta a plotagem ativa no paradigma `pyplot`.                                                                                                                                                                                                                |
| 106 | Considere o c√≥digo Scikit-learn: `from sklearn.linear_model import LogisticRegression; X = [[1],[2]]; y = [0,1]; model = LogisticRegression().fit(X, y)`. O atributo `model.coef_` conter√° um array bidimensional representando os coeficientes, mesmo para uma √∫nica feature.                                                                                  | V        | ‚úÖ Verdadeiro. Scikit-learn mant√©m consist√™ncia. `coef_` √© geralmente 2D `(n_classes, n_features)` ou `(1, n_features)` para regress√£o bin√°ria/multiclasse com uma sa√≠da por classe. Para regress√£o log√≠stica bin√°ria, ser√° `(1, n_features)`. üìê **Dica:** `coef_` e `intercept_` s√£o arrays, mesmo para casos simples, para manter a API geral.                                                                                                                      |
| 107 | No TensorFlow, `import tensorflow as tf; v = tf.Variable(5.0); with tf.GradientTape() as tape: y = v * v; grads = tape.gradient(y, v)`. A vari√°vel `grads` conter√° o valor `10.0` como um `tf.Tensor`.                                                                                                                                                          | V        | ‚úÖ Verdadeiro. A derivada de `v^2` em rela√ß√£o a `v` √© `2v`. Com `v=5.0`, o gradiente √© `10.0`. `tape.gradient` retorna um `tf.Tensor`. üìà **Dica:** `GradientTape` √© o "gravador" de opera√ß√µes para diferencia√ß√£o autom√°tica. Lembre-se das regras b√°sicas de deriva√ß√£o.                                                                                                                                                                                               |
| 108 | O c√≥digo PyTorch: `import torch; layer = torch.nn.Linear(10, 5); input_tensor = torch.randn(1, 10); output = layer(input_tensor)`. A forma (shape) de `output` ser√° `(1, 10)`.                                                                                                                                                                                  | F        | ‚ùå Falso. `torch.nn.Linear(in_features, out_features)` transforma um tensor de entrada com `in_features` para um tensor de sa√≠da com `out_features`. Portanto, `output.shape` ser√° `(1, 5)`. ‚û°Ô∏è **Dica:** `nn.Linear(A, B)` pega `A` features e devolve `B` features. O batch size (primeira dimens√£o) geralmente √© mantido.                                                                                                                                           |
| 109 | Dado o c√≥digo NLTK: `from nltk.tokenize import word_tokenize; text = "NLTK is fun!"; tokens = word_tokenize(text)`. A vari√°vel `tokens` ser√° a lista `['NLTK', 'is', 'fun']`.                                                                                                                                                                                   | F        | ‚ùå Falso. `word_tokenize` do NLTK geralmente trata a pontua√ß√£o como um token separado. A sa√≠da mais prov√°vel √© `['NLTK', 'is', 'fun', '!']`. üó£Ô∏è punctuation **Dica:** Tokenizadores padr√£o costumam separar pontua√ß√£o. Verifique a documenta√ß√£o para comportamentos espec√≠ficos.                                                                                                                                                                                      |
| 110 | No Matplotlib: `import matplotlib.pyplot as plt; fig, ax = plt.subplots(); ax.plot([0,1], [0,1]); ax.set_xlabel("Eixo X"); plt.xlabel("Novo Eixo X")`. O r√≥tulo final do eixo X ser√° "Novo Eixo X".                                                                                                                                                             | V        | ‚úÖ Verdadeiro. No Jupyter e ambientes similares, `plt.xlabel` (interface `pyplot`) opera no "eixo atual". Se `ax` foi o √∫ltimo eixo criado ou ativo, `plt.xlabel` o modificar√°, sobrescrevendo `ax.set_xlabel`. üé® **Dica:** `pyplot` mant√©m um estado global do eixo/figura atual. Chamadas `plt.` afetam esse estado. Em scripts, a ordem pode ser mais previs√≠vel.                                                                                                  |
| 111 | O c√≥digo Scikit-learn `from sklearn.model_selection import train_test_split; X = [[1],[2],[3],[4]]; y = [0,0,1,1]; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)`. `X_test` conter√° `[[3],[4]]`.                                                                                                                      | V        | ‚úÖ Verdadeiro. Com `shuffle=False`, `train_test_split` pega os primeiros `(1 - test_size)` % para treino e o restante para teste. Com `test_size=0.5`, os primeiros 50% (`[[1],[2]]`) v√£o para `X_train` e os √∫ltimos 50% (`[[3],[4]]`) para `X_test`. üî™ **Dica:** `shuffle=False` = divis√£o sequencial. √ötil para s√©ries temporais se a ordem importa.                                                                                                               |
| 112 | Em TensorFlow, o c√≥digo `import tensorflow as tf; a = tf.constant([1,2]); b = tf.constant([3,4]); c = tf.add(a,b)`. A vari√°vel `c` ser√° um objeto `tf.Tensor` com valor `[4, 6]`.                                                                                                                                                                               | V        | ‚úÖ Verdadeiro. `tf.add` realiza a adi√ß√£o elemento a elemento entre dois tensores. O resultado √© um `tf.Tensor` contendo a soma. ‚ûï **Dica:** Opera√ß√µes b√°sicas em tensores s√£o geralmente elemento a elemento.                                                                                                                                                                                                                                                          |
| 113 | C√≥digo PyTorch: `import torch; model = torch.nn.Sequential(torch.nn.Linear(4,2), torch.nn.ReLU(), torch.nn.Linear(2,1)); print(len(list(model.parameters())))`. A sa√≠da ser√° `2`.                                                                                                                                                                               | F        | ‚ùå Falso. Uma camada `torch.nn.Linear(A,B)` tem dois tensores de par√¢metros: pesos (matriz `B x A`) e bias (vetor `B`). Ent√£o, `Linear(4,2)` tem 2 par√¢metros (pesos, bias) e `Linear(2,1)` tem 2 par√¢metros (pesos, bias). `ReLU` n√£o tem par√¢metros trein√°veis. Total: 4 tensores de par√¢metros. üî¢ **Dica:** Cada `Linear` tem pesos E bias. Conte-os!                                                                                                              |
| 114 | O c√≥digo NLTK: `from nltk.corpus import stopwords; stop_words = stopwords.words('english'); print(len(stop_words) > 10)`. Esta afirma√ß√£o resultar√° em `True`.                                                                                                                                                                                                   | V        | ‚úÖ Verdadeiro. A lista de stopwords em ingl√™s padr√£o do NLTK cont√©m um n√∫mero significativo de palavras comuns (geralmente mais de 100). üìö **Dica:** As listas de stopwords s√£o razoavelmente extensas.                                                                                                                                                                                                                                                               |
| 115 | No Matplotlib, `import matplotlib.pyplot as plt; x = [1,2,3]; y1 = [1,4,9]; y2 = [1,2,3]; plt.plot(x, y1, label='Quadratic'); plt.plot(x, y2, label='Linear'); plt.legend()`. A legenda exibir√° "Quadratic" e "Linear".                                                                                                                                         | V        | ‚úÖ Verdadeiro. Ao fornecer o argumento `label` para cada chamada `plt.plot()` e depois chamar `plt.legend()`, o Matplotlib cria uma legenda com os r√≥tulos especificados. üè∑Ô∏è **Dica:** `label` no plot + `legend()` = legenda funcional.                                                                                                                                                                                                                              |
| 116 | Scikit-learn: `from sklearn.cluster import KMeans; import numpy as np; X = np.array([[1,1],[1,2],[5,5],[5,6]]); kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto').fit(X)`. O atributo `kmeans.labels_` conter√° um array indicando a qual dos 2 clusters cada ponto de `X` foi atribu√≠do.                                                             | V        | ‚úÖ Verdadeiro. Ap√≥s o `fit`, o atributo `labels_` de um objeto `KMeans` armazena o √≠ndice do cluster (0 ou 1, neste caso) para cada amostra de entrada. üè∑Ô∏è‚û°Ô∏èüë• **Dica:** `labels_` √© o resultado da clusteriza√ß√£o, indicando o grupo de cada ponto.                                                                                                                                                                                                                   |
| 117 | TensorFlow: `import tensorflow as tf; model = tf.keras.Sequential([tf.keras.layers.Dense(10, input_shape=(5,)), tf.keras.layers.Dense(1)]); model.compile(optimizer='sgd', loss='mse')`. Este modelo n√£o pode ser treinado porque a fun√ß√£o de ativa√ß√£o n√£o foi especificada para as camadas `Dense`.                                                            | F        | ‚ùå Falso. As camadas `Dense` em Keras t√™m uma ativa√ß√£o padr√£o se n√£o especificada (geralmente `None`, o que significa ativa√ß√£o linear `a(x)=x`). O modelo compila e pode ser treinado. A performance pode n√£o ser √≥tima sem ativa√ß√µes n√£o-lineares entre as camadas, mas ele treina. ‚öôÔ∏è **Dica:** Keras tem padr√µes sensatos. A aus√™ncia de ativa√ß√£o expl√≠cita n√£o impede a compila√ß√£o.                                                                                |
| 118 | PyTorch: `import torch; t = torch.randn(3,4); t_transposed = t.T`. O tensor `t_transposed` ter√° `shape (3,4)` e compartilhar√° os mesmos dados subjacentes que `t`.                                                                                                                                                                                              | F        | ‚ùå Falso. `t.T` (ou `t.transpose(0,1)`) transp√µe o tensor. Se `t` tem `shape (3,4)`, `t_transposed` ter√° `shape (4,3)`. Ele compartilhar√° os dados subjacentes (√© uma view). üîÑ **Dica:** `.T` √© o atalho para transpor. Dimens√µes trocam de lugar.                                                                                                                                                                                                                    |
| 119 | NLTK: `from nltk.probability import FreqDist; words = ['apple', 'banana', 'apple']; fd = FreqDist(words); print(fd['apple'])`. A sa√≠da ser√° `1`.                                                                                                                                                                                                                | F        | ‚ùå Falso. `FreqDist` conta a frequ√™ncia das palavras. "apple" aparece 2 vezes. A sa√≠da ser√° `2`. üìä **Dica:** `FreqDist` √© um contador.                                                                                                                                                                                                                                                                                                                                |
| 120 | Matplotlib: `import matplotlib.pyplot as plt; fig, (ax1, ax2) = plt.subplots(1, 2); ax1.plot([1,2]); ax2.scatter([1,2],[2,1])`. O c√≥digo cria uma figura com dois subplots lado a lado; o primeiro com um gr√°fico de linha e o segundo com um gr√°fico de dispers√£o.                                                                                             | V        | ‚úÖ Verdadeiro. `plt.subplots(1, 2)` cria uma figura e uma grade de 1 linha e 2 colunas de eixos. `ax1` e `ax2` referenciam esses eixos, nos quais diferentes tipos de plots s√£o desenhados. üñºÔ∏èüñºÔ∏è **Dica:** `subplots(nrows, ncols)` √© a chave para m√∫ltiplos gr√°ficos em uma figura.                                                                                                                                                                                 |
| 121 | C√≥digo Scikit-learn: `from sklearn.preprocessing import MinMaxScaler; data = [[-1], [0], [1]]; scaler = MinMaxScaler(); scaled_data = scaler.fit_transform(data)`. `scaled_data[0][0]` ser√° `-1.0`.                                                                                                                                                             | F        | ‚ùå Falso. `MinMaxScaler` escala os dados para um intervalo padr√£o de `[0, 1]`. O valor m√≠nimo `-1` ser√° mapeado para `0`, e o m√°ximo `1` ser√° mapeado para `1`. `0` ser√° mapeado para `0.5`.  üìè **Dica:** `MinMaxScaler` padr√£o = `[0, 1]`. Pense em uma r√©gua onde o m√≠nimo vira 0 e o m√°ximo vira 1.                                                                                                                                                                |
| 122 | TensorFlow: `import tensorflow as tf; x = tf.constant([1.0, 2.0]); y = tf.Variable([3.0, 4.0]); z = x + y`. A vari√°vel `z` ser√° um `tf.Variable`.                                                                                                                                                                                                               | F        | ‚ùå Falso. A adi√ß√£o de um `tf.constant` e um `tf.Variable` resulta em um `tf.Tensor`. Para que `z` fosse uma `tf.Variable`, seria necess√°rio atribuir o resultado a uma `tf.Variable` ou usar `assign` (se `z` j√° fosse uma `tf.Variable`). üß± **Dica:** Opera√ß√µes entre `Tensor` e `Variable` geralmente resultam em `Tensor`. `Variable` √© mais sobre o cont√™iner que *pode ser* modificado.                                                                          |
| 123 | PyTorch: `import torch; loss_fn = torch.nn.CrossEntropyLoss(); outputs = torch.randn(4, 10); targets = torch.randint(0, 10, (4,)); loss = loss_fn(outputs, targets)`. A vari√°vel `loss` ser√° um tensor escalar contendo o valor da perda.                                                                                                                       | V        | ‚úÖ Verdadeiro. `CrossEntropyLoss` espera logits brutos (outputs) e √≠ndices de classe (targets) e calcula a perda de entropia cruzada, que √© um valor escalar (um tensor de 0 dimens√µes). üìâ **Dica:** Fun√ß√µes de perda geralmente agregam os erros em um √∫nico valor escalar para a retropropaga√ß√£o.                                                                                                                                                                   |
| 124 | NLTK: `from nltk.util import ngrams; sentence = "this is a test".split(); print(list(ngrams(sentence, 2)))`. A sa√≠da incluir√° o bigrama `('a', 'test')`.                                                                                                                                                                                                        | V        | ‚úÖ Verdadeiro. `ngrams(sequence, n)` gera tuplas de n itens consecutivos da sequ√™ncia. Para `n=2` (bigramas), ele produzir√° `[('this', 'is'), ('is', 'a'), ('a', 'test')]`. üîó **Dica:** N-gramas s√£o sequ√™ncias de N tokens. Bigramas = 2 tokens.                                                                                                                                                                                                                     |
| 125 | Matplotlib: `import matplotlib.pyplot as plt; plt.bar(['A','B'], [10,15]); plt.savefig('my_plot')`. Este c√≥digo salvar√° o gr√°fico de barras como `my_plot.png` por padr√£o, se a extens√£o n√£o for especificada.                                                                                                                                                  | V        | ‚úÖ Verdadeiro. Se nenhuma extens√£o for fornecida em `savefig`, Matplotlib frequentemente assume `.png` como padr√£o, mas isso pode depender da configura√ß√£o do backend. √â sempre melhor ser expl√≠cito: `plt.savefig('my_plot.png')`. üíæ **Dica:** Seja expl√≠cito com extens√µes em `savefig` para evitar surpresas. `.png` √© um padr√£o comum.                                                                                                                            |
| 126 | Scikit-learn: `from sklearn.ensemble import RandomForestClassifier; from sklearn.pipeline import Pipeline; pipe = Pipeline([('rf', RandomForestClassifier())]); pipe.set_params(rf__n_estimators=50)`. Este c√≥digo configura o pipeline `pipe` para que o `RandomForestClassifier` use 50 √°rvores.                                                              | V        | ‚úÖ Verdadeiro. `set_params` com a sintaxe `nome_do_passo__nome_do_parametro` permite definir hiperpar√¢metros de estimadores dentro de um `Pipeline`. üå≤üå≤ **Dica:** `__` (duplo underscore) √© usado para acessar par√¢metros de steps dentro de um Pipeline ou outros meta-estimadores.                                                                                                                                                                                 |
| 127 | TensorFlow: `import tensorflow as tf; strategy = tf.distribute.MirroredStrategy(); print(strategy.num_replicas_in_sync)`. Se executado em uma m√°quina com 2 GPUs vis√≠veis para TensorFlow, a sa√≠da ser√° `1`.                                                                                                                                                    | F        | ‚ùå Falso. `MirroredStrategy` √© projetada para usar todas as GPUs dispon√≠veis na m√°quina. Se 2 GPUs s√£o vis√≠veis, `num_replicas_in_sync` ser√° `2`. üñ•Ô∏èüñ•Ô∏è **Dica:** `MirroredStrategy` espelha o modelo em todas as GPUs que o TF pode ver.                                                                                                                                                                                                                             |
| 128 | PyTorch: `import torch; x = torch.ones(2,2, requires_grad=True); with torch.no_grad(): y = x * 2;`. O tensor `y` ter√° `requires_grad=False`.                                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro. O bloco `with torch.no_grad():` desabilita o rastreamento de gradientes para todas as opera√ß√µes dentro dele. Assim, `y` n√£o far√° parte do grafo computacional para `backward()`, e `y.requires_grad` ser√° `False`. üö´üìà **Dica:** `torch.no_grad()` √© o "modo de seguran√ßa" para opera√ß√µes que n√£o devem afetar gradientes (e.g., infer√™ncia, atualiza√ß√£o de m√©tricas).                                                                                 |
| 129 | NLTK: `from nltk.corpus import wordnet; syns = wordnet.synsets("dog", pos=wordnet.NOUN); print(len(syns) > 0)`. Este c√≥digo provavelmente imprimir√° `True`.                                                                                                                                                                                                     | V        | ‚úÖ Verdadeiro. "dog" como substantivo (NOUN) tem m√∫ltiplos sentidos (synsets) no WordNet (e.g., o animal, uma pessoa desprez√≠vel). üêïüìö **Dica:** WordNet √© rico. Palavras comuns t√™m v√°rios synsets. `pos` ajuda a desambiguar.                                                                                                                                                                                                                                       |
| 130 | Matplotlib: `import matplotlib.pyplot as plt; plt.plot([1,2,3], [3,2,1], color='red', linestyle='--'); plt.plot([1,2,3], [1,2,3], color='blue', marker='o')`. O gr√°fico resultante ter√° uma linha vermelha tracejada e uma linha azul com marcadores circulares.                                                                                                | V        | ‚úÖ Verdadeiro. Os par√¢metros `color`, `linestyle` e `marker` controlam a apar√™ncia das linhas e pontos plotados. üé®‚ú® **Dica:** Matplotlib oferece vasta customiza√ß√£o. `linestyle` para tra√ßos, `marker` para pontos.                                                                                                                                                                                                                                                   |
| 131 | Scikit-learn: `from sklearn.metrics import accuracy_score; y_true = [0,1,0,1]; y_pred = [0,0,1,1]; print(accuracy_score(y_true, y_pred))`. A sa√≠da ser√° `0.5`.                                                                                                                                                                                                  | V        | ‚úÖ Verdadeiro. Acur√°cia = (Previs√µes Corretas) / (Total de Previs√µes). Corretas: y_true[0]==y_pred[0] (0==0) e y_true[3]==y_pred[3] (1==1). Duas corretas em quatro. 2/4 = 0.5. üéØ **Dica:** Calcule a acur√°cia manualmente para entender. Combine os elementos correspondentes e conte os acertos.                                                                                                                                                                    |
| 132 | TensorFlow: `import tensorflow as tf; dataset = tf.data.Dataset.from_tensor_slices([1,2,3,4,5]); dataset = dataset.batch(2)`. O primeiro elemento iterado de `dataset` ser√° um tensor `tf.Tensor([1, 2], shape=(2,), dtype=int32)`.                                                                                                                             | V        | ‚úÖ Verdadeiro. `.batch(2)` agrupa os elementos do dataset em lotes de tamanho 2. O primeiro lote ser√° `[1,2]`. üì¶ **Dica:** `tf.data.Dataset.batch()` √© fundamental para preparar dados para treinamento em lotes.                                                                                                                                                                                                                                                     |
| 133 | PyTorch: `import torch; p = torch.nn.Parameter(torch.randn(5)); print(p.requires_grad)`. A sa√≠da ser√° `False`.                                                                                                                                                                                                                                                  | F        | ‚ùå Falso. `torch.nn.Parameter` √© um tipo especial de tensor que √© automaticamente registrado como um par√¢metro de um `nn.Module`. Por padr√£o, `requires_grad` para `nn.Parameter` √© `True`, pois eles s√£o destinados a serem treinados. üî• **Dica:** `nn.Parameter` = Tensor que o modelo *deve* aprender. `requires_grad` √© `True` por padr√£o.                                                                                                                        |
| 134 | Jupyter Notebook: `%%timeit -n 10 -r 3 \n for i in range(100): pass`. Este magic command de c√©lula medir√° o tempo de execu√ß√£o do loop `for`, executando-o 10 vezes em cada um dos 3 ciclos de repeti√ß√£o para obter uma m√©dia.                                                                                                                                   | V        | ‚úÖ Verdadeiro. `%%timeit` √© para medir performance. `-n 10` significa que o loop interno √© executado 10 vezes para cada medi√ß√£o. `-r 3` significa que esse processo de medi√ß√£o √© repetido 3 vezes, e a melhor m√©dia √© reportada. ‚è±Ô∏è **Dica:** `%%timeit`: `-n` = loops por medi√ß√£o, `-r` = repeti√ß√µes da medi√ß√£o.                                                                                                                                                      |
| 135 | Matplotlib: `import matplotlib.pyplot as plt; import numpy as np; x = np.linspace(0,10,100); y = np.sin(x); plt.figure(); plt.subplot(2,1,1); plt.plot(x,y); plt.subplot(2,1,2); plt.plot(x,np.cos(x))`. Este c√≥digo produzir√° dois gr√°ficos sobrepostos na mesma √°rea de plotagem.                                                                             | F        | ‚ùå Falso. `plt.subplot(2,1,1)` cria um sistema de eixos na primeira posi√ß√£o de uma grade 2x1. `plt.subplot(2,1,2)` cria outro sistema de eixos na segunda posi√ß√£o *abaixo* do primeiro. Os gr√°ficos estar√£o em √°reas separadas, um em cima do outro. üìä<0xF0><0x9F><0x97><0x84>Ô∏è **Dica:** `subplot(nrows, ncols, index)` define a grade E seleciona o subplot ativo. Chamadas subsequentes com diferentes `index` criam/selecionam outros subplots.                   |
| 136 | Scikit-learn: `from sklearn.decomposition import PCA; import numpy as np; X = np.random.rand(10,5); pca = PCA(n_components=0.95)`. O objeto `pca` ser√° configurado para reter o n√∫mero m√≠nimo de componentes principais que explicam 95% da vari√¢ncia dos dados.                                                                                                | V        | ‚úÖ Verdadeiro. Quando `n_components` √© um float entre 0 e 1, PCA seleciona o n√∫mero de componentes tal que a quantidade de vari√¢ncia explicada seja maior ou igual a esse valor. üéØüìâ **Dica:** `n_components` como float (0-1) = reter % de vari√¢ncia. Como int = reter N componentes.                                                                                                                                                                                |
| 137 | TensorFlow: `import tensorflow as tf; @tf.function def my_func(a, b): if tf.reduce_sum(a) > tf.reduce_sum(b): return a else: return b; result = my_func(tf.constant([1,2]), tf.constant([0,1]))`. O valor de `result` ser√° `tf.Tensor([1 2], shape=(2,), dtype=int32)`.                                                                                         | V        | ‚úÖ Verdadeiro. `tf.reduce_sum([1,2])` √© 3. `tf.reduce_sum([0,1])` √© 1. Como 3 > 1, a fun√ß√£o retorna `a`, que √© `tf.constant([1,2])`. ‚öôÔ∏èüëç **Dica:** `tf.function` tra√ßa o c√≥digo Python que usa ops do TF. Condicionais com tensores (como `tf.cond` ou `if` com tensores boolianos) s√£o convertidos para ops de grafo.                                                                                                                                                |
| 138 | PyTorch: `import torch; linear = torch.nn.Linear(3,1); optimizer = torch.optim.SGD(linear.parameters(), lr=0.1); optimizer.zero_grad()`. Este c√≥digo zera os pesos e bias da camada `linear`.                                                                                                                                                                   | F        | ‚ùå Falso. `optimizer.zero_grad()` zera os *gradientes* (`.grad`) dos par√¢metros que o otimizador est√° gerenciando (neste caso, os par√¢metros de `linear`). N√£o zera os valores dos par√¢metros (pesos e bias) em si. üßπ‚â†0Ô∏è‚É£ **Dica:** `zero_grad()` limpa os gradientes acumulados, n√£o os pesos. Isso √© feito no in√≠cio de cada itera√ß√£o de treinamento.                                                                                                               |
| 139 | NLTK: `from nltk.stem import WordNetLemmatizer; wnl = WordNetLemmatizer(); print(wnl.lemmatize("corpora", pos="n"))`. A sa√≠da ser√° `corpus`.                                                                                                                                                                                                                    | V        | ‚úÖ Verdadeiro. O `WordNetLemmatizer` com a POS tag correta (`"n"` para noun) consegue identificar que "corpora" √© o plural de "corpus". üìö‚û°Ô∏èüìñ **Dica:** Lematiza√ß√£o com POS tag correta √© mais precisa, especialmente para plurais irregulares ou palavras com m√∫ltiplas fun√ß√µes gramaticais.                                                                                                                                                                         |
| 140 | Jupyter Notebook: `my_var = 10 # C√©lula 1 \n print(my_var) # C√©lula 2 \n my_var = 20 # C√©lula 3`. Se a C√©lula 2 for executada ap√≥s a C√©lula 3, a sa√≠da ser√° `10`.                                                                                                                                                                                               | F        | ‚ùå Falso. O kernel do Jupyter mant√©m o estado. Se a C√©lula 3 (`my_var = 20`) for executada, o valor de `my_var` no kernel se torna `20`. Executar a C√©lula 2 (`print(my_var)`) depois disso imprimir√° `20`. üîÑüî¢ **Dica:** A ordem de execu√ß√£o das c√©lulas, n√£o a ordem no papel, dita o estado das vari√°veis no Jupyter. "Restart Kernel and Run All" √© seu amigo para testar a reprodutibilidade.                                                                    |
| 141 | Scikit-learn: `from sklearn.svm import SVC; model = SVC(kernel='linear', C=1.0)`. O par√¢metro `C` controla a largura da margem; valores menores de `C` levam a uma margem menor e menos viola√ß√µes.                                                                                                                                                              | F        | ‚ùå Falso. `C` √© o par√¢metro de regulariza√ß√£o. Valores *menores* de `C` levam a uma margem *maior* (mais suave), permitindo mais viola√ß√µes da margem (classifica√ß√µes incorretas no treino). Valores maiores de `C` tentam classificar todos os exemplos corretamente, levando a uma margem menor e overfitting potencial. ‚ÜîÔ∏èüöß **Dica:** `C` √© o inverso da for√ßa da regulariza√ß√£o. `C` pequeno = mais regulariza√ß√£o = margem maior.                                    |
| 142 | TensorFlow: `import tensorflow as tf; layer = tf.keras.layers.Dropout(0.5); input_data = tf.ones((10,5)); output_training = layer(input_data, training=True); output_inference = layer(input_data, training=False)`. `output_training` e `output_inference` ter√£o os mesmos valores.                                                                            | F        | ‚ùå Falso. Dropout se comporta diferentemente durante o treinamento e infer√™ncia. Com `training=True`, alguns elementos da entrada s√£o zerados aleatoriamente (e o restante √© escalado). Com `training=False`, o dropout n√£o √© aplicado (a camada se comporta como identidade, ou os pesos s√£o escalados no momento do treinamento). üé≠ **Dica:** `training=True` ativa o comportamento de treino (dropout ativo). `training=False` desativa (infer√™ncia).              |
| 143 | PyTorch: `import torch; a = torch.tensor([1,2]); b = torch.tensor([1,2]); print(a == b)`. A sa√≠da ser√° `True`.                                                                                                                                                                                                                                                  | F        | ‚ùå Falso. `a == b` em PyTorch realiza uma compara√ß√£o elemento a elemento e retorna um tensor booleano. A sa√≠da ser√° `tensor([True, True])`. Para um √∫nico booleano indicando se todos os elementos s√£o iguais, use `torch.equal(a,b)`. ‚ÜîÔ∏è‚úÖ **Dica:** Compara√ß√£o de tensores √© elemento a elemento. `torch.equal()` para igualdade total.                                                                                                                               |
| 144 | Matplotlib: `import matplotlib.pyplot as plt; plt.imshow([[1,2],[3,4]], cmap='viridis'); plt.colorbar()`. Este c√≥digo exibir√° uma imagem 2x2 onde os valores s√£o mapeados para cores usando o colormap 'viridis', com uma barra de cores ao lado.                                                                                                               | V        | ‚úÖ Verdadeiro. `imshow` √© usado para exibir dados como uma imagem. `cmap` define o esquema de cores. `colorbar()` adiciona uma legenda que mapeia os valores dos dados para as cores. üñºÔ∏èüåà **Dica:** `imshow` para matrizes como imagens. `colorbar` para entender a escala de cores.                                                                                                                                                                                 |
| 145 | Jupyter: Em uma c√©lula de c√≥digo de um Jupyter Notebook rodando um kernel Python, a execu√ß√£o de `!pip install numpy` instalar√° o pacote NumPy no ambiente Python associado ao kernel, e o pacote estar√° dispon√≠vel para importa√ß√£o em c√©lulas subsequentes sem reiniciar o kernel.                                                                              | V        | ‚úÖ Verdadeiro. O `!` antes de um comando no Jupyter o executa como um comando de shell. `!pip install` geralmente funciona para instalar pacotes no ambiente atual. Na maioria dos casos, o pacote rec√©m-instalado pode ser importado em seguida, embora um rein√≠cio de kernel seja √†s vezes mais seguro para garantir que tudo seja carregado corretamente. ÏÖ∏Ô∏èüêç **Dica:** `!` = comando do shell. √ötil para instala√ß√µes r√°pidas ou comandos do sistema.              |
| 146 | Scikit-learn: `from sklearn.naive_bayes import GaussianNB; import numpy as np; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]); Y = np.array([1, 1, 2, 2]); clf = GaussianNB(); clf.fit(X, Y); print(clf.predict([[-0.8, -1]]))`. A sa√≠da ser√° `[1]`.                                                                                                        | V        | ‚úÖ Verdadeiro. O ponto `[-0.8, -1]` est√° muito mais pr√≥ximo dos pontos da classe 1 (e.g., `[-1,-1]`, `[-2,-1]`) do que dos pontos da classe 2, assumindo uma distribui√ß√£o Gaussiana para cada classe. `GaussianNB` calcularia as probabilidades e classificaria como 1. üéØ‚òÅÔ∏è **Dica:** `GaussianNB` assume features Gaussianas. Visualize (mentalmente) onde o novo ponto se encaixa.                                                                                  |
| 147 | TensorFlow/Keras: `model.add(tf.keras.layers.LSTM(10, return_sequences=True)); model.add(tf.keras.layers.LSTM(5))`. A primeira camada LSTM retornar√° a sequ√™ncia completa de outputs para cada timestep, enquanto a segunda retornar√° apenas o output do √∫ltimo timestep.                                                                                       | V        | ‚úÖ Verdadeiro. `return_sequences=True` faz com que a camada LSTM retorne o estado oculto para cada timestep da sequ√™ncia de entrada (formato 3D). Se `False` (padr√£o para a √∫ltima LSTM se n√£o for seguida por outra recorrente), ela retorna apenas o estado oculto do √∫ltimo timestep (formato 2D). ‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è **Dica:** `return_sequences=True` se voc√™ vai empilhar outra camada recorrente ou precisa de todos os outputs.                                            |
| 148 | PyTorch: `import torch; t = torch.arange(4).reshape(2,2); print(t[0])`. A sa√≠da ser√° `tensor([0, 1])`.                                                                                                                                                                                                                                                          | V        | ‚úÖ Verdadeiro. `torch.arange(4)` cria `tensor([0, 1, 2, 3])`. `reshape(2,2)` o transforma em `tensor([[0, 1], [2, 3]])`. `t[0]` seleciona a primeira linha. üî™üß± **Dica:** Indexa√ß√£o em tensores PyTorch √© similar a NumPy. `t[i]` pega a i-√©sima linha.                                                                                                                                                                                                               |
| 149 | NLTK: `from nltk.tag import UnigramTagger; from nltk.corpus import brown; train_sents = brown.tagged_sents(categories='news')[:100]; tagger = UnigramTagger(train_sents)`. O `tagger` aprender√° a etiquetar palavras com base apenas na palavra anterior na senten√ßa.                                                                                           | F        | ‚ùå Falso. `UnigramTagger` etiqueta cada palavra com a tag mais frequente para *aquela palavra espec√≠fica* nos dados de treinamento, independentemente do contexto. `BigramTagger` consideraria a palavra anterior. üè∑Ô∏è1Ô∏è‚É£ **Dica:** Unigram = 1 palavra (sem contexto). Bigram = 2 palavras (contexto da anterior).                                                                                                                                                    |
| 150 | Matplotlib: `import matplotlib.pyplot as plt; plt.plot([1,2,3]); plt.clf(); plt.title("Teste"); plt.show()`. O gr√°fico exibido n√£o conter√° a linha plotada por `plt.plot([1,2,3])` mas ter√° o t√≠tulo "Teste".                                                                                                                                                   | V        | ‚úÖ Verdadeiro. `plt.clf()` (Clear Figure) remove todo o conte√∫do da figura atual, incluindo plots, mas n√£o remove a figura em si. A chamada subsequente `plt.title()` adiciona um t√≠tulo √† figura agora vazia. üßπüñºÔ∏è **Dica:** `clf()` limpa a figura. `cla()` limpa o eixo (Axes). √ötil para redesenhar em loops.                                                                                                                                                     |

